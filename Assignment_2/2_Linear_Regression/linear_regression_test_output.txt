### 1. random dataset ###
Training set: X_train shape = (10, 850), y_train shape = (850,)
Development set: X_dev shape = (10, 50), y_dev shape = (50,)
Test set: X_test shape = (10, 100), y_test shape = (100,)
Iteration 1/100: Training Loss = 2245.9618535604964
Validation Loss = 2245.9618535604964
Iteration 2/100: Training Loss = 776.1036501627084
Validation Loss = 776.1036501627084
Iteration 3/100: Training Loss = 274.9157442532406
Validation Loss = 274.9157442532406
Iteration 4/100: Training Loss = 98.88831208337116
Validation Loss = 98.88831208337116
Iteration 5/100: Training Loss = 35.520467441660685
Validation Loss = 35.520467441660685
Iteration 6/100: Training Loss = 13.376467409950367
Validation Loss = 13.376467409950367
Iteration 7/100: Training Loss = 5.4083714169607955
Validation Loss = 5.4083714169607955
Iteration 8/100: Training Loss = 2.599498869654456
Validation Loss = 2.599498869654456
Iteration 9/100: Training Loss = 1.539702587584912
Validation Loss = 1.539702587584912
Iteration 10/100: Training Loss = 1.1563429472937785
Validation Loss = 1.1563429472937785
Iteration 11/100: Training Loss = 1.0162176635295352
Validation Loss = 1.0162176635295352
Iteration 12/100: Training Loss = 0.9620559746161046
Validation Loss = 0.9620559746161046
Iteration 13/100: Training Loss = 0.9425308495117244
Validation Loss = 0.9425308495117244
Iteration 14/100: Training Loss = 0.93646105938496
Validation Loss = 0.93646105938496
Iteration 15/100: Training Loss = 0.9330801404396595
Validation Loss = 0.9330801404396595
Iteration 16/100: Training Loss = 0.9310544796861814
Validation Loss = 0.9310544796861814
Iteration 17/100: Training Loss = 0.9307187408566792
Validation Loss = 0.9307187408566792
Iteration 18/100: Training Loss = 0.9301585378569901
Validation Loss = 0.9301585378569901
Iteration 19/100: Training Loss = 0.9296832543329734
Validation Loss = 0.9296832543329734
Iteration 20/100: Training Loss = 0.9291295288714981
Validation Loss = 0.9291295288714981
Iteration 21/100: Training Loss = 0.9290209208260245
Validation Loss = 0.9290209208260245
Early stopping at epoch 21
Training finished after 21 iterations
Mean squared error: 1.11
Coefficient of determination: 1.00
Training set: X_train shape = (10, 8500), y_train shape = (8500,)
Development set: X_dev shape = (10, 500), y_dev shape = (500,)
Test set: X_test shape = (10, 1000), y_test shape = (1000,)
Iteration 1/100: Training Loss = 1.4482828534694738
Validation Loss = 1.4482828534694738
Iteration 2/100: Training Loss = 1.0264575765743769
Validation Loss = 1.0264575765743769
Iteration 3/100: Training Loss = 1.0276290375556794
Validation Loss = 1.0276290375556794
Iteration 4/100: Training Loss = 1.0205654147487908
Validation Loss = 1.0205654147487908
Iteration 5/100: Training Loss = 1.018940666353084
Validation Loss = 1.018940666353084
Iteration 6/100: Training Loss = 1.0189908331588662
Validation Loss = 1.0189908331588662
Iteration 7/100: Training Loss = 1.0170559821370644
Validation Loss = 1.0170559821370644
Iteration 8/100: Training Loss = 1.014893670106092
Validation Loss = 1.014893670106092
Iteration 9/100: Training Loss = 1.0141485077397001
Validation Loss = 1.0141485077397001
Iteration 10/100: Training Loss = 1.015980979656395
Validation Loss = 1.015980979656395
Iteration 11/100: Training Loss = 1.0137211924618255
Validation Loss = 1.0137211924618255
Iteration 12/100: Training Loss = 1.0136300330244197
Validation Loss = 1.0136300330244197
Iteration 13/100: Training Loss = 1.013571400559637
Validation Loss = 1.013571400559637
Iteration 14/100: Training Loss = 1.0143107199270411
Validation Loss = 1.0143107199270411
Iteration 15/100: Training Loss = 1.0149169293904972
Validation Loss = 1.0149169293904972
Iteration 16/100: Training Loss = 1.013351802228298
Validation Loss = 1.013351802228298
Iteration 17/100: Training Loss = 1.0140832395593555
Validation Loss = 1.0140832395593555
Early stopping at epoch 17
Training finished after 17 iterations
Mean squared error: 1.00
Coefficient of determination: 1.00
Training set: X_train shape = (10, 85000), y_train shape = (85000,)
Development set: X_dev shape = (10, 5000), y_dev shape = (5000,)
Test set: X_test shape = (10, 10000), y_test shape = (10000,)
Iteration 1/100: Training Loss = 0.9911960480159696
Validation Loss = 0.9911960480159696
Iteration 2/100: Training Loss = 0.9884390875741446
Validation Loss = 0.9884390875741446
Iteration 3/100: Training Loss = 0.989035238965807
Validation Loss = 0.989035238965807
Iteration 4/100: Training Loss = 0.9896272529081637
Validation Loss = 0.9896272529081637
Iteration 5/100: Training Loss = 0.9876460555641275
Validation Loss = 0.9876460555641275
Iteration 6/100: Training Loss = 0.98831209677852
Validation Loss = 0.98831209677852
Iteration 7/100: Training Loss = 0.9874930140230386
Validation Loss = 0.9874930140230386
Iteration 8/100: Training Loss = 0.9912793842935763
Validation Loss = 0.9912793842935763
Iteration 9/100: Training Loss = 0.9908938880044366
Validation Loss = 0.9908938880044366
Iteration 10/100: Training Loss = 0.9893397532685421
Validation Loss = 0.9893397532685421
Iteration 11/100: Training Loss = 0.9881910589448699
Validation Loss = 0.9881910589448699
Iteration 12/100: Training Loss = 0.9887277368913858
Validation Loss = 0.9887277368913858
Early stopping at epoch 12
Training finished after 12 iterations
Mean squared error: 0.97
Coefficient of determination: 1.00

### 2. diabets ###
Iteration 1/10000: Training Loss = 25807.90547478002
Validation Loss = 25807.90547478002
Iteration 2/10000: Training Loss = 22832.833999234008
Validation Loss = 22832.833999234008
Iteration 3/10000: Training Loss = 20296.688059701188
Validation Loss = 20296.688059701188
Iteration 4/10000: Training Loss = 18162.502451149503
Validation Loss = 18162.502451149503
Iteration 5/10000: Training Loss = 16315.9165481815
Validation Loss = 16315.9165481815
Iteration 6/10000: Training Loss = 14753.24423140904
Validation Loss = 14753.24423140904
Iteration 7/10000: Training Loss = 13429.687355291784
Validation Loss = 13429.687355291784
Iteration 8/10000: Training Loss = 12294.82220117181
Validation Loss = 12294.82220117181
Iteration 9/10000: Training Loss = 11329.139428655111
Validation Loss = 11329.139428655111
Iteration 10/10000: Training Loss = 10516.589096435502
Validation Loss = 10516.589096435502
Iteration 11/10000: Training Loss = 9821.877916714293
Validation Loss = 9821.877916714293
Iteration 12/10000: Training Loss = 9231.737208017901
Validation Loss = 9231.737208017901
Iteration 13/10000: Training Loss = 8717.316500992378
Validation Loss = 8717.316500992378
Iteration 14/10000: Training Loss = 8279.206148435987
Validation Loss = 8279.206148435987
Iteration 15/10000: Training Loss = 7918.179059430322
Validation Loss = 7918.179059430322
Iteration 16/10000: Training Loss = 7604.942977498717
Validation Loss = 7604.942977498717
Iteration 17/10000: Training Loss = 7339.176060650833
Validation Loss = 7339.176060650833
Iteration 18/10000: Training Loss = 7112.519550149417
Validation Loss = 7112.519550149417
Iteration 19/10000: Training Loss = 6919.685538968397
Validation Loss = 6919.685538968397
Iteration 20/10000: Training Loss = 6754.871108873316
Validation Loss = 6754.871108873316
Iteration 21/10000: Training Loss = 6614.751172982134
Validation Loss = 6614.751172982134
Iteration 22/10000: Training Loss = 6494.1270744136655
Validation Loss = 6494.1270744136655
Iteration 23/10000: Training Loss = 6394.263569891174
Validation Loss = 6394.263569891174
Iteration 24/10000: Training Loss = 6305.992791698056
Validation Loss = 6305.992791698056
Iteration 25/10000: Training Loss = 6232.119393656465
Validation Loss = 6232.119393656465
Iteration 26/10000: Training Loss = 6169.0539222463385
Validation Loss = 6169.0539222463385
Iteration 27/10000: Training Loss = 6113.689442590928
Validation Loss = 6113.689442590928
Iteration 28/10000: Training Loss = 6066.467395346977
Validation Loss = 6066.467395346977
Iteration 29/10000: Training Loss = 6025.5101435834795
Validation Loss = 6025.5101435834795
Iteration 30/10000: Training Loss = 5991.27043851321
Validation Loss = 5991.27043851321
Iteration 31/10000: Training Loss = 5961.995368783147
Validation Loss = 5961.995368783147
Iteration 32/10000: Training Loss = 5936.709134286338
Validation Loss = 5936.709134286338
Iteration 33/10000: Training Loss = 5914.681397427928
Validation Loss = 5914.681397427928
Iteration 34/10000: Training Loss = 5895.097885423221
Validation Loss = 5895.097885423221
Iteration 35/10000: Training Loss = 5878.517561559831
Validation Loss = 5878.517561559831
Iteration 36/10000: Training Loss = 5864.190407076865
Validation Loss = 5864.190407076865
Iteration 37/10000: Training Loss = 5850.811578625953
Validation Loss = 5850.811578625953
Iteration 38/10000: Training Loss = 5839.335224326087
Validation Loss = 5839.335224326087
Iteration 39/10000: Training Loss = 5828.823856996689
Validation Loss = 5828.823856996689
Iteration 40/10000: Training Loss = 5819.313824967589
Validation Loss = 5819.313824967589
Iteration 41/10000: Training Loss = 5811.098682196046
Validation Loss = 5811.098682196046
Iteration 42/10000: Training Loss = 5803.90868215368
Validation Loss = 5803.90868215368
Iteration 43/10000: Training Loss = 5797.575107637602
Validation Loss = 5797.575107637602
Iteration 44/10000: Training Loss = 5791.086730913784
Validation Loss = 5791.086730913784
Iteration 45/10000: Training Loss = 5785.61514940829
Validation Loss = 5785.61514940829
Iteration 46/10000: Training Loss = 5780.765657262998
Validation Loss = 5780.765657262998
Iteration 47/10000: Training Loss = 5775.721664372121
Validation Loss = 5775.721664372121
Iteration 48/10000: Training Loss = 5771.295600809904
Validation Loss = 5771.295600809904
Iteration 49/10000: Training Loss = 5767.199400226636
Validation Loss = 5767.199400226636
Iteration 50/10000: Training Loss = 5762.919213645083
Validation Loss = 5762.919213645083
Iteration 51/10000: Training Loss = 5759.364089771792
Validation Loss = 5759.364089771792
Iteration 52/10000: Training Loss = 5755.812682419704
Validation Loss = 5755.812682419704
Iteration 53/10000: Training Loss = 5752.423096016473
Validation Loss = 5752.423096016473
Iteration 54/10000: Training Loss = 5749.295563527694
Validation Loss = 5749.295563527694
Iteration 55/10000: Training Loss = 5746.462492299382
Validation Loss = 5746.462492299382
Iteration 56/10000: Training Loss = 5743.354104887357
Validation Loss = 5743.354104887357
Iteration 57/10000: Training Loss = 5740.341304191441
Validation Loss = 5740.341304191441
Iteration 58/10000: Training Loss = 5737.451600136767
Validation Loss = 5737.451600136767
Iteration 59/10000: Training Loss = 5734.598820326394
Validation Loss = 5734.598820326394
Iteration 60/10000: Training Loss = 5731.692056458116
Validation Loss = 5731.692056458116
Iteration 61/10000: Training Loss = 5728.932794616741
Validation Loss = 5728.932794616741
Iteration 62/10000: Training Loss = 5726.3493984116585
Validation Loss = 5726.3493984116585
Iteration 63/10000: Training Loss = 5723.435281742444
Validation Loss = 5723.435281742444
Iteration 64/10000: Training Loss = 5720.589622943351
Validation Loss = 5720.589622943351
Iteration 65/10000: Training Loss = 5717.673559764499
Validation Loss = 5717.673559764499
Iteration 66/10000: Training Loss = 5714.89294072013
Validation Loss = 5714.89294072013
Iteration 67/10000: Training Loss = 5712.031181100904
Validation Loss = 5712.031181100904
Iteration 68/10000: Training Loss = 5709.2367305379585
Validation Loss = 5709.2367305379585
Iteration 69/10000: Training Loss = 5706.50063647614
Validation Loss = 5706.50063647614
Iteration 70/10000: Training Loss = 5703.842331871557
Validation Loss = 5703.842331871557
Iteration 71/10000: Training Loss = 5701.135244928001
Validation Loss = 5701.135244928001
Iteration 72/10000: Training Loss = 5698.4683790069075
Validation Loss = 5698.4683790069075
Iteration 73/10000: Training Loss = 5695.754491348557
Validation Loss = 5695.754491348557
Iteration 74/10000: Training Loss = 5693.162452691629
Validation Loss = 5693.162452691629
Iteration 75/10000: Training Loss = 5690.53045266819
Validation Loss = 5690.53045266819
Iteration 76/10000: Training Loss = 5687.869214951889
Validation Loss = 5687.869214951889
Iteration 77/10000: Training Loss = 5685.207863373393
Validation Loss = 5685.207863373393
Iteration 78/10000: Training Loss = 5682.559548829702
Validation Loss = 5682.559548829702
Iteration 79/10000: Training Loss = 5679.9027905691255
Validation Loss = 5679.9027905691255
Iteration 80/10000: Training Loss = 5677.330969527402
Validation Loss = 5677.330969527402
Iteration 81/10000: Training Loss = 5674.719084552472
Validation Loss = 5674.719084552472
Iteration 82/10000: Training Loss = 5672.094751290433
Validation Loss = 5672.094751290433
Iteration 83/10000: Training Loss = 5669.473713109196
Validation Loss = 5669.473713109196
Iteration 84/10000: Training Loss = 5666.878350149816
Validation Loss = 5666.878350149816
Iteration 85/10000: Training Loss = 5664.254739599972
Validation Loss = 5664.254739599972
Iteration 86/10000: Training Loss = 5661.662431629698
Validation Loss = 5661.662431629698
Iteration 87/10000: Training Loss = 5659.053813854501
Validation Loss = 5659.053813854501
Iteration 88/10000: Training Loss = 5656.453927753995
Validation Loss = 5656.453927753995
Iteration 89/10000: Training Loss = 5653.847247312208
Validation Loss = 5653.847247312208
Iteration 90/10000: Training Loss = 5651.272496098811
Validation Loss = 5651.272496098811
Iteration 91/10000: Training Loss = 5648.694092301408
Validation Loss = 5648.694092301408
Iteration 92/10000: Training Loss = 5646.110147564093
Validation Loss = 5646.110147564093
Iteration 93/10000: Training Loss = 5643.5193795479745
Validation Loss = 5643.5193795479745
Iteration 94/10000: Training Loss = 5640.929848537847
Validation Loss = 5640.929848537847
Iteration 95/10000: Training Loss = 5638.343516611804
Validation Loss = 5638.343516611804
Iteration 96/10000: Training Loss = 5635.790222933343
Validation Loss = 5635.790222933343
Iteration 97/10000: Training Loss = 5633.197427915388
Validation Loss = 5633.197427915388
Iteration 98/10000: Training Loss = 5630.597216492875
Validation Loss = 5630.597216492875
Iteration 99/10000: Training Loss = 5628.0527392475715
Validation Loss = 5628.0527392475715
Iteration 100/10000: Training Loss = 5625.458517349039
Validation Loss = 5625.458517349039
Iteration 101/10000: Training Loss = 5622.931383276618
Validation Loss = 5622.931383276618
Iteration 102/10000: Training Loss = 5620.3906342026385
Validation Loss = 5620.3906342026385
Iteration 103/10000: Training Loss = 5617.862998798514
Validation Loss = 5617.862998798514
Iteration 104/10000: Training Loss = 5615.33374601806
Validation Loss = 5615.33374601806
Iteration 105/10000: Training Loss = 5612.822505653179
Validation Loss = 5612.822505653179
Iteration 106/10000: Training Loss = 5610.302303913623
Validation Loss = 5610.302303913623
Iteration 107/10000: Training Loss = 5607.777259889585
Validation Loss = 5607.777259889585
Iteration 108/10000: Training Loss = 5605.275155283324
Validation Loss = 5605.275155283324
Iteration 109/10000: Training Loss = 5602.758176589979
Validation Loss = 5602.758176589979
Iteration 110/10000: Training Loss = 5600.226970241786
Validation Loss = 5600.226970241786
Iteration 111/10000: Training Loss = 5597.724584863633
Validation Loss = 5597.724584863633
Iteration 112/10000: Training Loss = 5595.220130071202
Validation Loss = 5595.220130071202
Iteration 113/10000: Training Loss = 5592.737471535115
Validation Loss = 5592.737471535115
Iteration 114/10000: Training Loss = 5590.234258792639
Validation Loss = 5590.234258792639
Iteration 115/10000: Training Loss = 5587.720363333175
Validation Loss = 5587.720363333175
Iteration 116/10000: Training Loss = 5585.213957249802
Validation Loss = 5585.213957249802
Iteration 117/10000: Training Loss = 5582.735189835184
Validation Loss = 5582.735189835184
Iteration 118/10000: Training Loss = 5580.236974021038
Validation Loss = 5580.236974021038
Iteration 119/10000: Training Loss = 5577.750424118486
Validation Loss = 5577.750424118486
Iteration 120/10000: Training Loss = 5575.283802970368
Validation Loss = 5575.283802970368
Iteration 121/10000: Training Loss = 5572.795722356106
Validation Loss = 5572.795722356106
Iteration 122/10000: Training Loss = 5570.343749162589
Validation Loss = 5570.343749162589
Iteration 123/10000: Training Loss = 5567.873126724213
Validation Loss = 5567.873126724213
Iteration 124/10000: Training Loss = 5565.408553238489
Validation Loss = 5565.408553238489
Iteration 125/10000: Training Loss = 5562.933336381193
Validation Loss = 5562.933336381193
Iteration 126/10000: Training Loss = 5560.425688392374
Validation Loss = 5560.425688392374
Iteration 127/10000: Training Loss = 5557.9543096517555
Validation Loss = 5557.9543096517555
Iteration 128/10000: Training Loss = 5555.479166496305
Validation Loss = 5555.479166496305
Iteration 129/10000: Training Loss = 5553.0170467399375
Validation Loss = 5553.0170467399375
Iteration 130/10000: Training Loss = 5550.5922916392865
Validation Loss = 5550.5922916392865
Iteration 131/10000: Training Loss = 5548.131289142875
Validation Loss = 5548.131289142875
Iteration 132/10000: Training Loss = 5545.670170823907
Validation Loss = 5545.670170823907
Iteration 133/10000: Training Loss = 5543.232348453829
Validation Loss = 5543.232348453829
Iteration 134/10000: Training Loss = 5540.783726738994
Validation Loss = 5540.783726738994
Iteration 135/10000: Training Loss = 5538.377530095138
Validation Loss = 5538.377530095138
Iteration 136/10000: Training Loss = 5535.9383906823705
Validation Loss = 5535.9383906823705
Iteration 137/10000: Training Loss = 5533.49556456779
Validation Loss = 5533.49556456779
Iteration 138/10000: Training Loss = 5531.0864947050495
Validation Loss = 5531.0864947050495
Iteration 139/10000: Training Loss = 5528.663604182791
Validation Loss = 5528.663604182791
Iteration 140/10000: Training Loss = 5526.267452204526
Validation Loss = 5526.267452204526
Iteration 141/10000: Training Loss = 5523.851529555516
Validation Loss = 5523.851529555516
Iteration 142/10000: Training Loss = 5521.434954969995
Validation Loss = 5521.434954969995
Iteration 143/10000: Training Loss = 5518.985564825406
Validation Loss = 5518.985564825406
Iteration 144/10000: Training Loss = 5516.564030259726
Validation Loss = 5516.564030259726
Iteration 145/10000: Training Loss = 5514.129991061025
Validation Loss = 5514.129991061025
Iteration 146/10000: Training Loss = 5511.735883699498
Validation Loss = 5511.735883699498
Iteration 147/10000: Training Loss = 5509.351466890039
Validation Loss = 5509.351466890039
Iteration 148/10000: Training Loss = 5506.947935724752
Validation Loss = 5506.947935724752
Iteration 149/10000: Training Loss = 5504.542839005192
Validation Loss = 5504.542839005192
Iteration 150/10000: Training Loss = 5502.172994687985
Validation Loss = 5502.172994687985
Iteration 151/10000: Training Loss = 5499.780654304076
Validation Loss = 5499.780654304076
Iteration 152/10000: Training Loss = 5497.405059652226
Validation Loss = 5497.405059652226
Iteration 153/10000: Training Loss = 5495.030762107015
Validation Loss = 5495.030762107015
Iteration 154/10000: Training Loss = 5492.6460462603745
Validation Loss = 5492.6460462603745
Iteration 155/10000: Training Loss = 5490.242084605922
Validation Loss = 5490.242084605922
Iteration 156/10000: Training Loss = 5487.840905756408
Validation Loss = 5487.840905756408
Iteration 157/10000: Training Loss = 5485.452448127707
Validation Loss = 5485.452448127707
Iteration 158/10000: Training Loss = 5483.096824230939
Validation Loss = 5483.096824230939
Iteration 159/10000: Training Loss = 5480.737910308416
Validation Loss = 5480.737910308416
Iteration 160/10000: Training Loss = 5478.414156259947
Validation Loss = 5478.414156259947
Iteration 161/10000: Training Loss = 5476.056821258593
Validation Loss = 5476.056821258593
Iteration 162/10000: Training Loss = 5473.715302015019
Validation Loss = 5473.715302015019
Iteration 163/10000: Training Loss = 5471.329604149696
Validation Loss = 5471.329604149696
Iteration 164/10000: Training Loss = 5468.950281144176
Validation Loss = 5468.950281144176
Iteration 165/10000: Training Loss = 5466.605396720885
Validation Loss = 5466.605396720885
Iteration 166/10000: Training Loss = 5464.287217945796
Validation Loss = 5464.287217945796
Iteration 167/10000: Training Loss = 5461.932758468649
Validation Loss = 5461.932758468649
Iteration 168/10000: Training Loss = 5459.555761920268
Validation Loss = 5459.555761920268
Iteration 169/10000: Training Loss = 5457.224304341012
Validation Loss = 5457.224304341012
Iteration 170/10000: Training Loss = 5454.89719609239
Validation Loss = 5454.89719609239
Iteration 171/10000: Training Loss = 5452.532419324644
Validation Loss = 5452.532419324644
Iteration 172/10000: Training Loss = 5450.234210318703
Validation Loss = 5450.234210318703
Iteration 173/10000: Training Loss = 5447.9465285448305
Validation Loss = 5447.9465285448305
Iteration 174/10000: Training Loss = 5445.608865648719
Validation Loss = 5445.608865648719
Iteration 175/10000: Training Loss = 5443.297818520963
Validation Loss = 5443.297818520963
Iteration 176/10000: Training Loss = 5440.974558866194
Validation Loss = 5440.974558866194
Iteration 177/10000: Training Loss = 5438.669415101431
Validation Loss = 5438.669415101431
Iteration 178/10000: Training Loss = 5436.374668827546
Validation Loss = 5436.374668827546
Iteration 179/10000: Training Loss = 5434.107486682147
Validation Loss = 5434.107486682147
Iteration 180/10000: Training Loss = 5431.811334769053
Validation Loss = 5431.811334769053
Iteration 181/10000: Training Loss = 5429.506373479696
Validation Loss = 5429.506373479696
Iteration 182/10000: Training Loss = 5427.177891541275
Validation Loss = 5427.177891541275
Iteration 183/10000: Training Loss = 5424.86821322708
Validation Loss = 5424.86821322708
Iteration 184/10000: Training Loss = 5422.570140140781
Validation Loss = 5422.570140140781
Iteration 185/10000: Training Loss = 5420.279858571463
Validation Loss = 5420.279858571463
Iteration 186/10000: Training Loss = 5417.994885680362
Validation Loss = 5417.994885680362
Iteration 187/10000: Training Loss = 5415.722468846618
Validation Loss = 5415.722468846618
Iteration 188/10000: Training Loss = 5413.434961765165
Validation Loss = 5413.434961765165
Iteration 189/10000: Training Loss = 5411.169759474864
Validation Loss = 5411.169759474864
Iteration 190/10000: Training Loss = 5408.893862175564
Validation Loss = 5408.893862175564
Iteration 191/10000: Training Loss = 5406.628285434347
Validation Loss = 5406.628285434347
Iteration 192/10000: Training Loss = 5404.37033216365
Validation Loss = 5404.37033216365
Iteration 193/10000: Training Loss = 5402.124435159635
Validation Loss = 5402.124435159635
Iteration 194/10000: Training Loss = 5399.868728148391
Validation Loss = 5399.868728148391
Iteration 195/10000: Training Loss = 5397.598066879425
Validation Loss = 5397.598066879425
Iteration 196/10000: Training Loss = 5395.340787796537
Validation Loss = 5395.340787796537
Iteration 197/10000: Training Loss = 5393.07139965751
Validation Loss = 5393.07139965751
Iteration 198/10000: Training Loss = 5390.767381784926
Validation Loss = 5390.767381784926
Iteration 199/10000: Training Loss = 5388.492892807427
Validation Loss = 5388.492892807427
Iteration 200/10000: Training Loss = 5386.250497776094
Validation Loss = 5386.250497776094
Iteration 201/10000: Training Loss = 5383.990958600642
Validation Loss = 5383.990958600642
Iteration 202/10000: Training Loss = 5381.716631074223
Validation Loss = 5381.716631074223
Iteration 203/10000: Training Loss = 5379.465974126973
Validation Loss = 5379.465974126973
Iteration 204/10000: Training Loss = 5377.246829570187
Validation Loss = 5377.246829570187
Iteration 205/10000: Training Loss = 5375.0023383219
Validation Loss = 5375.0023383219
Iteration 206/10000: Training Loss = 5372.762930298115
Validation Loss = 5372.762930298115
Iteration 207/10000: Training Loss = 5370.537780882352
Validation Loss = 5370.537780882352
Iteration 208/10000: Training Loss = 5368.301135251771
Validation Loss = 5368.301135251771
Iteration 209/10000: Training Loss = 5366.07940104749
Validation Loss = 5366.07940104749
Iteration 210/10000: Training Loss = 5363.855124915782
Validation Loss = 5363.855124915782
Iteration 211/10000: Training Loss = 5361.642534257674
Validation Loss = 5361.642534257674
Iteration 212/10000: Training Loss = 5359.44287294303
Validation Loss = 5359.44287294303
Iteration 213/10000: Training Loss = 5357.2085277712795
Validation Loss = 5357.2085277712795
Iteration 214/10000: Training Loss = 5355.034241861805
Validation Loss = 5355.034241861805
Iteration 215/10000: Training Loss = 5352.827859977564
Validation Loss = 5352.827859977564
Iteration 216/10000: Training Loss = 5350.6064712781335
Validation Loss = 5350.6064712781335
Iteration 217/10000: Training Loss = 5348.380943224396
Validation Loss = 5348.380943224396
Iteration 218/10000: Training Loss = 5346.154505632406
Validation Loss = 5346.154505632406
Iteration 219/10000: Training Loss = 5343.931033462514
Validation Loss = 5343.931033462514
Iteration 220/10000: Training Loss = 5341.736610016332
Validation Loss = 5341.736610016332
Iteration 221/10000: Training Loss = 5339.528427801617
Validation Loss = 5339.528427801617
Iteration 222/10000: Training Loss = 5337.337636731289
Validation Loss = 5337.337636731289
Iteration 223/10000: Training Loss = 5335.163309956785
Validation Loss = 5335.163309956785
Iteration 224/10000: Training Loss = 5332.975978844054
Validation Loss = 5332.975978844054
Iteration 225/10000: Training Loss = 5330.7837076381375
Validation Loss = 5330.7837076381375
Iteration 226/10000: Training Loss = 5328.59520813918
Validation Loss = 5328.59520813918
Iteration 227/10000: Training Loss = 5326.405057621297
Validation Loss = 5326.405057621297
Iteration 228/10000: Training Loss = 5324.237743260355
Validation Loss = 5324.237743260355
Iteration 229/10000: Training Loss = 5322.050409957078
Validation Loss = 5322.050409957078
Iteration 230/10000: Training Loss = 5319.854856848434
Validation Loss = 5319.854856848434
Iteration 231/10000: Training Loss = 5317.6880399934935
Validation Loss = 5317.6880399934935
Iteration 232/10000: Training Loss = 5315.524658299119
Validation Loss = 5315.524658299119
Iteration 233/10000: Training Loss = 5313.341960516181
Validation Loss = 5313.341960516181
Iteration 234/10000: Training Loss = 5311.178385713207
Validation Loss = 5311.178385713207
Iteration 235/10000: Training Loss = 5309.012195960253
Validation Loss = 5309.012195960253
Iteration 236/10000: Training Loss = 5306.847668966625
Validation Loss = 5306.847668966625
Iteration 237/10000: Training Loss = 5304.678641543029
Validation Loss = 5304.678641543029
Iteration 238/10000: Training Loss = 5302.5423079805905
Validation Loss = 5302.5423079805905
Iteration 239/10000: Training Loss = 5300.4004686634
Validation Loss = 5300.4004686634
Iteration 240/10000: Training Loss = 5298.272091761909
Validation Loss = 5298.272091761909
Iteration 241/10000: Training Loss = 5296.135218572511
Validation Loss = 5296.135218572511
Iteration 242/10000: Training Loss = 5294.003522509144
Validation Loss = 5294.003522509144
Iteration 243/10000: Training Loss = 5291.867569424921
Validation Loss = 5291.867569424921
Iteration 244/10000: Training Loss = 5289.711322790799
Validation Loss = 5289.711322790799
Iteration 245/10000: Training Loss = 5287.558940155048
Validation Loss = 5287.558940155048
Iteration 246/10000: Training Loss = 5285.435237582412
Validation Loss = 5285.435237582412
Iteration 247/10000: Training Loss = 5283.31962879076
Validation Loss = 5283.31962879076
Iteration 248/10000: Training Loss = 5281.176429216336
Validation Loss = 5281.176429216336
Iteration 249/10000: Training Loss = 5279.050297974984
Validation Loss = 5279.050297974984
Iteration 250/10000: Training Loss = 5276.948758357345
Validation Loss = 5276.948758357345
Iteration 251/10000: Training Loss = 5274.830801623569
Validation Loss = 5274.830801623569
Iteration 252/10000: Training Loss = 5272.720636056251
Validation Loss = 5272.720636056251
Iteration 253/10000: Training Loss = 5270.60719862854
Validation Loss = 5270.60719862854
Iteration 254/10000: Training Loss = 5268.512256446708
Validation Loss = 5268.512256446708
Iteration 255/10000: Training Loss = 5266.395302988663
Validation Loss = 5266.395302988663
Iteration 256/10000: Training Loss = 5264.307482264192
Validation Loss = 5264.307482264192
Iteration 257/10000: Training Loss = 5262.215433044514
Validation Loss = 5262.215433044514
Iteration 258/10000: Training Loss = 5260.127662723016
Validation Loss = 5260.127662723016
Iteration 259/10000: Training Loss = 5258.000535421784
Validation Loss = 5258.000535421784
Iteration 260/10000: Training Loss = 5255.922991992854
Validation Loss = 5255.922991992854
Iteration 261/10000: Training Loss = 5253.854601773183
Validation Loss = 5253.854601773183
Iteration 262/10000: Training Loss = 5251.776371312755
Validation Loss = 5251.776371312755
Iteration 263/10000: Training Loss = 5249.68058491638
Validation Loss = 5249.68058491638
Iteration 264/10000: Training Loss = 5247.586769790032
Validation Loss = 5247.586769790032
Iteration 265/10000: Training Loss = 5245.493544377094
Validation Loss = 5245.493544377094
Iteration 266/10000: Training Loss = 5243.416312461199
Validation Loss = 5243.416312461199
Iteration 267/10000: Training Loss = 5241.348185009136
Validation Loss = 5241.348185009136
Iteration 268/10000: Training Loss = 5239.255841973672
Validation Loss = 5239.255841973672
Iteration 269/10000: Training Loss = 5237.194094019134
Validation Loss = 5237.194094019134
Iteration 270/10000: Training Loss = 5235.1377044928495
Validation Loss = 5235.1377044928495
Iteration 271/10000: Training Loss = 5233.0799053951
Validation Loss = 5233.0799053951
Iteration 272/10000: Training Loss = 5230.986959995467
Validation Loss = 5230.986959995467
Iteration 273/10000: Training Loss = 5228.922427048596
Validation Loss = 5228.922427048596
Iteration 274/10000: Training Loss = 5226.8450040737525
Validation Loss = 5226.8450040737525
Iteration 275/10000: Training Loss = 5224.8258139715
Validation Loss = 5224.8258139715
Iteration 276/10000: Training Loss = 5222.751538862286
Validation Loss = 5222.751538862286
Iteration 277/10000: Training Loss = 5220.677660157526
Validation Loss = 5220.677660157526
Iteration 278/10000: Training Loss = 5218.60215005814
Validation Loss = 5218.60215005814
Iteration 279/10000: Training Loss = 5216.525627448043
Validation Loss = 5216.525627448043
Iteration 280/10000: Training Loss = 5214.45570525516
Validation Loss = 5214.45570525516
Iteration 281/10000: Training Loss = 5212.3998714671725
Validation Loss = 5212.3998714671725
Iteration 282/10000: Training Loss = 5210.345222990942
Validation Loss = 5210.345222990942
Iteration 283/10000: Training Loss = 5208.326120850257
Validation Loss = 5208.326120850257
Iteration 284/10000: Training Loss = 5206.300392305725
Validation Loss = 5206.300392305725
Iteration 285/10000: Training Loss = 5204.2660577016395
Validation Loss = 5204.2660577016395
Iteration 286/10000: Training Loss = 5202.245690062489
Validation Loss = 5202.245690062489
Iteration 287/10000: Training Loss = 5200.198480484047
Validation Loss = 5200.198480484047
Iteration 288/10000: Training Loss = 5198.1778646168705
Validation Loss = 5198.1778646168705
Iteration 289/10000: Training Loss = 5196.17449378501
Validation Loss = 5196.17449378501
Iteration 290/10000: Training Loss = 5194.174812056971
Validation Loss = 5194.174812056971
Iteration 291/10000: Training Loss = 5192.176896698789
Validation Loss = 5192.176896698789
Iteration 292/10000: Training Loss = 5190.109876132009
Validation Loss = 5190.109876132009
Iteration 293/10000: Training Loss = 5188.102221824459
Validation Loss = 5188.102221824459
Iteration 294/10000: Training Loss = 5186.100845892734
Validation Loss = 5186.100845892734
Iteration 295/10000: Training Loss = 5184.116339185541
Validation Loss = 5184.116339185541
Iteration 296/10000: Training Loss = 5182.138252426762
Validation Loss = 5182.138252426762
Iteration 297/10000: Training Loss = 5180.150315111536
Validation Loss = 5180.150315111536
Iteration 298/10000: Training Loss = 5178.128270532921
Validation Loss = 5178.128270532921
Iteration 299/10000: Training Loss = 5176.132911262676
Validation Loss = 5176.132911262676
Iteration 300/10000: Training Loss = 5174.151582347737
Validation Loss = 5174.151582347737
Iteration 301/10000: Training Loss = 5172.172782771855
Validation Loss = 5172.172782771855
Iteration 302/10000: Training Loss = 5170.188248209475
Validation Loss = 5170.188248209475
Iteration 303/10000: Training Loss = 5168.2116874540025
Validation Loss = 5168.2116874540025
Iteration 304/10000: Training Loss = 5166.253155306892
Validation Loss = 5166.253155306892
Iteration 305/10000: Training Loss = 5164.253948074269
Validation Loss = 5164.253948074269
Iteration 306/10000: Training Loss = 5162.263174705325
Validation Loss = 5162.263174705325
Iteration 307/10000: Training Loss = 5160.287660969932
Validation Loss = 5160.287660969932
Iteration 308/10000: Training Loss = 5158.317573716928
Validation Loss = 5158.317573716928
Iteration 309/10000: Training Loss = 5156.289718127356
Validation Loss = 5156.289718127356
Iteration 310/10000: Training Loss = 5154.331835712566
Validation Loss = 5154.331835712566
Iteration 311/10000: Training Loss = 5152.338356888386
Validation Loss = 5152.338356888386
Iteration 312/10000: Training Loss = 5150.40383250753
Validation Loss = 5150.40383250753
Iteration 313/10000: Training Loss = 5148.448182208985
Validation Loss = 5148.448182208985
Iteration 314/10000: Training Loss = 5146.491037100353
Validation Loss = 5146.491037100353
Iteration 315/10000: Training Loss = 5144.541129528762
Validation Loss = 5144.541129528762
Iteration 316/10000: Training Loss = 5142.588566404034
Validation Loss = 5142.588566404034
Iteration 317/10000: Training Loss = 5140.646223043609
Validation Loss = 5140.646223043609
Iteration 318/10000: Training Loss = 5138.696413684577
Validation Loss = 5138.696413684577
Iteration 319/10000: Training Loss = 5136.762018721213
Validation Loss = 5136.762018721213
Iteration 320/10000: Training Loss = 5134.8099895088635
Validation Loss = 5134.8099895088635
Iteration 321/10000: Training Loss = 5132.870673326559
Validation Loss = 5132.870673326559
Iteration 322/10000: Training Loss = 5130.917607662796
Validation Loss = 5130.917607662796
Iteration 323/10000: Training Loss = 5128.983761512385
Validation Loss = 5128.983761512385
Iteration 324/10000: Training Loss = 5127.022324835074
Validation Loss = 5127.022324835074
Iteration 325/10000: Training Loss = 5125.089538468266
Validation Loss = 5125.089538468266
Iteration 326/10000: Training Loss = 5123.170191992123
Validation Loss = 5123.170191992123
Iteration 327/10000: Training Loss = 5121.256574009678
Validation Loss = 5121.256574009678
Iteration 328/10000: Training Loss = 5119.309073526981
Validation Loss = 5119.309073526981
Iteration 329/10000: Training Loss = 5117.357816938213
Validation Loss = 5117.357816938213
Iteration 330/10000: Training Loss = 5115.441647203852
Validation Loss = 5115.441647203852
Iteration 331/10000: Training Loss = 5113.533590268129
Validation Loss = 5113.533590268129
Iteration 332/10000: Training Loss = 5111.611695675556
Validation Loss = 5111.611695675556
Iteration 333/10000: Training Loss = 5109.69791861566
Validation Loss = 5109.69791861566
Iteration 334/10000: Training Loss = 5107.794490497172
Validation Loss = 5107.794490497172
Iteration 335/10000: Training Loss = 5105.891623334531
Validation Loss = 5105.891623334531
Iteration 336/10000: Training Loss = 5103.973646380237
Validation Loss = 5103.973646380237
Iteration 337/10000: Training Loss = 5102.084130257764
Validation Loss = 5102.084130257764
Iteration 338/10000: Training Loss = 5100.149529414579
Validation Loss = 5100.149529414579
Iteration 339/10000: Training Loss = 5098.2426288226825
Validation Loss = 5098.2426288226825
Iteration 340/10000: Training Loss = 5096.354826083246
Validation Loss = 5096.354826083246
Iteration 341/10000: Training Loss = 5094.461761380814
Validation Loss = 5094.461761380814
Iteration 342/10000: Training Loss = 5092.584110727939
Validation Loss = 5092.584110727939
Iteration 343/10000: Training Loss = 5090.698726318751
Validation Loss = 5090.698726318751
Iteration 344/10000: Training Loss = 5088.767395192741
Validation Loss = 5088.767395192741
Iteration 345/10000: Training Loss = 5086.877682534427
Validation Loss = 5086.877682534427
Iteration 346/10000: Training Loss = 5085.013724857826
Validation Loss = 5085.013724857826
Iteration 347/10000: Training Loss = 5083.149029191977
Validation Loss = 5083.149029191977
Iteration 348/10000: Training Loss = 5081.267550359088
Validation Loss = 5081.267550359088
Iteration 349/10000: Training Loss = 5079.41754396036
Validation Loss = 5079.41754396036
Iteration 350/10000: Training Loss = 5077.530631529834
Validation Loss = 5077.530631529834
Iteration 351/10000: Training Loss = 5075.632323458273
Validation Loss = 5075.632323458273
Iteration 352/10000: Training Loss = 5073.789062576781
Validation Loss = 5073.789062576781
Iteration 353/10000: Training Loss = 5071.926148702116
Validation Loss = 5071.926148702116
Iteration 354/10000: Training Loss = 5070.057866696606
Validation Loss = 5070.057866696606
Iteration 355/10000: Training Loss = 5068.186512082994
Validation Loss = 5068.186512082994
Iteration 356/10000: Training Loss = 5066.319662880512
Validation Loss = 5066.319662880512
Iteration 357/10000: Training Loss = 5064.439887554937
Validation Loss = 5064.439887554937
Iteration 358/10000: Training Loss = 5062.583698970822
Validation Loss = 5062.583698970822
Iteration 359/10000: Training Loss = 5060.734060485717
Validation Loss = 5060.734060485717
Iteration 360/10000: Training Loss = 5058.852533681847
Validation Loss = 5058.852533681847
Iteration 361/10000: Training Loss = 5057.022089751483
Validation Loss = 5057.022089751483
Iteration 362/10000: Training Loss = 5055.151422964048
Validation Loss = 5055.151422964048
Iteration 363/10000: Training Loss = 5053.316209568842
Validation Loss = 5053.316209568842
Iteration 364/10000: Training Loss = 5051.468353693361
Validation Loss = 5051.468353693361
Iteration 365/10000: Training Loss = 5049.627031770237
Validation Loss = 5049.627031770237
Iteration 366/10000: Training Loss = 5047.78590288536
Validation Loss = 5047.78590288536
Iteration 367/10000: Training Loss = 5045.9395905247775
Validation Loss = 5045.9395905247775
Iteration 368/10000: Training Loss = 5044.089599258885
Validation Loss = 5044.089599258885
Iteration 369/10000: Training Loss = 5042.271245317734
Validation Loss = 5042.271245317734
Iteration 370/10000: Training Loss = 5040.427468309086
Validation Loss = 5040.427468309086
Iteration 371/10000: Training Loss = 5038.611283498675
Validation Loss = 5038.611283498675
Iteration 372/10000: Training Loss = 5036.834497501528
Validation Loss = 5036.834497501528
Iteration 373/10000: Training Loss = 5035.012721406137
Validation Loss = 5035.012721406137
Iteration 374/10000: Training Loss = 5033.173141712174
Validation Loss = 5033.173141712174
Iteration 375/10000: Training Loss = 5031.329850286917
Validation Loss = 5031.329850286917
Iteration 376/10000: Training Loss = 5029.520771679534
Validation Loss = 5029.520771679534
Iteration 377/10000: Training Loss = 5027.707106266079
Validation Loss = 5027.707106266079
Iteration 378/10000: Training Loss = 5025.897110821145
Validation Loss = 5025.897110821145
Iteration 379/10000: Training Loss = 5024.056527831504
Validation Loss = 5024.056527831504
Iteration 380/10000: Training Loss = 5022.2369951936125
Validation Loss = 5022.2369951936125
Iteration 381/10000: Training Loss = 5020.414739633617
Validation Loss = 5020.414739633617
Iteration 382/10000: Training Loss = 5018.62258569416
Validation Loss = 5018.62258569416
Iteration 383/10000: Training Loss = 5016.824994307396
Validation Loss = 5016.824994307396
Iteration 384/10000: Training Loss = 5015.023632505368
Validation Loss = 5015.023632505368
Iteration 385/10000: Training Loss = 5013.211099676832
Validation Loss = 5013.211099676832
Iteration 386/10000: Training Loss = 5011.434279589227
Validation Loss = 5011.434279589227
Iteration 387/10000: Training Loss = 5009.657083510381
Validation Loss = 5009.657083510381
Iteration 388/10000: Training Loss = 5007.881690207381
Validation Loss = 5007.881690207381
Iteration 389/10000: Training Loss = 5006.103272260252
Validation Loss = 5006.103272260252
Iteration 390/10000: Training Loss = 5004.3374226503865
Validation Loss = 5004.3374226503865
Iteration 391/10000: Training Loss = 5002.539811371518
Validation Loss = 5002.539811371518
Iteration 392/10000: Training Loss = 5000.73849432866
Validation Loss = 5000.73849432866
Iteration 393/10000: Training Loss = 4998.979649768221
Validation Loss = 4998.979649768221
Iteration 394/10000: Training Loss = 4997.169392606209
Validation Loss = 4997.169392606209
Iteration 395/10000: Training Loss = 4995.380814549313
Validation Loss = 4995.380814549313
Iteration 396/10000: Training Loss = 4993.601856112181
Validation Loss = 4993.601856112181
Iteration 397/10000: Training Loss = 4991.823401341861
Validation Loss = 4991.823401341861
Iteration 398/10000: Training Loss = 4990.018577607334
Validation Loss = 4990.018577607334
Iteration 399/10000: Training Loss = 4988.265611330433
Validation Loss = 4988.265611330433
Iteration 400/10000: Training Loss = 4986.513261377091
Validation Loss = 4986.513261377091
Iteration 401/10000: Training Loss = 4984.784994594845
Validation Loss = 4984.784994594845
Iteration 402/10000: Training Loss = 4983.023105937108
Validation Loss = 4983.023105937108
Iteration 403/10000: Training Loss = 4981.2851912128235
Validation Loss = 4981.2851912128235
Iteration 404/10000: Training Loss = 4979.533178776873
Validation Loss = 4979.533178776873
Iteration 405/10000: Training Loss = 4977.769946347039
Validation Loss = 4977.769946347039
Iteration 406/10000: Training Loss = 4976.006278700769
Validation Loss = 4976.006278700769
Iteration 407/10000: Training Loss = 4974.255445131244
Validation Loss = 4974.255445131244
Iteration 408/10000: Training Loss = 4972.5390532791835
Validation Loss = 4972.5390532791835
Iteration 409/10000: Training Loss = 4970.798321137893
Validation Loss = 4970.798321137893
Iteration 410/10000: Training Loss = 4969.0376649045975
Validation Loss = 4969.0376649045975
Iteration 411/10000: Training Loss = 4967.252586085053
Validation Loss = 4967.252586085053
Iteration 412/10000: Training Loss = 4965.512496929637
Validation Loss = 4965.512496929637
Iteration 413/10000: Training Loss = 4963.787274720316
Validation Loss = 4963.787274720316
Iteration 414/10000: Training Loss = 4962.062341631644
Validation Loss = 4962.062341631644
Iteration 415/10000: Training Loss = 4960.368474744009
Validation Loss = 4960.368474744009
Iteration 416/10000: Training Loss = 4958.637703003426
Validation Loss = 4958.637703003426
Iteration 417/10000: Training Loss = 4956.882110129635
Validation Loss = 4956.882110129635
Iteration 418/10000: Training Loss = 4955.160565545217
Validation Loss = 4955.160565545217
Iteration 419/10000: Training Loss = 4953.405229829882
Validation Loss = 4953.405229829882
Iteration 420/10000: Training Loss = 4951.6864363548075
Validation Loss = 4951.6864363548075
Iteration 421/10000: Training Loss = 4949.95426584406
Validation Loss = 4949.95426584406
Iteration 422/10000: Training Loss = 4948.242445298344
Validation Loss = 4948.242445298344
Iteration 423/10000: Training Loss = 4946.499236556297
Validation Loss = 4946.499236556297
Iteration 424/10000: Training Loss = 4944.743978213714
Validation Loss = 4944.743978213714
Iteration 425/10000: Training Loss = 4943.037325777892
Validation Loss = 4943.037325777892
Iteration 426/10000: Training Loss = 4941.317828066354
Validation Loss = 4941.317828066354
Iteration 427/10000: Training Loss = 4939.615596404705
Validation Loss = 4939.615596404705
Iteration 428/10000: Training Loss = 4937.909537939714
Validation Loss = 4937.909537939714
Iteration 429/10000: Training Loss = 4936.235796500326
Validation Loss = 4936.235796500326
Iteration 430/10000: Training Loss = 4934.521369392587
Validation Loss = 4934.521369392587
Iteration 431/10000: Training Loss = 4932.825949819366
Validation Loss = 4932.825949819366
Iteration 432/10000: Training Loss = 4931.143222007046
Validation Loss = 4931.143222007046
Iteration 433/10000: Training Loss = 4929.432103821596
Validation Loss = 4929.432103821596
Iteration 434/10000: Training Loss = 4927.741560783776
Validation Loss = 4927.741560783776
Iteration 435/10000: Training Loss = 4926.043456930565
Validation Loss = 4926.043456930565
Iteration 436/10000: Training Loss = 4924.351188300113
Validation Loss = 4924.351188300113
Iteration 437/10000: Training Loss = 4922.645007910035
Validation Loss = 4922.645007910035
Iteration 438/10000: Training Loss = 4920.937860372579
Validation Loss = 4920.937860372579
Iteration 439/10000: Training Loss = 4919.260055828155
Validation Loss = 4919.260055828155
Iteration 440/10000: Training Loss = 4917.558614194105
Validation Loss = 4917.558614194105
Iteration 441/10000: Training Loss = 4915.847690389836
Validation Loss = 4915.847690389836
Iteration 442/10000: Training Loss = 4914.1813888407105
Validation Loss = 4914.1813888407105
Iteration 443/10000: Training Loss = 4912.497546275405
Validation Loss = 4912.497546275405
Iteration 444/10000: Training Loss = 4910.809947663282
Validation Loss = 4910.809947663282
Iteration 445/10000: Training Loss = 4909.125355756797
Validation Loss = 4909.125355756797
Iteration 446/10000: Training Loss = 4907.465905811647
Validation Loss = 4907.465905811647
Iteration 447/10000: Training Loss = 4905.784126291592
Validation Loss = 4905.784126291592
Iteration 448/10000: Training Loss = 4904.113154369864
Validation Loss = 4904.113154369864
Iteration 449/10000: Training Loss = 4902.431643080153
Validation Loss = 4902.431643080153
Iteration 450/10000: Training Loss = 4900.774058899125
Validation Loss = 4900.774058899125
Iteration 451/10000: Training Loss = 4899.101504355094
Validation Loss = 4899.101504355094
Iteration 452/10000: Training Loss = 4897.444490837201
Validation Loss = 4897.444490837201
Iteration 453/10000: Training Loss = 4895.767553760066
Validation Loss = 4895.767553760066
Iteration 454/10000: Training Loss = 4894.116906310322
Validation Loss = 4894.116906310322
Iteration 455/10000: Training Loss = 4892.466771054933
Validation Loss = 4892.466771054933
Iteration 456/10000: Training Loss = 4890.828077841651
Validation Loss = 4890.828077841651
Iteration 457/10000: Training Loss = 4889.167713103537
Validation Loss = 4889.167713103537
Iteration 458/10000: Training Loss = 4887.525415775848
Validation Loss = 4887.525415775848
Iteration 459/10000: Training Loss = 4885.860498418192
Validation Loss = 4885.860498418192
Iteration 460/10000: Training Loss = 4884.216667613257
Validation Loss = 4884.216667613257
Iteration 461/10000: Training Loss = 4882.554926745127
Validation Loss = 4882.554926745127
Iteration 462/10000: Training Loss = 4880.911165831669
Validation Loss = 4880.911165831669
Iteration 463/10000: Training Loss = 4879.240542243917
Validation Loss = 4879.240542243917
Iteration 464/10000: Training Loss = 4877.600538113506
Validation Loss = 4877.600538113506
Iteration 465/10000: Training Loss = 4875.947024622531
Validation Loss = 4875.947024622531
Iteration 466/10000: Training Loss = 4874.299630032701
Validation Loss = 4874.299630032701
Iteration 467/10000: Training Loss = 4872.662163580722
Validation Loss = 4872.662163580722
Iteration 468/10000: Training Loss = 4871.025370609692
Validation Loss = 4871.025370609692
Iteration 469/10000: Training Loss = 4869.398055969188
Validation Loss = 4869.398055969188
Iteration 470/10000: Training Loss = 4867.764544205635
Validation Loss = 4867.764544205635
Iteration 471/10000: Training Loss = 4866.145384196933
Validation Loss = 4866.145384196933
Iteration 472/10000: Training Loss = 4864.52460584987
Validation Loss = 4864.52460584987
Iteration 473/10000: Training Loss = 4862.897030866768
Validation Loss = 4862.897030866768
Iteration 474/10000: Training Loss = 4861.282666519927
Validation Loss = 4861.282666519927
Iteration 475/10000: Training Loss = 4859.64936787456
Validation Loss = 4859.64936787456
Iteration 476/10000: Training Loss = 4858.024160459164
Validation Loss = 4858.024160459164
Iteration 477/10000: Training Loss = 4856.413505034891
Validation Loss = 4856.413505034891
Iteration 478/10000: Training Loss = 4854.788598346311
Validation Loss = 4854.788598346311
Iteration 479/10000: Training Loss = 4853.172148167266
Validation Loss = 4853.172148167266
Iteration 480/10000: Training Loss = 4851.540782146547
Validation Loss = 4851.540782146547
Iteration 481/10000: Training Loss = 4849.914382453562
Validation Loss = 4849.914382453562
Iteration 482/10000: Training Loss = 4848.318906308479
Validation Loss = 4848.318906308479
Iteration 483/10000: Training Loss = 4846.712224858183
Validation Loss = 4846.712224858183
Iteration 484/10000: Training Loss = 4845.122410731028
Validation Loss = 4845.122410731028
Iteration 485/10000: Training Loss = 4843.498056163956
Validation Loss = 4843.498056163956
Iteration 486/10000: Training Loss = 4841.894498149026
Validation Loss = 4841.894498149026
Iteration 487/10000: Training Loss = 4840.313243295827
Validation Loss = 4840.313243295827
Iteration 488/10000: Training Loss = 4838.714361089257
Validation Loss = 4838.714361089257
Iteration 489/10000: Training Loss = 4837.124766833808
Validation Loss = 4837.124766833808
Iteration 490/10000: Training Loss = 4835.523793423644
Validation Loss = 4835.523793423644
Iteration 491/10000: Training Loss = 4833.928806024741
Validation Loss = 4833.928806024741
Iteration 492/10000: Training Loss = 4832.325470647126
Validation Loss = 4832.325470647126
Iteration 493/10000: Training Loss = 4830.7417147310325
Validation Loss = 4830.7417147310325
Iteration 494/10000: Training Loss = 4829.159781066347
Validation Loss = 4829.159781066347
Iteration 495/10000: Training Loss = 4827.576596619728
Validation Loss = 4827.576596619728
Iteration 496/10000: Training Loss = 4825.991786208808
Validation Loss = 4825.991786208808
Iteration 497/10000: Training Loss = 4824.424962762504
Validation Loss = 4824.424962762504
Iteration 498/10000: Training Loss = 4822.8597492456365
Validation Loss = 4822.8597492456365
Iteration 499/10000: Training Loss = 4821.27737961651
Validation Loss = 4821.27737961651
Iteration 500/10000: Training Loss = 4819.682842621562
Validation Loss = 4819.682842621562
Iteration 501/10000: Training Loss = 4818.112086266822
Validation Loss = 4818.112086266822
Iteration 502/10000: Training Loss = 4816.55915542132
Validation Loss = 4816.55915542132
Iteration 503/10000: Training Loss = 4814.965122450623
Validation Loss = 4814.965122450623
Iteration 504/10000: Training Loss = 4813.416214070072
Validation Loss = 4813.416214070072
Iteration 505/10000: Training Loss = 4811.855147883928
Validation Loss = 4811.855147883928
Iteration 506/10000: Training Loss = 4810.28632517415
Validation Loss = 4810.28632517415
Iteration 507/10000: Training Loss = 4808.733475830422
Validation Loss = 4808.733475830422
Iteration 508/10000: Training Loss = 4807.164944490764
Validation Loss = 4807.164944490764
Iteration 509/10000: Training Loss = 4805.612536168705
Validation Loss = 4805.612536168705
Iteration 510/10000: Training Loss = 4804.069225477671
Validation Loss = 4804.069225477671
Iteration 511/10000: Training Loss = 4802.54165469255
Validation Loss = 4802.54165469255
Iteration 512/10000: Training Loss = 4800.954439391043
Validation Loss = 4800.954439391043
Iteration 513/10000: Training Loss = 4799.395536271386
Validation Loss = 4799.395536271386
Iteration 514/10000: Training Loss = 4797.840023177425
Validation Loss = 4797.840023177425
Iteration 515/10000: Training Loss = 4796.262349375667
Validation Loss = 4796.262349375667
Iteration 516/10000: Training Loss = 4794.726630633633
Validation Loss = 4794.726630633633
Iteration 517/10000: Training Loss = 4793.182256302764
Validation Loss = 4793.182256302764
Iteration 518/10000: Training Loss = 4791.638546028697
Validation Loss = 4791.638546028697
Iteration 519/10000: Training Loss = 4790.072666541247
Validation Loss = 4790.072666541247
Iteration 520/10000: Training Loss = 4788.524588154864
Validation Loss = 4788.524588154864
Iteration 521/10000: Training Loss = 4786.995781731248
Validation Loss = 4786.995781731248
Iteration 522/10000: Training Loss = 4785.449514108993
Validation Loss = 4785.449514108993
Iteration 523/10000: Training Loss = 4783.915265930376
Validation Loss = 4783.915265930376
Iteration 524/10000: Training Loss = 4782.400718655405
Validation Loss = 4782.400718655405
Iteration 525/10000: Training Loss = 4780.874426378734
Validation Loss = 4780.874426378734
Iteration 526/10000: Training Loss = 4779.330566663813
Validation Loss = 4779.330566663813
Iteration 527/10000: Training Loss = 4777.806313310178
Validation Loss = 4777.806313310178
Iteration 528/10000: Training Loss = 4776.288568845104
Validation Loss = 4776.288568845104
Iteration 529/10000: Training Loss = 4774.7773720340565
Validation Loss = 4774.7773720340565
Iteration 530/10000: Training Loss = 4773.266070628262
Validation Loss = 4773.266070628262
Iteration 531/10000: Training Loss = 4771.742217992943
Validation Loss = 4771.742217992943
Iteration 532/10000: Training Loss = 4770.22918999545
Validation Loss = 4770.22918999545
Iteration 533/10000: Training Loss = 4768.716457759069
Validation Loss = 4768.716457759069
Iteration 534/10000: Training Loss = 4767.2002476954485
Validation Loss = 4767.2002476954485
Iteration 535/10000: Training Loss = 4765.7186003229235
Validation Loss = 4765.7186003229235
Iteration 536/10000: Training Loss = 4764.192224076255
Validation Loss = 4764.192224076255
Iteration 537/10000: Training Loss = 4762.681976739169
Validation Loss = 4762.681976739169
Iteration 538/10000: Training Loss = 4761.194945652857
Validation Loss = 4761.194945652857
Iteration 539/10000: Training Loss = 4759.698859569291
Validation Loss = 4759.698859569291
Iteration 540/10000: Training Loss = 4758.198026521688
Validation Loss = 4758.198026521688
Iteration 541/10000: Training Loss = 4756.694039328351
Validation Loss = 4756.694039328351
Iteration 542/10000: Training Loss = 4755.211272906484
Validation Loss = 4755.211272906484
Iteration 543/10000: Training Loss = 4753.716459925555
Validation Loss = 4753.716459925555
Iteration 544/10000: Training Loss = 4752.216170475487
Validation Loss = 4752.216170475487
Iteration 545/10000: Training Loss = 4750.735815835438
Validation Loss = 4750.735815835438
Iteration 546/10000: Training Loss = 4749.224285508097
Validation Loss = 4749.224285508097
Iteration 547/10000: Training Loss = 4747.741909904606
Validation Loss = 4747.741909904606
Iteration 548/10000: Training Loss = 4746.266950435696
Validation Loss = 4746.266950435696
Iteration 549/10000: Training Loss = 4744.785175884142
Validation Loss = 4744.785175884142
Iteration 550/10000: Training Loss = 4743.308638779795
Validation Loss = 4743.308638779795
Iteration 551/10000: Training Loss = 4741.825149752651
Validation Loss = 4741.825149752651
Iteration 552/10000: Training Loss = 4740.33658209855
Validation Loss = 4740.33658209855
Iteration 553/10000: Training Loss = 4738.861979992725
Validation Loss = 4738.861979992725
Iteration 554/10000: Training Loss = 4737.378979636985
Validation Loss = 4737.378979636985
Iteration 555/10000: Training Loss = 4735.897861830058
Validation Loss = 4735.897861830058
Iteration 556/10000: Training Loss = 4734.43294786765
Validation Loss = 4734.43294786765
Iteration 557/10000: Training Loss = 4732.965492142266
Validation Loss = 4732.965492142266
Iteration 558/10000: Training Loss = 4731.503840515354
Validation Loss = 4731.503840515354
Iteration 559/10000: Training Loss = 4730.036750920004
Validation Loss = 4730.036750920004
Iteration 560/10000: Training Loss = 4728.559985706737
Validation Loss = 4728.559985706737
Iteration 561/10000: Training Loss = 4727.084187025487
Validation Loss = 4727.084187025487
Iteration 562/10000: Training Loss = 4725.601536145712
Validation Loss = 4725.601536145712
Iteration 563/10000: Training Loss = 4724.1539960254095
Validation Loss = 4724.1539960254095
Iteration 564/10000: Training Loss = 4722.686795309913
Validation Loss = 4722.686795309913
Iteration 565/10000: Training Loss = 4721.213554578185
Validation Loss = 4721.213554578185
Iteration 566/10000: Training Loss = 4719.757675537189
Validation Loss = 4719.757675537189
Iteration 567/10000: Training Loss = 4718.2974455404055
Validation Loss = 4718.2974455404055
Iteration 568/10000: Training Loss = 4716.864956690276
Validation Loss = 4716.864956690276
Iteration 569/10000: Training Loss = 4715.430589566346
Validation Loss = 4715.430589566346
Iteration 570/10000: Training Loss = 4713.958514495262
Validation Loss = 4713.958514495262
Iteration 571/10000: Training Loss = 4712.503869225404
Validation Loss = 4712.503869225404
Iteration 572/10000: Training Loss = 4711.043850097765
Validation Loss = 4711.043850097765
Iteration 573/10000: Training Loss = 4709.579092703164
Validation Loss = 4709.579092703164
Iteration 574/10000: Training Loss = 4708.111160176958
Validation Loss = 4708.111160176958
Iteration 575/10000: Training Loss = 4706.653750270923
Validation Loss = 4706.653750270923
Iteration 576/10000: Training Loss = 4705.215229641798
Validation Loss = 4705.215229641798
Iteration 577/10000: Training Loss = 4703.779251494569
Validation Loss = 4703.779251494569
Iteration 578/10000: Training Loss = 4702.337898724332
Validation Loss = 4702.337898724332
Iteration 579/10000: Training Loss = 4700.905120814804
Validation Loss = 4700.905120814804
Iteration 580/10000: Training Loss = 4699.45416513733
Validation Loss = 4699.45416513733
Iteration 581/10000: Training Loss = 4698.033429184149
Validation Loss = 4698.033429184149
Iteration 582/10000: Training Loss = 4696.589697158849
Validation Loss = 4696.589697158849
Iteration 583/10000: Training Loss = 4695.171949197973
Validation Loss = 4695.171949197973
Iteration 584/10000: Training Loss = 4693.749759920004
Validation Loss = 4693.749759920004
Iteration 585/10000: Training Loss = 4692.337816189842
Validation Loss = 4692.337816189842
Iteration 586/10000: Training Loss = 4690.92914407839
Validation Loss = 4690.92914407839
Iteration 587/10000: Training Loss = 4689.486789423137
Validation Loss = 4689.486789423137
Iteration 588/10000: Training Loss = 4688.035394806531
Validation Loss = 4688.035394806531
Iteration 589/10000: Training Loss = 4686.603374207843
Validation Loss = 4686.603374207843
Iteration 590/10000: Training Loss = 4685.182155728895
Validation Loss = 4685.182155728895
Iteration 591/10000: Training Loss = 4683.747343501198
Validation Loss = 4683.747343501198
Iteration 592/10000: Training Loss = 4682.3537589181105
Validation Loss = 4682.3537589181105
Iteration 593/10000: Training Loss = 4680.949587452794
Validation Loss = 4680.949587452794
Iteration 594/10000: Training Loss = 4679.53759522262
Validation Loss = 4679.53759522262
Iteration 595/10000: Training Loss = 4678.127302339968
Validation Loss = 4678.127302339968
Iteration 596/10000: Training Loss = 4676.723914776773
Validation Loss = 4676.723914776773
Iteration 597/10000: Training Loss = 4675.30502547065
Validation Loss = 4675.30502547065
Iteration 598/10000: Training Loss = 4673.906134010254
Validation Loss = 4673.906134010254
Iteration 599/10000: Training Loss = 4672.492223122971
Validation Loss = 4672.492223122971
Iteration 600/10000: Training Loss = 4671.091059447062
Validation Loss = 4671.091059447062
Iteration 601/10000: Training Loss = 4669.6792154667655
Validation Loss = 4669.6792154667655
Iteration 602/10000: Training Loss = 4668.293496061128
Validation Loss = 4668.293496061128
Iteration 603/10000: Training Loss = 4666.909660861993
Validation Loss = 4666.909660861993
Iteration 604/10000: Training Loss = 4665.501639036884
Validation Loss = 4665.501639036884
Iteration 605/10000: Training Loss = 4664.119607765548
Validation Loss = 4664.119607765548
Iteration 606/10000: Training Loss = 4662.724703729096
Validation Loss = 4662.724703729096
Iteration 607/10000: Training Loss = 4661.329639981316
Validation Loss = 4661.329639981316
Iteration 608/10000: Training Loss = 4659.9210647415075
Validation Loss = 4659.9210647415075
Iteration 609/10000: Training Loss = 4658.515452322835
Validation Loss = 4658.515452322835
Iteration 610/10000: Training Loss = 4657.134176427316
Validation Loss = 4657.134176427316
Iteration 611/10000: Training Loss = 4655.747766868217
Validation Loss = 4655.747766868217
Iteration 612/10000: Training Loss = 4654.368200848406
Validation Loss = 4654.368200848406
Iteration 613/10000: Training Loss = 4652.988562666501
Validation Loss = 4652.988562666501
Iteration 614/10000: Training Loss = 4651.62738718831
Validation Loss = 4651.62738718831
Iteration 615/10000: Training Loss = 4650.243989901789
Validation Loss = 4650.243989901789
Iteration 616/10000: Training Loss = 4648.869360852363
Validation Loss = 4648.869360852363
Iteration 617/10000: Training Loss = 4647.477021817041
Validation Loss = 4647.477021817041
Iteration 618/10000: Training Loss = 4646.087426599382
Validation Loss = 4646.087426599382
Iteration 619/10000: Training Loss = 4644.711348026851
Validation Loss = 4644.711348026851
Iteration 620/10000: Training Loss = 4643.365614268475
Validation Loss = 4643.365614268475
Iteration 621/10000: Training Loss = 4641.994746645641
Validation Loss = 4641.994746645641
Iteration 622/10000: Training Loss = 4640.620003234419
Validation Loss = 4640.620003234419
Iteration 623/10000: Training Loss = 4639.235708684958
Validation Loss = 4639.235708684958
Iteration 624/10000: Training Loss = 4637.863549567208
Validation Loss = 4637.863549567208
Iteration 625/10000: Training Loss = 4636.464358366191
Validation Loss = 4636.464358366191
Iteration 626/10000: Training Loss = 4635.091594410968
Validation Loss = 4635.091594410968
Iteration 627/10000: Training Loss = 4633.73688338739
Validation Loss = 4633.73688338739
Iteration 628/10000: Training Loss = 4632.376835531039
Validation Loss = 4632.376835531039
Iteration 629/10000: Training Loss = 4631.024777648276
Validation Loss = 4631.024777648276
Iteration 630/10000: Training Loss = 4629.668788119167
Validation Loss = 4629.668788119167
Iteration 631/10000: Training Loss = 4628.303009756741
Validation Loss = 4628.303009756741
Iteration 632/10000: Training Loss = 4626.946599824025
Validation Loss = 4626.946599824025
Iteration 633/10000: Training Loss = 4625.610214285824
Validation Loss = 4625.610214285824
Iteration 634/10000: Training Loss = 4624.278121300808
Validation Loss = 4624.278121300808
Iteration 635/10000: Training Loss = 4622.943032966564
Validation Loss = 4622.943032966564
Iteration 636/10000: Training Loss = 4621.590282700196
Validation Loss = 4621.590282700196
Iteration 637/10000: Training Loss = 4620.234939019471
Validation Loss = 4620.234939019471
Iteration 638/10000: Training Loss = 4618.889476398577
Validation Loss = 4618.889476398577
Iteration 639/10000: Training Loss = 4617.55691019928
Validation Loss = 4617.55691019928
Iteration 640/10000: Training Loss = 4616.209401925148
Validation Loss = 4616.209401925148
Iteration 641/10000: Training Loss = 4614.879958962898
Validation Loss = 4614.879958962898
Iteration 642/10000: Training Loss = 4613.560169493352
Validation Loss = 4613.560169493352
Iteration 643/10000: Training Loss = 4612.2508414295835
Validation Loss = 4612.2508414295835
Iteration 644/10000: Training Loss = 4610.908539272684
Validation Loss = 4610.908539272684
Iteration 645/10000: Training Loss = 4609.561070988487
Validation Loss = 4609.561070988487
Iteration 646/10000: Training Loss = 4608.241027532185
Validation Loss = 4608.241027532185
Iteration 647/10000: Training Loss = 4606.941952088651
Validation Loss = 4606.941952088651
Iteration 648/10000: Training Loss = 4605.638892451669
Validation Loss = 4605.638892451669
Iteration 649/10000: Training Loss = 4604.308664261363
Validation Loss = 4604.308664261363
Iteration 650/10000: Training Loss = 4602.968843330669
Validation Loss = 4602.968843330669
Iteration 651/10000: Training Loss = 4601.660467016793
Validation Loss = 4601.660467016793
Iteration 652/10000: Training Loss = 4600.341592518186
Validation Loss = 4600.341592518186
Iteration 653/10000: Training Loss = 4599.03225811505
Validation Loss = 4599.03225811505
Iteration 654/10000: Training Loss = 4597.730308282217
Validation Loss = 4597.730308282217
Iteration 655/10000: Training Loss = 4596.408657709979
Validation Loss = 4596.408657709979
Iteration 656/10000: Training Loss = 4595.086434099269
Validation Loss = 4595.086434099269
Iteration 657/10000: Training Loss = 4593.7611692162045
Validation Loss = 4593.7611692162045
Iteration 658/10000: Training Loss = 4592.452680809528
Validation Loss = 4592.452680809528
Iteration 659/10000: Training Loss = 4591.1645310333
Validation Loss = 4591.1645310333
Iteration 660/10000: Training Loss = 4589.850601211067
Validation Loss = 4589.850601211067
Iteration 661/10000: Training Loss = 4588.543733027346
Validation Loss = 4588.543733027346
Iteration 662/10000: Training Loss = 4587.241776737408
Validation Loss = 4587.241776737408
Iteration 663/10000: Training Loss = 4585.959342483002
Validation Loss = 4585.959342483002
Iteration 664/10000: Training Loss = 4584.6599973065195
Validation Loss = 4584.6599973065195
Iteration 665/10000: Training Loss = 4583.339514733409
Validation Loss = 4583.339514733409
Iteration 666/10000: Training Loss = 4582.049507035869
Validation Loss = 4582.049507035869
Iteration 667/10000: Training Loss = 4580.741931724671
Validation Loss = 4580.741931724671
Iteration 668/10000: Training Loss = 4579.436157345161
Validation Loss = 4579.436157345161
Iteration 669/10000: Training Loss = 4578.1249910733995
Validation Loss = 4578.1249910733995
Iteration 670/10000: Training Loss = 4576.827231310721
Validation Loss = 4576.827231310721
Iteration 671/10000: Training Loss = 4575.521769240794
Validation Loss = 4575.521769240794
Iteration 672/10000: Training Loss = 4574.2377805413025
Validation Loss = 4574.2377805413025
Iteration 673/10000: Training Loss = 4572.951744114296
Validation Loss = 4572.951744114296
Iteration 674/10000: Training Loss = 4571.657272031995
Validation Loss = 4571.657272031995
Iteration 675/10000: Training Loss = 4570.377355947292
Validation Loss = 4570.377355947292
Iteration 676/10000: Training Loss = 4569.117380859604
Validation Loss = 4569.117380859604
Iteration 677/10000: Training Loss = 4567.831719355387
Validation Loss = 4567.831719355387
Iteration 678/10000: Training Loss = 4566.5501355861015
Validation Loss = 4566.5501355861015
Iteration 679/10000: Training Loss = 4565.276179522211
Validation Loss = 4565.276179522211
Iteration 680/10000: Training Loss = 4563.980090247297
Validation Loss = 4563.980090247297
Iteration 681/10000: Training Loss = 4562.709807097841
Validation Loss = 4562.709807097841
Iteration 682/10000: Training Loss = 4561.44012120582
Validation Loss = 4561.44012120582
Iteration 683/10000: Training Loss = 4560.165588229828
Validation Loss = 4560.165588229828
Iteration 684/10000: Training Loss = 4558.8974717533965
Validation Loss = 4558.8974717533965
Iteration 685/10000: Training Loss = 4557.623234835137
Validation Loss = 4557.623234835137
Iteration 686/10000: Training Loss = 4556.324834362219
Validation Loss = 4556.324834362219
Iteration 687/10000: Training Loss = 4555.058498607339
Validation Loss = 4555.058498607339
Iteration 688/10000: Training Loss = 4553.795825921517
Validation Loss = 4553.795825921517
Iteration 689/10000: Training Loss = 4552.53085391136
Validation Loss = 4552.53085391136
Iteration 690/10000: Training Loss = 4551.260692848818
Validation Loss = 4551.260692848818
Iteration 691/10000: Training Loss = 4549.997168612787
Validation Loss = 4549.997168612787
Iteration 692/10000: Training Loss = 4548.734045515663
Validation Loss = 4548.734045515663
Iteration 693/10000: Training Loss = 4547.477434214544
Validation Loss = 4547.477434214544
Iteration 694/10000: Training Loss = 4546.233460250615
Validation Loss = 4546.233460250615
Iteration 695/10000: Training Loss = 4545.001109392166
Validation Loss = 4545.001109392166
Iteration 696/10000: Training Loss = 4543.76824227276
Validation Loss = 4543.76824227276
Iteration 697/10000: Training Loss = 4542.515841148531
Validation Loss = 4542.515841148531
Iteration 698/10000: Training Loss = 4541.259224389129
Validation Loss = 4541.259224389129
Iteration 699/10000: Training Loss = 4539.983078683405
Validation Loss = 4539.983078683405
Iteration 700/10000: Training Loss = 4538.752525945026
Validation Loss = 4538.752525945026
Iteration 701/10000: Training Loss = 4537.518291643222
Validation Loss = 4537.518291643222
Iteration 702/10000: Training Loss = 4536.262801633131
Validation Loss = 4536.262801633131
Iteration 703/10000: Training Loss = 4535.02828192854
Validation Loss = 4535.02828192854
Iteration 704/10000: Training Loss = 4533.787580097744
Validation Loss = 4533.787580097744
Iteration 705/10000: Training Loss = 4532.561274572564
Validation Loss = 4532.561274572564
Iteration 706/10000: Training Loss = 4531.303269662178
Validation Loss = 4531.303269662178
Iteration 707/10000: Training Loss = 4530.074770983141
Validation Loss = 4530.074770983141
Iteration 708/10000: Training Loss = 4528.815641206339
Validation Loss = 4528.815641206339
Iteration 709/10000: Training Loss = 4527.557484750454
Validation Loss = 4527.557484750454
Iteration 710/10000: Training Loss = 4526.323631361796
Validation Loss = 4526.323631361796
Iteration 711/10000: Training Loss = 4525.1066018700085
Validation Loss = 4525.1066018700085
Iteration 712/10000: Training Loss = 4523.862601231385
Validation Loss = 4523.862601231385
Iteration 713/10000: Training Loss = 4522.6322523378685
Validation Loss = 4522.6322523378685
Iteration 714/10000: Training Loss = 4521.445337610045
Validation Loss = 4521.445337610045
Iteration 715/10000: Training Loss = 4520.20509093821
Validation Loss = 4520.20509093821
Iteration 716/10000: Training Loss = 4518.9565510114435
Validation Loss = 4518.9565510114435
Iteration 717/10000: Training Loss = 4517.747345564834
Validation Loss = 4517.747345564834
Iteration 718/10000: Training Loss = 4516.518252234758
Validation Loss = 4516.518252234758
Iteration 719/10000: Training Loss = 4515.2939482582315
Validation Loss = 4515.2939482582315
Iteration 720/10000: Training Loss = 4514.084597021839
Validation Loss = 4514.084597021839
Iteration 721/10000: Training Loss = 4512.866436443542
Validation Loss = 4512.866436443542
Iteration 722/10000: Training Loss = 4511.6235268866
Validation Loss = 4511.6235268866
Iteration 723/10000: Training Loss = 4510.413146692349
Validation Loss = 4510.413146692349
Iteration 724/10000: Training Loss = 4509.181849556767
Validation Loss = 4509.181849556767
Iteration 725/10000: Training Loss = 4507.958378666797
Validation Loss = 4507.958378666797
Iteration 726/10000: Training Loss = 4506.741139815101
Validation Loss = 4506.741139815101
Iteration 727/10000: Training Loss = 4505.523717260021
Validation Loss = 4505.523717260021
Iteration 728/10000: Training Loss = 4504.2992217018855
Validation Loss = 4504.2992217018855
Iteration 729/10000: Training Loss = 4503.062188933359
Validation Loss = 4503.062188933359
Iteration 730/10000: Training Loss = 4501.867143835631
Validation Loss = 4501.867143835631
Iteration 731/10000: Training Loss = 4500.6736799092205
Validation Loss = 4500.6736799092205
Iteration 732/10000: Training Loss = 4499.469232378183
Validation Loss = 4499.469232378183
Iteration 733/10000: Training Loss = 4498.249265332087
Validation Loss = 4498.249265332087
Iteration 734/10000: Training Loss = 4497.044662573136
Validation Loss = 4497.044662573136
Iteration 735/10000: Training Loss = 4495.846964999309
Validation Loss = 4495.846964999309
Iteration 736/10000: Training Loss = 4494.644408765643
Validation Loss = 4494.644408765643
Iteration 737/10000: Training Loss = 4493.436534563277
Validation Loss = 4493.436534563277
Iteration 738/10000: Training Loss = 4492.240517679187
Validation Loss = 4492.240517679187
Iteration 739/10000: Training Loss = 4491.040851254387
Validation Loss = 4491.040851254387
Iteration 740/10000: Training Loss = 4489.845053611167
Validation Loss = 4489.845053611167
Iteration 741/10000: Training Loss = 4488.635292121494
Validation Loss = 4488.635292121494
Iteration 742/10000: Training Loss = 4487.421145757022
Validation Loss = 4487.421145757022
Iteration 743/10000: Training Loss = 4486.214435940953
Validation Loss = 4486.214435940953
Iteration 744/10000: Training Loss = 4485.027348984838
Validation Loss = 4485.027348984838
Iteration 745/10000: Training Loss = 4483.870525944166
Validation Loss = 4483.870525944166
Iteration 746/10000: Training Loss = 4482.667790175383
Validation Loss = 4482.667790175383
Iteration 747/10000: Training Loss = 4481.485625600294
Validation Loss = 4481.485625600294
Iteration 748/10000: Training Loss = 4480.307399122608
Validation Loss = 4480.307399122608
Iteration 749/10000: Training Loss = 4479.1153573444535
Validation Loss = 4479.1153573444535
Iteration 750/10000: Training Loss = 4477.9289181560225
Validation Loss = 4477.9289181560225
Iteration 751/10000: Training Loss = 4476.768049091668
Validation Loss = 4476.768049091668
Iteration 752/10000: Training Loss = 4475.56702740997
Validation Loss = 4475.56702740997
Iteration 753/10000: Training Loss = 4474.372686596191
Validation Loss = 4474.372686596191
Iteration 754/10000: Training Loss = 4473.182696778354
Validation Loss = 4473.182696778354
Iteration 755/10000: Training Loss = 4472.003913192915
Validation Loss = 4472.003913192915
Iteration 756/10000: Training Loss = 4470.826762712256
Validation Loss = 4470.826762712256
Iteration 757/10000: Training Loss = 4469.657579763005
Validation Loss = 4469.657579763005
Iteration 758/10000: Training Loss = 4468.4767546524645
Validation Loss = 4468.4767546524645
Iteration 759/10000: Training Loss = 4467.33221517619
Validation Loss = 4467.33221517619
Iteration 760/10000: Training Loss = 4466.164862932771
Validation Loss = 4466.164862932771
Iteration 761/10000: Training Loss = 4464.988707907866
Validation Loss = 4464.988707907866
Iteration 762/10000: Training Loss = 4463.831727171462
Validation Loss = 4463.831727171462
Iteration 763/10000: Training Loss = 4462.6727903163855
Validation Loss = 4462.6727903163855
Iteration 764/10000: Training Loss = 4461.508906363871
Validation Loss = 4461.508906363871
Iteration 765/10000: Training Loss = 4460.345901195352
Validation Loss = 4460.345901195352
Iteration 766/10000: Training Loss = 4459.174610140062
Validation Loss = 4459.174610140062
Iteration 767/10000: Training Loss = 4458.012312875506
Validation Loss = 4458.012312875506
Iteration 768/10000: Training Loss = 4456.8566356491465
Validation Loss = 4456.8566356491465
Iteration 769/10000: Training Loss = 4455.72067869037
Validation Loss = 4455.72067869037
Iteration 770/10000: Training Loss = 4454.556227235412
Validation Loss = 4454.556227235412
Iteration 771/10000: Training Loss = 4453.387845585828
Validation Loss = 4453.387845585828
Iteration 772/10000: Training Loss = 4452.236787192879
Validation Loss = 4452.236787192879
Iteration 773/10000: Training Loss = 4451.081068019529
Validation Loss = 4451.081068019529
Iteration 774/10000: Training Loss = 4449.944025271546
Validation Loss = 4449.944025271546
Iteration 775/10000: Training Loss = 4448.791851702552
Validation Loss = 4448.791851702552
Iteration 776/10000: Training Loss = 4447.644476796565
Validation Loss = 4447.644476796565
Iteration 777/10000: Training Loss = 4446.4979626693685
Validation Loss = 4446.4979626693685
Iteration 778/10000: Training Loss = 4445.362830918519
Validation Loss = 4445.362830918519
Iteration 779/10000: Training Loss = 4444.235126023576
Validation Loss = 4444.235126023576
Iteration 780/10000: Training Loss = 4443.105222475908
Validation Loss = 4443.105222475908
Iteration 781/10000: Training Loss = 4441.97642317555
Validation Loss = 4441.97642317555
Iteration 782/10000: Training Loss = 4440.814627139082
Validation Loss = 4440.814627139082
Iteration 783/10000: Training Loss = 4439.649417823291
Validation Loss = 4439.649417823291
Iteration 784/10000: Training Loss = 4438.5071404041455
Validation Loss = 4438.5071404041455
Iteration 785/10000: Training Loss = 4437.360886860346
Validation Loss = 4437.360886860346
Iteration 786/10000: Training Loss = 4436.221350134351
Validation Loss = 4436.221350134351
Iteration 787/10000: Training Loss = 4435.114481294689
Validation Loss = 4435.114481294689
Iteration 788/10000: Training Loss = 4433.985786318996
Validation Loss = 4433.985786318996
Iteration 789/10000: Training Loss = 4432.839260663956
Validation Loss = 4432.839260663956
Iteration 790/10000: Training Loss = 4431.705242698819
Validation Loss = 4431.705242698819
Iteration 791/10000: Training Loss = 4430.570495598156
Validation Loss = 4430.570495598156
Iteration 792/10000: Training Loss = 4429.4512441910365
Validation Loss = 4429.4512441910365
Iteration 793/10000: Training Loss = 4428.349082586992
Validation Loss = 4428.349082586992
Iteration 794/10000: Training Loss = 4427.244011322456
Validation Loss = 4427.244011322456
Iteration 795/10000: Training Loss = 4426.112560204865
Validation Loss = 4426.112560204865
Iteration 796/10000: Training Loss = 4424.996919898197
Validation Loss = 4424.996919898197
Iteration 797/10000: Training Loss = 4423.866242128138
Validation Loss = 4423.866242128138
Iteration 798/10000: Training Loss = 4422.736496922
Validation Loss = 4422.736496922
Iteration 799/10000: Training Loss = 4421.62649905484
Validation Loss = 4421.62649905484
Iteration 800/10000: Training Loss = 4420.507971106873
Validation Loss = 4420.507971106873
Iteration 801/10000: Training Loss = 4419.373347600533
Validation Loss = 4419.373347600533
Iteration 802/10000: Training Loss = 4418.247969697176
Validation Loss = 4418.247969697176
Iteration 803/10000: Training Loss = 4417.165393403082
Validation Loss = 4417.165393403082
Iteration 804/10000: Training Loss = 4416.048819194807
Validation Loss = 4416.048819194807
Iteration 805/10000: Training Loss = 4414.936182270509
Validation Loss = 4414.936182270509
Iteration 806/10000: Training Loss = 4413.819369138108
Validation Loss = 4413.819369138108
Iteration 807/10000: Training Loss = 4412.694487029921
Validation Loss = 4412.694487029921
Iteration 808/10000: Training Loss = 4411.592823029155
Validation Loss = 4411.592823029155
Iteration 809/10000: Training Loss = 4410.483598209548
Validation Loss = 4410.483598209548
Iteration 810/10000: Training Loss = 4409.37237136528
Validation Loss = 4409.37237136528
Iteration 811/10000: Training Loss = 4408.274274853735
Validation Loss = 4408.274274853735
Iteration 812/10000: Training Loss = 4407.169109915511
Validation Loss = 4407.169109915511
Iteration 813/10000: Training Loss = 4406.054930235648
Validation Loss = 4406.054930235648
Iteration 814/10000: Training Loss = 4404.956207319727
Validation Loss = 4404.956207319727
Iteration 815/10000: Training Loss = 4403.852186949307
Validation Loss = 4403.852186949307
Iteration 816/10000: Training Loss = 4402.750505018116
Validation Loss = 4402.750505018116
Iteration 817/10000: Training Loss = 4401.640401218252
Validation Loss = 4401.640401218252
Iteration 818/10000: Training Loss = 4400.542475920549
Validation Loss = 4400.542475920549
Iteration 819/10000: Training Loss = 4399.442194318107
Validation Loss = 4399.442194318107
Iteration 820/10000: Training Loss = 4398.351064906843
Validation Loss = 4398.351064906843
Iteration 821/10000: Training Loss = 4397.256091925252
Validation Loss = 4397.256091925252
Iteration 822/10000: Training Loss = 4396.1702576099015
Validation Loss = 4396.1702576099015
Iteration 823/10000: Training Loss = 4395.088030523993
Validation Loss = 4395.088030523993
Iteration 824/10000: Training Loss = 4394.002772591968
Validation Loss = 4394.002772591968
Iteration 825/10000: Training Loss = 4392.921164733641
Validation Loss = 4392.921164733641
Iteration 826/10000: Training Loss = 4391.8376737772305
Validation Loss = 4391.8376737772305
Iteration 827/10000: Training Loss = 4390.744051849019
Validation Loss = 4390.744051849019
Iteration 828/10000: Training Loss = 4389.642099941504
Validation Loss = 4389.642099941504
Iteration 829/10000: Training Loss = 4388.572947137388
Validation Loss = 4388.572947137388
Iteration 830/10000: Training Loss = 4387.477171567633
Validation Loss = 4387.477171567633
Iteration 831/10000: Training Loss = 4386.402461602275
Validation Loss = 4386.402461602275
Iteration 832/10000: Training Loss = 4385.32684857309
Validation Loss = 4385.32684857309
Iteration 833/10000: Training Loss = 4384.2431930607945
Validation Loss = 4384.2431930607945
Iteration 834/10000: Training Loss = 4383.205176030848
Validation Loss = 4383.205176030848
Iteration 835/10000: Training Loss = 4382.1352550000365
Validation Loss = 4382.1352550000365
Iteration 836/10000: Training Loss = 4381.063841393374
Validation Loss = 4381.063841393374
Iteration 837/10000: Training Loss = 4379.975462796922
Validation Loss = 4379.975462796922
Iteration 838/10000: Training Loss = 4378.9142161493655
Validation Loss = 4378.9142161493655
Iteration 839/10000: Training Loss = 4377.821482588018
Validation Loss = 4377.821482588018
Iteration 840/10000: Training Loss = 4376.761727074779
Validation Loss = 4376.761727074779
Iteration 841/10000: Training Loss = 4375.699468081751
Validation Loss = 4375.699468081751
Iteration 842/10000: Training Loss = 4374.6256444944665
Validation Loss = 4374.6256444944665
Iteration 843/10000: Training Loss = 4373.559517962201
Validation Loss = 4373.559517962201
Iteration 844/10000: Training Loss = 4372.4944885788545
Validation Loss = 4372.4944885788545
Iteration 845/10000: Training Loss = 4371.431362168545
Validation Loss = 4371.431362168545
Iteration 846/10000: Training Loss = 4370.353498163643
Validation Loss = 4370.353498163643
Iteration 847/10000: Training Loss = 4369.289150265051
Validation Loss = 4369.289150265051
Iteration 848/10000: Training Loss = 4368.215843442757
Validation Loss = 4368.215843442757
Iteration 849/10000: Training Loss = 4367.134823101335
Validation Loss = 4367.134823101335
Iteration 850/10000: Training Loss = 4366.064206957825
Validation Loss = 4366.064206957825
Iteration 851/10000: Training Loss = 4365.004405299225
Validation Loss = 4365.004405299225
Iteration 852/10000: Training Loss = 4363.948579786889
Validation Loss = 4363.948579786889
Iteration 853/10000: Training Loss = 4362.8955547608375
Validation Loss = 4362.8955547608375
Iteration 854/10000: Training Loss = 4361.843137838463
Validation Loss = 4361.843137838463
Iteration 855/10000: Training Loss = 4360.775628485555
Validation Loss = 4360.775628485555
Iteration 856/10000: Training Loss = 4359.711224249278
Validation Loss = 4359.711224249278
Iteration 857/10000: Training Loss = 4358.662640189777
Validation Loss = 4358.662640189777
Iteration 858/10000: Training Loss = 4357.6181964320085
Validation Loss = 4357.6181964320085
Iteration 859/10000: Training Loss = 4356.59040837586
Validation Loss = 4356.59040837586
Iteration 860/10000: Training Loss = 4355.559070986109
Validation Loss = 4355.559070986109
Iteration 861/10000: Training Loss = 4354.506394457831
Validation Loss = 4354.506394457831
Iteration 862/10000: Training Loss = 4353.458496844993
Validation Loss = 4353.458496844993
Iteration 863/10000: Training Loss = 4352.411567056279
Validation Loss = 4352.411567056279
Iteration 864/10000: Training Loss = 4351.374150079852
Validation Loss = 4351.374150079852
Iteration 865/10000: Training Loss = 4350.341666578261
Validation Loss = 4350.341666578261
Iteration 866/10000: Training Loss = 4349.285912433069
Validation Loss = 4349.285912433069
Iteration 867/10000: Training Loss = 4348.228994792221
Validation Loss = 4348.228994792221
Iteration 868/10000: Training Loss = 4347.170060586628
Validation Loss = 4347.170060586628
Iteration 869/10000: Training Loss = 4346.138386549908
Validation Loss = 4346.138386549908
Iteration 870/10000: Training Loss = 4345.108554927776
Validation Loss = 4345.108554927776
Iteration 871/10000: Training Loss = 4344.070466828348
Validation Loss = 4344.070466828348
Iteration 872/10000: Training Loss = 4343.025984069557
Validation Loss = 4343.025984069557
Iteration 873/10000: Training Loss = 4342.000387358602
Validation Loss = 4342.000387358602
Iteration 874/10000: Training Loss = 4340.965596894748
Validation Loss = 4340.965596894748
Iteration 875/10000: Training Loss = 4339.938487177389
Validation Loss = 4339.938487177389
Iteration 876/10000: Training Loss = 4338.918254446033
Validation Loss = 4338.918254446033
Iteration 877/10000: Training Loss = 4337.896658255498
Validation Loss = 4337.896658255498
Iteration 878/10000: Training Loss = 4336.859516903096
Validation Loss = 4336.859516903096
Iteration 879/10000: Training Loss = 4335.830427149548
Validation Loss = 4335.830427149548
Iteration 880/10000: Training Loss = 4334.799552260718
Validation Loss = 4334.799552260718
Iteration 881/10000: Training Loss = 4333.775280735314
Validation Loss = 4333.775280735314
Iteration 882/10000: Training Loss = 4332.755361158361
Validation Loss = 4332.755361158361
Iteration 883/10000: Training Loss = 4331.724531242712
Validation Loss = 4331.724531242712
Iteration 884/10000: Training Loss = 4330.711969430811
Validation Loss = 4330.711969430811
Iteration 885/10000: Training Loss = 4329.69434531745
Validation Loss = 4329.69434531745
Iteration 886/10000: Training Loss = 4328.676850260495
Validation Loss = 4328.676850260495
Iteration 887/10000: Training Loss = 4327.648937389564
Validation Loss = 4327.648937389564
Iteration 888/10000: Training Loss = 4326.633769264593
Validation Loss = 4326.633769264593
Iteration 889/10000: Training Loss = 4325.619782068835
Validation Loss = 4325.619782068835
Iteration 890/10000: Training Loss = 4324.60308085866
Validation Loss = 4324.60308085866
Iteration 891/10000: Training Loss = 4323.582910962807
Validation Loss = 4323.582910962807
Iteration 892/10000: Training Loss = 4322.575095898948
Validation Loss = 4322.575095898948
Iteration 893/10000: Training Loss = 4321.583729796154
Validation Loss = 4321.583729796154
Iteration 894/10000: Training Loss = 4320.549761698042
Validation Loss = 4320.549761698042
Iteration 895/10000: Training Loss = 4319.559323328402
Validation Loss = 4319.559323328402
Iteration 896/10000: Training Loss = 4318.554934810522
Validation Loss = 4318.554934810522
Iteration 897/10000: Training Loss = 4317.553433127505
Validation Loss = 4317.553433127505
Iteration 898/10000: Training Loss = 4316.539388189618
Validation Loss = 4316.539388189618
Iteration 899/10000: Training Loss = 4315.539559382699
Validation Loss = 4315.539559382699
Iteration 900/10000: Training Loss = 4314.550985095194
Validation Loss = 4314.550985095194
Iteration 901/10000: Training Loss = 4313.545014388344
Validation Loss = 4313.545014388344
Iteration 902/10000: Training Loss = 4312.548018073681
Validation Loss = 4312.548018073681
Iteration 903/10000: Training Loss = 4311.566166619736
Validation Loss = 4311.566166619736
Iteration 904/10000: Training Loss = 4310.580755286219
Validation Loss = 4310.580755286219
Iteration 905/10000: Training Loss = 4309.580205243149
Validation Loss = 4309.580205243149
Iteration 906/10000: Training Loss = 4308.567886165055
Validation Loss = 4308.567886165055
Iteration 907/10000: Training Loss = 4307.603455157585
Validation Loss = 4307.603455157585
Iteration 908/10000: Training Loss = 4306.60284531133
Validation Loss = 4306.60284531133
Iteration 909/10000: Training Loss = 4305.60268760718
Validation Loss = 4305.60268760718
Iteration 910/10000: Training Loss = 4304.6036211177025
Validation Loss = 4304.6036211177025
Iteration 911/10000: Training Loss = 4303.61216924198
Validation Loss = 4303.61216924198
Iteration 912/10000: Training Loss = 4302.619180302915
Validation Loss = 4302.619180302915
Iteration 913/10000: Training Loss = 4301.655321126251
Validation Loss = 4301.655321126251
Iteration 914/10000: Training Loss = 4300.661501506046
Validation Loss = 4300.661501506046
Iteration 915/10000: Training Loss = 4299.677082149675
Validation Loss = 4299.677082149675
Iteration 916/10000: Training Loss = 4298.6913025278045
Validation Loss = 4298.6913025278045
Iteration 917/10000: Training Loss = 4297.707384484053
Validation Loss = 4297.707384484053
Iteration 918/10000: Training Loss = 4296.7478499262215
Validation Loss = 4296.7478499262215
Iteration 919/10000: Training Loss = 4295.769487709998
Validation Loss = 4295.769487709998
Iteration 920/10000: Training Loss = 4294.799418935305
Validation Loss = 4294.799418935305
Iteration 921/10000: Training Loss = 4293.793778517012
Validation Loss = 4293.793778517012
Iteration 922/10000: Training Loss = 4292.820655783063
Validation Loss = 4292.820655783063
Iteration 923/10000: Training Loss = 4291.873748120111
Validation Loss = 4291.873748120111
Iteration 924/10000: Training Loss = 4290.877372980207
Validation Loss = 4290.877372980207
Iteration 925/10000: Training Loss = 4289.897293059637
Validation Loss = 4289.897293059637
Iteration 926/10000: Training Loss = 4288.917447258528
Validation Loss = 4288.917447258528
Iteration 927/10000: Training Loss = 4287.94894397053
Validation Loss = 4287.94894397053
Iteration 928/10000: Training Loss = 4286.973655306065
Validation Loss = 4286.973655306065
Iteration 929/10000: Training Loss = 4285.9953832538
Validation Loss = 4285.9953832538
Iteration 930/10000: Training Loss = 4285.019621559557
Validation Loss = 4285.019621559557
Iteration 931/10000: Training Loss = 4284.043914072564
Validation Loss = 4284.043914072564
Iteration 932/10000: Training Loss = 4283.072207888602
Validation Loss = 4283.072207888602
Iteration 933/10000: Training Loss = 4282.111124002021
Validation Loss = 4282.111124002021
Iteration 934/10000: Training Loss = 4281.150252663988
Validation Loss = 4281.150252663988
Iteration 935/10000: Training Loss = 4280.185809549534
Validation Loss = 4280.185809549534
Iteration 936/10000: Training Loss = 4279.209910335422
Validation Loss = 4279.209910335422
Iteration 937/10000: Training Loss = 4278.2250452674125
Validation Loss = 4278.2250452674125
Iteration 938/10000: Training Loss = 4277.273337736816
Validation Loss = 4277.273337736816
Iteration 939/10000: Training Loss = 4276.310597031334
Validation Loss = 4276.310597031334
Iteration 940/10000: Training Loss = 4275.355745909154
Validation Loss = 4275.355745909154
Iteration 941/10000: Training Loss = 4274.405304984735
Validation Loss = 4274.405304984735
Iteration 942/10000: Training Loss = 4273.437081510854
Validation Loss = 4273.437081510854
Iteration 943/10000: Training Loss = 4272.472640291025
Validation Loss = 4272.472640291025
Iteration 944/10000: Training Loss = 4271.532402450325
Validation Loss = 4271.532402450325
Iteration 945/10000: Training Loss = 4270.576685326144
Validation Loss = 4270.576685326144
Iteration 946/10000: Training Loss = 4269.621601388365
Validation Loss = 4269.621601388365
Iteration 947/10000: Training Loss = 4268.670338387972
Validation Loss = 4268.670338387972
Iteration 948/10000: Training Loss = 4267.724829436351
Validation Loss = 4267.724829436351
Iteration 949/10000: Training Loss = 4266.773320432836
Validation Loss = 4266.773320432836
Iteration 950/10000: Training Loss = 4265.8314858732265
Validation Loss = 4265.8314858732265
Iteration 951/10000: Training Loss = 4264.8767701640845
Validation Loss = 4264.8767701640845
Iteration 952/10000: Training Loss = 4263.930915849544
Validation Loss = 4263.930915849544
Iteration 953/10000: Training Loss = 4262.987944435701
Validation Loss = 4262.987944435701
Iteration 954/10000: Training Loss = 4262.045273028452
Validation Loss = 4262.045273028452
Iteration 955/10000: Training Loss = 4261.095998210503
Validation Loss = 4261.095998210503
Iteration 956/10000: Training Loss = 4260.14691597976
Validation Loss = 4260.14691597976
Iteration 957/10000: Training Loss = 4259.199346881779
Validation Loss = 4259.199346881779
Iteration 958/10000: Training Loss = 4258.259381237962
Validation Loss = 4258.259381237962
Iteration 959/10000: Training Loss = 4257.3209924693365
Validation Loss = 4257.3209924693365
Iteration 960/10000: Training Loss = 4256.38461603731
Validation Loss = 4256.38461603731
Iteration 961/10000: Training Loss = 4255.440265994049
Validation Loss = 4255.440265994049
Iteration 962/10000: Training Loss = 4254.497190907798
Validation Loss = 4254.497190907798
Iteration 963/10000: Training Loss = 4253.56524626763
Validation Loss = 4253.56524626763
Iteration 964/10000: Training Loss = 4252.626318065502
Validation Loss = 4252.626318065502
Iteration 965/10000: Training Loss = 4251.6922490689185
Validation Loss = 4251.6922490689185
Iteration 966/10000: Training Loss = 4250.75351253179
Validation Loss = 4250.75351253179
Iteration 967/10000: Training Loss = 4249.820877045622
Validation Loss = 4249.820877045622
Iteration 968/10000: Training Loss = 4248.8853564031815
Validation Loss = 4248.8853564031815
Iteration 969/10000: Training Loss = 4247.9552414692425
Validation Loss = 4247.9552414692425
Iteration 970/10000: Training Loss = 4247.032271024376
Validation Loss = 4247.032271024376
Iteration 971/10000: Training Loss = 4246.102482749352
Validation Loss = 4246.102482749352
Iteration 972/10000: Training Loss = 4245.171581904781
Validation Loss = 4245.171581904781
Iteration 973/10000: Training Loss = 4244.218930287083
Validation Loss = 4244.218930287083
Iteration 974/10000: Training Loss = 4243.289872153875
Validation Loss = 4243.289872153875
Iteration 975/10000: Training Loss = 4242.362285870974
Validation Loss = 4242.362285870974
Iteration 976/10000: Training Loss = 4241.4334313084455
Validation Loss = 4241.4334313084455
Iteration 977/10000: Training Loss = 4240.518011644382
Validation Loss = 4240.518011644382
Iteration 978/10000: Training Loss = 4239.605256710023
Validation Loss = 4239.605256710023
Iteration 979/10000: Training Loss = 4238.673476602209
Validation Loss = 4238.673476602209
Iteration 980/10000: Training Loss = 4237.744694835654
Validation Loss = 4237.744694835654
Iteration 981/10000: Training Loss = 4236.811514269794
Validation Loss = 4236.811514269794
Iteration 982/10000: Training Loss = 4235.884378024088
Validation Loss = 4235.884378024088
Iteration 983/10000: Training Loss = 4234.970743892496
Validation Loss = 4234.970743892496
Iteration 984/10000: Training Loss = 4234.060911699621
Validation Loss = 4234.060911699621
Iteration 985/10000: Training Loss = 4233.133270323441
Validation Loss = 4233.133270323441
Iteration 986/10000: Training Loss = 4232.204222088995
Validation Loss = 4232.204222088995
Iteration 987/10000: Training Loss = 4231.2882689849
Validation Loss = 4231.2882689849
Iteration 988/10000: Training Loss = 4230.3626254971605
Validation Loss = 4230.3626254971605
Iteration 989/10000: Training Loss = 4229.448102366309
Validation Loss = 4229.448102366309
Iteration 990/10000: Training Loss = 4228.535642120213
Validation Loss = 4228.535642120213
Iteration 991/10000: Training Loss = 4227.625337741099
Validation Loss = 4227.625337741099
Iteration 992/10000: Training Loss = 4226.730470240018
Validation Loss = 4226.730470240018
Iteration 993/10000: Training Loss = 4225.80603858294
Validation Loss = 4225.80603858294
Iteration 994/10000: Training Loss = 4224.898692675369
Validation Loss = 4224.898692675369
Iteration 995/10000: Training Loss = 4223.986387086767
Validation Loss = 4223.986387086767
Iteration 996/10000: Training Loss = 4223.087058235638
Validation Loss = 4223.087058235638
Iteration 997/10000: Training Loss = 4222.192244746499
Validation Loss = 4222.192244746499
Iteration 998/10000: Training Loss = 4221.293422900962
Validation Loss = 4221.293422900962
Iteration 999/10000: Training Loss = 4220.398149080979
Validation Loss = 4220.398149080979
Iteration 1000/10000: Training Loss = 4219.491088059606
Validation Loss = 4219.491088059606
Iteration 1001/10000: Training Loss = 4218.576276672772
Validation Loss = 4218.576276672772
Iteration 1002/10000: Training Loss = 4217.6799359341185
Validation Loss = 4217.6799359341185
Iteration 1003/10000: Training Loss = 4216.780413907587
Validation Loss = 4216.780413907587
Iteration 1004/10000: Training Loss = 4215.895294911537
Validation Loss = 4215.895294911537
Iteration 1005/10000: Training Loss = 4215.003153983272
Validation Loss = 4215.003153983272
Iteration 1006/10000: Training Loss = 4214.116425099296
Validation Loss = 4214.116425099296
Iteration 1007/10000: Training Loss = 4213.224519645865
Validation Loss = 4213.224519645865
Iteration 1008/10000: Training Loss = 4212.331727372667
Validation Loss = 4212.331727372667
Iteration 1009/10000: Training Loss = 4211.431389860671
Validation Loss = 4211.431389860671
Iteration 1010/10000: Training Loss = 4210.526658087641
Validation Loss = 4210.526658087641
Iteration 1011/10000: Training Loss = 4209.643085294834
Validation Loss = 4209.643085294834
Iteration 1012/10000: Training Loss = 4208.760051993545
Validation Loss = 4208.760051993545
Iteration 1013/10000: Training Loss = 4207.8680139236185
Validation Loss = 4207.8680139236185
Iteration 1014/10000: Training Loss = 4206.975199279078
Validation Loss = 4206.975199279078
Iteration 1015/10000: Training Loss = 4206.066389499749
Validation Loss = 4206.066389499749
Iteration 1016/10000: Training Loss = 4205.17376343482
Validation Loss = 4205.17376343482
Iteration 1017/10000: Training Loss = 4204.289791763562
Validation Loss = 4204.289791763562
Iteration 1018/10000: Training Loss = 4203.396904239681
Validation Loss = 4203.396904239681
Iteration 1019/10000: Training Loss = 4202.524854415858
Validation Loss = 4202.524854415858
Iteration 1020/10000: Training Loss = 4201.640471927983
Validation Loss = 4201.640471927983
Iteration 1021/10000: Training Loss = 4200.750282219372
Validation Loss = 4200.750282219372
Iteration 1022/10000: Training Loss = 4199.885383840093
Validation Loss = 4199.885383840093
Iteration 1023/10000: Training Loss = 4198.9781857681255
Validation Loss = 4198.9781857681255
Iteration 1024/10000: Training Loss = 4198.090550098962
Validation Loss = 4198.090550098962
Iteration 1025/10000: Training Loss = 4197.210421567611
Validation Loss = 4197.210421567611
Iteration 1026/10000: Training Loss = 4196.329958121489
Validation Loss = 4196.329958121489
Iteration 1027/10000: Training Loss = 4195.456317949401
Validation Loss = 4195.456317949401
Iteration 1028/10000: Training Loss = 4194.570015068664
Validation Loss = 4194.570015068664
Iteration 1029/10000: Training Loss = 4193.679650498794
Validation Loss = 4193.679650498794
Iteration 1030/10000: Training Loss = 4192.792644362446
Validation Loss = 4192.792644362446
Iteration 1031/10000: Training Loss = 4191.923788089726
Validation Loss = 4191.923788089726
Iteration 1032/10000: Training Loss = 4191.047175932442
Validation Loss = 4191.047175932442
Iteration 1033/10000: Training Loss = 4190.16639104635
Validation Loss = 4190.16639104635
Iteration 1034/10000: Training Loss = 4189.302352028056
Validation Loss = 4189.302352028056
Iteration 1035/10000: Training Loss = 4188.441094125555
Validation Loss = 4188.441094125555
Iteration 1036/10000: Training Loss = 4187.558516282756
Validation Loss = 4187.558516282756
Iteration 1037/10000: Training Loss = 4186.692062797512
Validation Loss = 4186.692062797512
Iteration 1038/10000: Training Loss = 4185.81748136431
Validation Loss = 4185.81748136431
Iteration 1039/10000: Training Loss = 4184.960433775089
Validation Loss = 4184.960433775089
Iteration 1040/10000: Training Loss = 4184.081918039473
Validation Loss = 4184.081918039473
Iteration 1041/10000: Training Loss = 4183.227898530716
Validation Loss = 4183.227898530716
Iteration 1042/10000: Training Loss = 4182.354742449944
Validation Loss = 4182.354742449944
Iteration 1043/10000: Training Loss = 4181.485182504856
Validation Loss = 4181.485182504856
Iteration 1044/10000: Training Loss = 4180.636156089887
Validation Loss = 4180.636156089887
Iteration 1045/10000: Training Loss = 4179.7818523669885
Validation Loss = 4179.7818523669885
Iteration 1046/10000: Training Loss = 4178.917249635946
Validation Loss = 4178.917249635946
Iteration 1047/10000: Training Loss = 4178.048340072528
Validation Loss = 4178.048340072528
Iteration 1048/10000: Training Loss = 4177.201871225363
Validation Loss = 4177.201871225363
Iteration 1049/10000: Training Loss = 4176.338561199134
Validation Loss = 4176.338561199134
Iteration 1050/10000: Training Loss = 4175.4750833618855
Validation Loss = 4175.4750833618855
Iteration 1051/10000: Training Loss = 4174.609594897663
Validation Loss = 4174.609594897663
Iteration 1052/10000: Training Loss = 4173.759487990735
Validation Loss = 4173.759487990735
Iteration 1053/10000: Training Loss = 4172.925679046034
Validation Loss = 4172.925679046034
Iteration 1054/10000: Training Loss = 4172.069174807878
Validation Loss = 4172.069174807878
Iteration 1055/10000: Training Loss = 4171.214623698636
Validation Loss = 4171.214623698636
Iteration 1056/10000: Training Loss = 4170.353147883247
Validation Loss = 4170.353147883247
Iteration 1057/10000: Training Loss = 4169.486647910317
Validation Loss = 4169.486647910317
Iteration 1058/10000: Training Loss = 4168.632763770088
Validation Loss = 4168.632763770088
Iteration 1059/10000: Training Loss = 4167.785613568268
Validation Loss = 4167.785613568268
Iteration 1060/10000: Training Loss = 4166.933961637959
Validation Loss = 4166.933961637959
Iteration 1061/10000: Training Loss = 4166.100233182719
Validation Loss = 4166.100233182719
Iteration 1062/10000: Training Loss = 4165.265841144469
Validation Loss = 4165.265841144469
Iteration 1063/10000: Training Loss = 4164.401955559802
Validation Loss = 4164.401955559802
Iteration 1064/10000: Training Loss = 4163.56678263472
Validation Loss = 4163.56678263472
Iteration 1065/10000: Training Loss = 4162.7167871471975
Validation Loss = 4162.7167871471975
Iteration 1066/10000: Training Loss = 4161.857990101859
Validation Loss = 4161.857990101859
Iteration 1067/10000: Training Loss = 4160.999170850205
Validation Loss = 4160.999170850205
Iteration 1068/10000: Training Loss = 4160.155762135452
Validation Loss = 4160.155762135452
Iteration 1069/10000: Training Loss = 4159.308007502647
Validation Loss = 4159.308007502647
Iteration 1070/10000: Training Loss = 4158.465632444449
Validation Loss = 4158.465632444449
Iteration 1071/10000: Training Loss = 4157.617370896992
Validation Loss = 4157.617370896992
Iteration 1072/10000: Training Loss = 4156.794966213507
Validation Loss = 4156.794966213507
Iteration 1073/10000: Training Loss = 4155.942773966111
Validation Loss = 4155.942773966111
Iteration 1074/10000: Training Loss = 4155.129956507688
Validation Loss = 4155.129956507688
Iteration 1075/10000: Training Loss = 4154.302759452614
Validation Loss = 4154.302759452614
Iteration 1076/10000: Training Loss = 4153.476224253241
Validation Loss = 4153.476224253241
Iteration 1077/10000: Training Loss = 4152.634862706999
Validation Loss = 4152.634862706999
Iteration 1078/10000: Training Loss = 4151.804602137648
Validation Loss = 4151.804602137648
Iteration 1079/10000: Training Loss = 4150.989089111485
Validation Loss = 4150.989089111485
Iteration 1080/10000: Training Loss = 4150.166347154057
Validation Loss = 4150.166347154057
Iteration 1081/10000: Training Loss = 4149.35856070913
Validation Loss = 4149.35856070913
Iteration 1082/10000: Training Loss = 4148.525730845221
Validation Loss = 4148.525730845221
Iteration 1083/10000: Training Loss = 4147.702491667879
Validation Loss = 4147.702491667879
Iteration 1084/10000: Training Loss = 4146.862262501461
Validation Loss = 4146.862262501461
Iteration 1085/10000: Training Loss = 4146.042271767936
Validation Loss = 4146.042271767936
Iteration 1086/10000: Training Loss = 4145.220246183505
Validation Loss = 4145.220246183505
Iteration 1087/10000: Training Loss = 4144.393073429591
Validation Loss = 4144.393073429591
Iteration 1088/10000: Training Loss = 4143.568880323858
Validation Loss = 4143.568880323858
Iteration 1089/10000: Training Loss = 4142.740723198645
Validation Loss = 4142.740723198645
Iteration 1090/10000: Training Loss = 4141.9327706398235
Validation Loss = 4141.9327706398235
Iteration 1091/10000: Training Loss = 4141.120734504628
Validation Loss = 4141.120734504628
Iteration 1092/10000: Training Loss = 4140.300869399372
Validation Loss = 4140.300869399372
Iteration 1093/10000: Training Loss = 4139.494504097446
Validation Loss = 4139.494504097446
Iteration 1094/10000: Training Loss = 4138.695308885568
Validation Loss = 4138.695308885568
Iteration 1095/10000: Training Loss = 4137.874368121203
Validation Loss = 4137.874368121203
Iteration 1096/10000: Training Loss = 4137.043762220046
Validation Loss = 4137.043762220046
Iteration 1097/10000: Training Loss = 4136.226121273828
Validation Loss = 4136.226121273828
Iteration 1098/10000: Training Loss = 4135.412151516792
Validation Loss = 4135.412151516792
Iteration 1099/10000: Training Loss = 4134.594883278972
Validation Loss = 4134.594883278972
Iteration 1100/10000: Training Loss = 4133.771609220418
Validation Loss = 4133.771609220418
Iteration 1101/10000: Training Loss = 4132.964164265171
Validation Loss = 4132.964164265171
Iteration 1102/10000: Training Loss = 4132.158936125832
Validation Loss = 4132.158936125832
Iteration 1103/10000: Training Loss = 4131.344857652745
Validation Loss = 4131.344857652745
Iteration 1104/10000: Training Loss = 4130.534725852345
Validation Loss = 4130.534725852345
Iteration 1105/10000: Training Loss = 4129.710005965374
Validation Loss = 4129.710005965374
Iteration 1106/10000: Training Loss = 4128.904706205418
Validation Loss = 4128.904706205418
Iteration 1107/10000: Training Loss = 4128.101059575472
Validation Loss = 4128.101059575472
Iteration 1108/10000: Training Loss = 4127.288657614531
Validation Loss = 4127.288657614531
Iteration 1109/10000: Training Loss = 4126.487599623343
Validation Loss = 4126.487599623343
Iteration 1110/10000: Training Loss = 4125.6777104124585
Validation Loss = 4125.6777104124585
Iteration 1111/10000: Training Loss = 4124.869485607229
Validation Loss = 4124.869485607229
Iteration 1112/10000: Training Loss = 4124.071194624949
Validation Loss = 4124.071194624949
Iteration 1113/10000: Training Loss = 4123.268382498103
Validation Loss = 4123.268382498103
Iteration 1114/10000: Training Loss = 4122.4705219873695
Validation Loss = 4122.4705219873695
Iteration 1115/10000: Training Loss = 4121.67426289772
Validation Loss = 4121.67426289772
Iteration 1116/10000: Training Loss = 4120.871963675867
Validation Loss = 4120.871963675867
Iteration 1117/10000: Training Loss = 4120.070949241819
Validation Loss = 4120.070949241819
Iteration 1118/10000: Training Loss = 4119.270884622149
Validation Loss = 4119.270884622149
Iteration 1119/10000: Training Loss = 4118.47562408726
Validation Loss = 4118.47562408726
Iteration 1120/10000: Training Loss = 4117.68586811543
Validation Loss = 4117.68586811543
Iteration 1121/10000: Training Loss = 4116.8965354734155
Validation Loss = 4116.8965354734155
Iteration 1122/10000: Training Loss = 4116.097779593454
Validation Loss = 4116.097779593454
Iteration 1123/10000: Training Loss = 4115.30522021881
Validation Loss = 4115.30522021881
Iteration 1124/10000: Training Loss = 4114.520401651808
Validation Loss = 4114.520401651808
Iteration 1125/10000: Training Loss = 4113.733377445862
Validation Loss = 4113.733377445862
Iteration 1126/10000: Training Loss = 4112.922506920174
Validation Loss = 4112.922506920174
Iteration 1127/10000: Training Loss = 4112.128510641056
Validation Loss = 4112.128510641056
Iteration 1128/10000: Training Loss = 4111.329455808961
Validation Loss = 4111.329455808961
Iteration 1129/10000: Training Loss = 4110.5405307393585
Validation Loss = 4110.5405307393585
Iteration 1130/10000: Training Loss = 4109.746784697457
Validation Loss = 4109.746784697457
Iteration 1131/10000: Training Loss = 4108.9545508037245
Validation Loss = 4108.9545508037245
Iteration 1132/10000: Training Loss = 4108.176250816237
Validation Loss = 4108.176250816237
Iteration 1133/10000: Training Loss = 4107.40866947876
Validation Loss = 4107.40866947876
Iteration 1134/10000: Training Loss = 4106.6437249759565
Validation Loss = 4106.6437249759565
Iteration 1135/10000: Training Loss = 4105.859550437762
Validation Loss = 4105.859550437762
Iteration 1136/10000: Training Loss = 4105.071819595178
Validation Loss = 4105.071819595178
Iteration 1137/10000: Training Loss = 4104.280648023085
Validation Loss = 4104.280648023085
Iteration 1138/10000: Training Loss = 4103.4705175166155
Validation Loss = 4103.4705175166155
Iteration 1139/10000: Training Loss = 4102.679128989478
Validation Loss = 4102.679128989478
Iteration 1140/10000: Training Loss = 4101.906951359393
Validation Loss = 4101.906951359393
Iteration 1141/10000: Training Loss = 4101.139346673105
Validation Loss = 4101.139346673105
Iteration 1142/10000: Training Loss = 4100.354435351122
Validation Loss = 4100.354435351122
Iteration 1143/10000: Training Loss = 4099.560285169225
Validation Loss = 4099.560285169225
Iteration 1144/10000: Training Loss = 4098.779871061629
Validation Loss = 4098.779871061629
Iteration 1145/10000: Training Loss = 4098.010040124423
Validation Loss = 4098.010040124423
Iteration 1146/10000: Training Loss = 4097.2227825227355
Validation Loss = 4097.2227825227355
Iteration 1147/10000: Training Loss = 4096.446519988084
Validation Loss = 4096.446519988084
Iteration 1148/10000: Training Loss = 4095.6688667640983
Validation Loss = 4095.6688667640983
Iteration 1149/10000: Training Loss = 4094.897920898583
Validation Loss = 4094.897920898583
Iteration 1150/10000: Training Loss = 4094.1249027846493
Validation Loss = 4094.1249027846493
Iteration 1151/10000: Training Loss = 4093.3627685082542
Validation Loss = 4093.3627685082542
Iteration 1152/10000: Training Loss = 4092.59144640785
Validation Loss = 4092.59144640785
Iteration 1153/10000: Training Loss = 4091.822973426516
Validation Loss = 4091.822973426516
Iteration 1154/10000: Training Loss = 4091.0489492273105
Validation Loss = 4091.0489492273105
Iteration 1155/10000: Training Loss = 4090.27679816278
Validation Loss = 4090.27679816278
Iteration 1156/10000: Training Loss = 4089.505628153222
Validation Loss = 4089.505628153222
Iteration 1157/10000: Training Loss = 4088.7294135299776
Validation Loss = 4088.7294135299776
Iteration 1158/10000: Training Loss = 4087.9703490809575
Validation Loss = 4087.9703490809575
Iteration 1159/10000: Training Loss = 4087.213419668062
Validation Loss = 4087.213419668062
Iteration 1160/10000: Training Loss = 4086.4501139410763
Validation Loss = 4086.4501139410763
Iteration 1161/10000: Training Loss = 4085.6793906672406
Validation Loss = 4085.6793906672406
Iteration 1162/10000: Training Loss = 4084.9161068063804
Validation Loss = 4084.9161068063804
Iteration 1163/10000: Training Loss = 4084.1502468213293
Validation Loss = 4084.1502468213293
Iteration 1164/10000: Training Loss = 4083.398422329369
Validation Loss = 4083.398422329369
Iteration 1165/10000: Training Loss = 4082.6432001057265
Validation Loss = 4082.6432001057265
Iteration 1166/10000: Training Loss = 4081.9005062874458
Validation Loss = 4081.9005062874458
Iteration 1167/10000: Training Loss = 4081.155501176876
Validation Loss = 4081.155501176876
Iteration 1168/10000: Training Loss = 4080.4019775210004
Validation Loss = 4080.4019775210004
Iteration 1169/10000: Training Loss = 4079.637172027741
Validation Loss = 4079.637172027741
Iteration 1170/10000: Training Loss = 4078.876128495133
Validation Loss = 4078.876128495133
Iteration 1171/10000: Training Loss = 4078.1232091782076
Validation Loss = 4078.1232091782076
Iteration 1172/10000: Training Loss = 4077.3701975020035
Validation Loss = 4077.3701975020035
Iteration 1173/10000: Training Loss = 4076.606703600872
Validation Loss = 4076.606703600872
Iteration 1174/10000: Training Loss = 4075.8511831902933
Validation Loss = 4075.8511831902933
Iteration 1175/10000: Training Loss = 4075.0954674104487
Validation Loss = 4075.0954674104487
Iteration 1176/10000: Training Loss = 4074.3330125984735
Validation Loss = 4074.3330125984735
Iteration 1177/10000: Training Loss = 4073.5821183688486
Validation Loss = 4073.5821183688486
Iteration 1178/10000: Training Loss = 4072.8329236356694
Validation Loss = 4072.8329236356694
Iteration 1179/10000: Training Loss = 4072.0838373907022
Validation Loss = 4072.0838373907022
Iteration 1180/10000: Training Loss = 4071.340943110093
Validation Loss = 4071.340943110093
Iteration 1181/10000: Training Loss = 4070.5956907264494
Validation Loss = 4070.5956907264494
Iteration 1182/10000: Training Loss = 4069.8473389497317
Validation Loss = 4069.8473389497317
Iteration 1183/10000: Training Loss = 4069.117324283934
Validation Loss = 4069.117324283934
Iteration 1184/10000: Training Loss = 4068.355710847486
Validation Loss = 4068.355710847486
Iteration 1185/10000: Training Loss = 4067.6092232917435
Validation Loss = 4067.6092232917435
Iteration 1186/10000: Training Loss = 4066.8662232833913
Validation Loss = 4066.8662232833913
Iteration 1187/10000: Training Loss = 4066.1242839837605
Validation Loss = 4066.1242839837605
Iteration 1188/10000: Training Loss = 4065.372886947042
Validation Loss = 4065.372886947042
Iteration 1189/10000: Training Loss = 4064.6441930805763
Validation Loss = 4064.6441930805763
Iteration 1190/10000: Training Loss = 4063.893498132063
Validation Loss = 4063.893498132063
Iteration 1191/10000: Training Loss = 4063.1675443767963
Validation Loss = 4063.1675443767963
Iteration 1192/10000: Training Loss = 4062.4353714111676
Validation Loss = 4062.4353714111676
Iteration 1193/10000: Training Loss = 4061.719566434765
Validation Loss = 4061.719566434765
Iteration 1194/10000: Training Loss = 4060.9770769634056
Validation Loss = 4060.9770769634056
Iteration 1195/10000: Training Loss = 4060.2377791034487
Validation Loss = 4060.2377791034487
Iteration 1196/10000: Training Loss = 4059.4939007407534
Validation Loss = 4059.4939007407534
Iteration 1197/10000: Training Loss = 4058.777521206607
Validation Loss = 4058.777521206607
Iteration 1198/10000: Training Loss = 4058.0228021001662
Validation Loss = 4058.0228021001662
Iteration 1199/10000: Training Loss = 4057.293353796042
Validation Loss = 4057.293353796042
Iteration 1200/10000: Training Loss = 4056.5550577736326
Validation Loss = 4056.5550577736326
Iteration 1201/10000: Training Loss = 4055.8384411719785
Validation Loss = 4055.8384411719785
Iteration 1202/10000: Training Loss = 4055.108420006394
Validation Loss = 4055.108420006394
Iteration 1203/10000: Training Loss = 4054.392625096176
Validation Loss = 4054.392625096176
Iteration 1204/10000: Training Loss = 4053.6449692561505
Validation Loss = 4053.6449692561505
Iteration 1205/10000: Training Loss = 4052.915447903901
Validation Loss = 4052.915447903901
Iteration 1206/10000: Training Loss = 4052.195458782486
Validation Loss = 4052.195458782486
Iteration 1207/10000: Training Loss = 4051.45569857085
Validation Loss = 4051.45569857085
Iteration 1208/10000: Training Loss = 4050.722118919553
Validation Loss = 4050.722118919553
Iteration 1209/10000: Training Loss = 4049.996005459105
Validation Loss = 4049.996005459105
Iteration 1210/10000: Training Loss = 4049.2769423687523
Validation Loss = 4049.2769423687523
Iteration 1211/10000: Training Loss = 4048.5502324726467
Validation Loss = 4048.5502324726467
Iteration 1212/10000: Training Loss = 4047.8355710665032
Validation Loss = 4047.8355710665032
Iteration 1213/10000: Training Loss = 4047.112844214799
Validation Loss = 4047.112844214799
Iteration 1214/10000: Training Loss = 4046.3871320455137
Validation Loss = 4046.3871320455137
Iteration 1215/10000: Training Loss = 4045.664287153431
Validation Loss = 4045.664287153431
Iteration 1216/10000: Training Loss = 4044.9483727318884
Validation Loss = 4044.9483727318884
Iteration 1217/10000: Training Loss = 4044.226561392705
Validation Loss = 4044.226561392705
Iteration 1218/10000: Training Loss = 4043.502425520561
Validation Loss = 4043.502425520561
Iteration 1219/10000: Training Loss = 4042.77422293703
Validation Loss = 4042.77422293703
Iteration 1220/10000: Training Loss = 4042.066421823206
Validation Loss = 4042.066421823206
Iteration 1221/10000: Training Loss = 4041.351095073306
Validation Loss = 4041.351095073306
Iteration 1222/10000: Training Loss = 4040.6394764649217
Validation Loss = 4040.6394764649217
Iteration 1223/10000: Training Loss = 4039.929906138143
Validation Loss = 4039.929906138143
Iteration 1224/10000: Training Loss = 4039.2255782971047
Validation Loss = 4039.2255782971047
Iteration 1225/10000: Training Loss = 4038.511446068844
Validation Loss = 4038.511446068844
Iteration 1226/10000: Training Loss = 4037.8048714437864
Validation Loss = 4037.8048714437864
Iteration 1227/10000: Training Loss = 4037.0909594860705
Validation Loss = 4037.0909594860705
Iteration 1228/10000: Training Loss = 4036.378643021386
Validation Loss = 4036.378643021386
Iteration 1229/10000: Training Loss = 4035.6664032271847
Validation Loss = 4035.6664032271847
Iteration 1230/10000: Training Loss = 4034.9593624077
Validation Loss = 4034.9593624077
Iteration 1231/10000: Training Loss = 4034.2581519749424
Validation Loss = 4034.2581519749424
Iteration 1232/10000: Training Loss = 4033.545010758573
Validation Loss = 4033.545010758573
Iteration 1233/10000: Training Loss = 4032.8300204950956
Validation Loss = 4032.8300204950956
Iteration 1234/10000: Training Loss = 4032.120857653121
Validation Loss = 4032.120857653121
Iteration 1235/10000: Training Loss = 4031.4245846992335
Validation Loss = 4031.4245846992335
Iteration 1236/10000: Training Loss = 4030.7143481824114
Validation Loss = 4030.7143481824114
Iteration 1237/10000: Training Loss = 4029.999882587332
Validation Loss = 4029.999882587332
Iteration 1238/10000: Training Loss = 4029.2944814384687
Validation Loss = 4029.2944814384687
Iteration 1239/10000: Training Loss = 4028.5888230115825
Validation Loss = 4028.5888230115825
Iteration 1240/10000: Training Loss = 4027.8681447772615
Validation Loss = 4027.8681447772615
Iteration 1241/10000: Training Loss = 4027.1578131633064
Validation Loss = 4027.1578131633064
Iteration 1242/10000: Training Loss = 4026.448109866664
Validation Loss = 4026.448109866664
Iteration 1243/10000: Training Loss = 4025.734751721609
Validation Loss = 4025.734751721609
Iteration 1244/10000: Training Loss = 4025.022724978575
Validation Loss = 4025.022724978575
Iteration 1245/10000: Training Loss = 4024.3232959805873
Validation Loss = 4024.3232959805873
Iteration 1246/10000: Training Loss = 4023.617973179085
Validation Loss = 4023.617973179085
Iteration 1247/10000: Training Loss = 4022.9170305253397
Validation Loss = 4022.9170305253397
Iteration 1248/10000: Training Loss = 4022.2179482716338
Validation Loss = 4022.2179482716338
Iteration 1249/10000: Training Loss = 4021.508431901324
Validation Loss = 4021.508431901324
Iteration 1250/10000: Training Loss = 4020.8007919043707
Validation Loss = 4020.8007919043707
Iteration 1251/10000: Training Loss = 4020.1023325243455
Validation Loss = 4020.1023325243455
Iteration 1252/10000: Training Loss = 4019.40986091386
Validation Loss = 4019.40986091386
Iteration 1253/10000: Training Loss = 4018.703479667742
Validation Loss = 4018.703479667742
Iteration 1254/10000: Training Loss = 4017.9988667886987
Validation Loss = 4017.9988667886987
Iteration 1255/10000: Training Loss = 4017.3208774043
Validation Loss = 4017.3208774043
Iteration 1256/10000: Training Loss = 4016.6163701646287
Validation Loss = 4016.6163701646287
Iteration 1257/10000: Training Loss = 4015.9223248188837
Validation Loss = 4015.9223248188837
Iteration 1258/10000: Training Loss = 4015.2310701715337
Validation Loss = 4015.2310701715337
Iteration 1259/10000: Training Loss = 4014.5331169841015
Validation Loss = 4014.5331169841015
Iteration 1260/10000: Training Loss = 4013.8451433496025
Validation Loss = 4013.8451433496025
Iteration 1261/10000: Training Loss = 4013.147754050442
Validation Loss = 4013.147754050442
Iteration 1262/10000: Training Loss = 4012.4524734350116
Validation Loss = 4012.4524734350116
Iteration 1263/10000: Training Loss = 4011.7667233167676
Validation Loss = 4011.7667233167676
Iteration 1264/10000: Training Loss = 4011.077239697136
Validation Loss = 4011.077239697136
Iteration 1265/10000: Training Loss = 4010.375340093806
Validation Loss = 4010.375340093806
Iteration 1266/10000: Training Loss = 4009.697544018948
Validation Loss = 4009.697544018948
Iteration 1267/10000: Training Loss = 4009.0108340739885
Validation Loss = 4009.0108340739885
Iteration 1268/10000: Training Loss = 4008.341576191826
Validation Loss = 4008.341576191826
Iteration 1269/10000: Training Loss = 4007.655136640926
Validation Loss = 4007.655136640926
Iteration 1270/10000: Training Loss = 4006.9519369560417
Validation Loss = 4006.9519369560417
Iteration 1271/10000: Training Loss = 4006.268516475518
Validation Loss = 4006.268516475518
Iteration 1272/10000: Training Loss = 4005.5907500411263
Validation Loss = 4005.5907500411263
Iteration 1273/10000: Training Loss = 4004.902793839876
Validation Loss = 4004.902793839876
Iteration 1274/10000: Training Loss = 4004.220415231821
Validation Loss = 4004.220415231821
Iteration 1275/10000: Training Loss = 4003.528645017595
Validation Loss = 4003.528645017595
Iteration 1276/10000: Training Loss = 4002.84060153088
Validation Loss = 4002.84060153088
Iteration 1277/10000: Training Loss = 4002.147645484802
Validation Loss = 4002.147645484802
Iteration 1278/10000: Training Loss = 4001.4646030640474
Validation Loss = 4001.4646030640474
Iteration 1279/10000: Training Loss = 4000.784561769467
Validation Loss = 4000.784561769467
Iteration 1280/10000: Training Loss = 4000.1002076968516
Validation Loss = 4000.1002076968516
Iteration 1281/10000: Training Loss = 3999.4156127608994
Validation Loss = 3999.4156127608994
Iteration 1282/10000: Training Loss = 3998.7385888629237
Validation Loss = 3998.7385888629237
Iteration 1283/10000: Training Loss = 3998.061843891612
Validation Loss = 3998.061843891612
Iteration 1284/10000: Training Loss = 3997.38056990355
Validation Loss = 3997.38056990355
Iteration 1285/10000: Training Loss = 3996.707172698796
Validation Loss = 3996.707172698796
Iteration 1286/10000: Training Loss = 3996.039820890533
Validation Loss = 3996.039820890533
Iteration 1287/10000: Training Loss = 3995.368300629125
Validation Loss = 3995.368300629125
Iteration 1288/10000: Training Loss = 3994.694397343754
Validation Loss = 3994.694397343754
Iteration 1289/10000: Training Loss = 3994.0205897138394
Validation Loss = 3994.0205897138394
Iteration 1290/10000: Training Loss = 3993.346785876438
Validation Loss = 3993.346785876438
Iteration 1291/10000: Training Loss = 3992.6708051797923
Validation Loss = 3992.6708051797923
Iteration 1292/10000: Training Loss = 3991.9992664658475
Validation Loss = 3991.9992664658475
Iteration 1293/10000: Training Loss = 3991.3383096445414
Validation Loss = 3991.3383096445414
Iteration 1294/10000: Training Loss = 3990.667385120949
Validation Loss = 3990.667385120949
Iteration 1295/10000: Training Loss = 3990.005687465459
Validation Loss = 3990.005687465459
Iteration 1296/10000: Training Loss = 3989.3433264430037
Validation Loss = 3989.3433264430037
Iteration 1297/10000: Training Loss = 3988.6654096535344
Validation Loss = 3988.6654096535344
Iteration 1298/10000: Training Loss = 3987.9997228022085
Validation Loss = 3987.9997228022085
Iteration 1299/10000: Training Loss = 3987.3366823261395
Validation Loss = 3987.3366823261395
Iteration 1300/10000: Training Loss = 3986.6802138254725
Validation Loss = 3986.6802138254725
Iteration 1301/10000: Training Loss = 3986.014228313724
Validation Loss = 3986.014228313724
Iteration 1302/10000: Training Loss = 3985.3604700030687
Validation Loss = 3985.3604700030687
Iteration 1303/10000: Training Loss = 3984.698146502198
Validation Loss = 3984.698146502198
Iteration 1304/10000: Training Loss = 3984.02712551328
Validation Loss = 3984.02712551328
Iteration 1305/10000: Training Loss = 3983.3694526686113
Validation Loss = 3983.3694526686113
Iteration 1306/10000: Training Loss = 3982.714701238134
Validation Loss = 3982.714701238134
Iteration 1307/10000: Training Loss = 3982.053466083737
Validation Loss = 3982.053466083737
Iteration 1308/10000: Training Loss = 3981.401623002897
Validation Loss = 3981.401623002897
Iteration 1309/10000: Training Loss = 3980.7293756465324
Validation Loss = 3980.7293756465324
Iteration 1310/10000: Training Loss = 3980.083670302353
Validation Loss = 3980.083670302353
Iteration 1311/10000: Training Loss = 3979.421234874785
Validation Loss = 3979.421234874785
Iteration 1312/10000: Training Loss = 3978.756281648971
Validation Loss = 3978.756281648971
Iteration 1313/10000: Training Loss = 3978.097050975716
Validation Loss = 3978.097050975716
Iteration 1314/10000: Training Loss = 3977.4394953502256
Validation Loss = 3977.4394953502256
Iteration 1315/10000: Training Loss = 3976.7750676702535
Validation Loss = 3976.7750676702535
Iteration 1316/10000: Training Loss = 3976.1191587370245
Validation Loss = 3976.1191587370245
Iteration 1317/10000: Training Loss = 3975.4601883438922
Validation Loss = 3975.4601883438922
Iteration 1318/10000: Training Loss = 3974.8057151949433
Validation Loss = 3974.8057151949433
Iteration 1319/10000: Training Loss = 3974.1536759798446
Validation Loss = 3974.1536759798446
Iteration 1320/10000: Training Loss = 3973.5047112137295
Validation Loss = 3973.5047112137295
Iteration 1321/10000: Training Loss = 3972.8564805072983
Validation Loss = 3972.8564805072983
Iteration 1322/10000: Training Loss = 3972.2025731621866
Validation Loss = 3972.2025731621866
Iteration 1323/10000: Training Loss = 3971.553751561883
Validation Loss = 3971.553751561883
Iteration 1324/10000: Training Loss = 3970.9018335513433
Validation Loss = 3970.9018335513433
Iteration 1325/10000: Training Loss = 3970.2621433647632
Validation Loss = 3970.2621433647632
Iteration 1326/10000: Training Loss = 3969.622415854724
Validation Loss = 3969.622415854724
Iteration 1327/10000: Training Loss = 3968.980853888322
Validation Loss = 3968.980853888322
Iteration 1328/10000: Training Loss = 3968.3299585283316
Validation Loss = 3968.3299585283316
Iteration 1329/10000: Training Loss = 3967.68566566454
Validation Loss = 3967.68566566454
Iteration 1330/10000: Training Loss = 3967.0375767803516
Validation Loss = 3967.0375767803516
Iteration 1331/10000: Training Loss = 3966.397450048678
Validation Loss = 3966.397450048678
Iteration 1332/10000: Training Loss = 3965.747637885581
Validation Loss = 3965.747637885581
Iteration 1333/10000: Training Loss = 3965.1132542830915
Validation Loss = 3965.1132542830915
Iteration 1334/10000: Training Loss = 3964.4620157700233
Validation Loss = 3964.4620157700233
Iteration 1335/10000: Training Loss = 3963.8105189567727
Validation Loss = 3963.8105189567727
Iteration 1336/10000: Training Loss = 3963.165274471903
Validation Loss = 3963.165274471903
Iteration 1337/10000: Training Loss = 3962.5380190744195
Validation Loss = 3962.5380190744195
Iteration 1338/10000: Training Loss = 3961.904453910787
Validation Loss = 3961.904453910787
Iteration 1339/10000: Training Loss = 3961.2647802424817
Validation Loss = 3961.2647802424817
Iteration 1340/10000: Training Loss = 3960.6315128368246
Validation Loss = 3960.6315128368246
Iteration 1341/10000: Training Loss = 3960.005244953956
Validation Loss = 3960.005244953956
Iteration 1342/10000: Training Loss = 3959.3505272197394
Validation Loss = 3959.3505272197394
Iteration 1343/10000: Training Loss = 3958.7213834000654
Validation Loss = 3958.7213834000654
Iteration 1344/10000: Training Loss = 3958.073830688847
Validation Loss = 3958.073830688847
Iteration 1345/10000: Training Loss = 3957.44830420669
Validation Loss = 3957.44830420669
Iteration 1346/10000: Training Loss = 3956.810142970981
Validation Loss = 3956.810142970981
Iteration 1347/10000: Training Loss = 3956.183487040416
Validation Loss = 3956.183487040416
Iteration 1348/10000: Training Loss = 3955.547318101995
Validation Loss = 3955.547318101995
Iteration 1349/10000: Training Loss = 3954.9116663063155
Validation Loss = 3954.9116663063155
Iteration 1350/10000: Training Loss = 3954.2759742543376
Validation Loss = 3954.2759742543376
Iteration 1351/10000: Training Loss = 3953.6414029398297
Validation Loss = 3953.6414029398297
Iteration 1352/10000: Training Loss = 3953.0136821365836
Validation Loss = 3953.0136821365836
Iteration 1353/10000: Training Loss = 3952.3797496624725
Validation Loss = 3952.3797496624725
Iteration 1354/10000: Training Loss = 3951.7544416521555
Validation Loss = 3951.7544416521555
Iteration 1355/10000: Training Loss = 3951.1225082159644
Validation Loss = 3951.1225082159644
Iteration 1356/10000: Training Loss = 3950.4890132776286
Validation Loss = 3950.4890132776286
Iteration 1357/10000: Training Loss = 3949.8675715010186
Validation Loss = 3949.8675715010186
Iteration 1358/10000: Training Loss = 3949.2382470357597
Validation Loss = 3949.2382470357597
Iteration 1359/10000: Training Loss = 3948.6121109147666
Validation Loss = 3948.6121109147666
Iteration 1360/10000: Training Loss = 3947.98986869761
Validation Loss = 3947.98986869761
Iteration 1361/10000: Training Loss = 3947.3689052098775
Validation Loss = 3947.3689052098775
Iteration 1362/10000: Training Loss = 3946.7521562874676
Validation Loss = 3946.7521562874676
Iteration 1363/10000: Training Loss = 3946.124560681146
Validation Loss = 3946.124560681146
Iteration 1364/10000: Training Loss = 3945.5007427204
Validation Loss = 3945.5007427204
Iteration 1365/10000: Training Loss = 3944.874613930039
Validation Loss = 3944.874613930039
Iteration 1366/10000: Training Loss = 3944.2547621010226
Validation Loss = 3944.2547621010226
Iteration 1367/10000: Training Loss = 3943.6273385988
Validation Loss = 3943.6273385988
Iteration 1368/10000: Training Loss = 3942.994406931048
Validation Loss = 3942.994406931048
Iteration 1369/10000: Training Loss = 3942.3726739270696
Validation Loss = 3942.3726739270696
Iteration 1370/10000: Training Loss = 3941.7363990699873
Validation Loss = 3941.7363990699873
Iteration 1371/10000: Training Loss = 3941.111409488375
Validation Loss = 3941.111409488375
Iteration 1372/10000: Training Loss = 3940.4957095347313
Validation Loss = 3940.4957095347313
Iteration 1373/10000: Training Loss = 3939.8697030245903
Validation Loss = 3939.8697030245903
Iteration 1374/10000: Training Loss = 3939.2608069435646
Validation Loss = 3939.2608069435646
Iteration 1375/10000: Training Loss = 3938.6314052164653
Validation Loss = 3938.6314052164653
Iteration 1376/10000: Training Loss = 3938.010582549146
Validation Loss = 3938.010582549146
Iteration 1377/10000: Training Loss = 3937.392210951458
Validation Loss = 3937.392210951458
Iteration 1378/10000: Training Loss = 3936.7750487059534
Validation Loss = 3936.7750487059534
Iteration 1379/10000: Training Loss = 3936.157304156092
Validation Loss = 3936.157304156092
Iteration 1380/10000: Training Loss = 3935.549460673151
Validation Loss = 3935.549460673151
Iteration 1381/10000: Training Loss = 3934.934586117086
Validation Loss = 3934.934586117086
Iteration 1382/10000: Training Loss = 3934.325860321855
Validation Loss = 3934.325860321855
Iteration 1383/10000: Training Loss = 3933.706993844401
Validation Loss = 3933.706993844401
Iteration 1384/10000: Training Loss = 3933.084143588023
Validation Loss = 3933.084143588023
Iteration 1385/10000: Training Loss = 3932.454955966156
Validation Loss = 3932.454955966156
Iteration 1386/10000: Training Loss = 3931.8640131356647
Validation Loss = 3931.8640131356647
Iteration 1387/10000: Training Loss = 3931.2440713041756
Validation Loss = 3931.2440713041756
Iteration 1388/10000: Training Loss = 3930.6319369676266
Validation Loss = 3930.6319369676266
Iteration 1389/10000: Training Loss = 3930.007735596298
Validation Loss = 3930.007735596298
Iteration 1390/10000: Training Loss = 3929.3956104957497
Validation Loss = 3929.3956104957497
Iteration 1391/10000: Training Loss = 3928.7814267302056
Validation Loss = 3928.7814267302056
Iteration 1392/10000: Training Loss = 3928.1803798626506
Validation Loss = 3928.1803798626506
Iteration 1393/10000: Training Loss = 3927.5656766546717
Validation Loss = 3927.5656766546717
Iteration 1394/10000: Training Loss = 3926.960753272611
Validation Loss = 3926.960753272611
Iteration 1395/10000: Training Loss = 3926.351457631877
Validation Loss = 3926.351457631877
Iteration 1396/10000: Training Loss = 3925.745245457629
Validation Loss = 3925.745245457629
Iteration 1397/10000: Training Loss = 3925.132164635471
Validation Loss = 3925.132164635471
Iteration 1398/10000: Training Loss = 3924.5247667449703
Validation Loss = 3924.5247667449703
Iteration 1399/10000: Training Loss = 3923.917990827429
Validation Loss = 3923.917990827429
Iteration 1400/10000: Training Loss = 3923.3137370344225
Validation Loss = 3923.3137370344225
Iteration 1401/10000: Training Loss = 3922.7082920023418
Validation Loss = 3922.7082920023418
Iteration 1402/10000: Training Loss = 3922.1153331368214
Validation Loss = 3922.1153331368214
Iteration 1403/10000: Training Loss = 3921.5225920214507
Validation Loss = 3921.5225920214507
Iteration 1404/10000: Training Loss = 3920.9248416372307
Validation Loss = 3920.9248416372307
Iteration 1405/10000: Training Loss = 3920.3110005044377
Validation Loss = 3920.3110005044377
Iteration 1406/10000: Training Loss = 3919.719174043081
Validation Loss = 3919.719174043081
Iteration 1407/10000: Training Loss = 3919.1356806133567
Validation Loss = 3919.1356806133567
Iteration 1408/10000: Training Loss = 3918.541402937383
Validation Loss = 3918.541402937383
Iteration 1409/10000: Training Loss = 3917.94663175013
Validation Loss = 3917.94663175013
Iteration 1410/10000: Training Loss = 3917.3485009397177
Validation Loss = 3917.3485009397177
Iteration 1411/10000: Training Loss = 3916.7533074150806
Validation Loss = 3916.7533074150806
Iteration 1412/10000: Training Loss = 3916.160256891618
Validation Loss = 3916.160256891618
Iteration 1413/10000: Training Loss = 3915.5492646508237
Validation Loss = 3915.5492646508237
Iteration 1414/10000: Training Loss = 3914.9595446112726
Validation Loss = 3914.9595446112726
Iteration 1415/10000: Training Loss = 3914.373950593339
Validation Loss = 3914.373950593339
Iteration 1416/10000: Training Loss = 3913.7754115348052
Validation Loss = 3913.7754115348052
Iteration 1417/10000: Training Loss = 3913.1705359330304
Validation Loss = 3913.1705359330304
Iteration 1418/10000: Training Loss = 3912.5636577109076
Validation Loss = 3912.5636577109076
Iteration 1419/10000: Training Loss = 3911.973540183077
Validation Loss = 3911.973540183077
Iteration 1420/10000: Training Loss = 3911.3792858228835
Validation Loss = 3911.3792858228835
Iteration 1421/10000: Training Loss = 3910.791959453113
Validation Loss = 3910.791959453113
Iteration 1422/10000: Training Loss = 3910.2022236006646
Validation Loss = 3910.2022236006646
Iteration 1423/10000: Training Loss = 3909.6163192553386
Validation Loss = 3909.6163192553386
Iteration 1424/10000: Training Loss = 3909.0322046446245
Validation Loss = 3909.0322046446245
Iteration 1425/10000: Training Loss = 3908.4280206546505
Validation Loss = 3908.4280206546505
Iteration 1426/10000: Training Loss = 3907.8413056946974
Validation Loss = 3907.8413056946974
Iteration 1427/10000: Training Loss = 3907.2558159331375
Validation Loss = 3907.2558159331375
Iteration 1428/10000: Training Loss = 3906.6592401494977
Validation Loss = 3906.6592401494977
Iteration 1429/10000: Training Loss = 3906.0721737848876
Validation Loss = 3906.0721737848876
Iteration 1430/10000: Training Loss = 3905.4874708199395
Validation Loss = 3905.4874708199395
Iteration 1431/10000: Training Loss = 3904.9069435338
Validation Loss = 3904.9069435338
Iteration 1432/10000: Training Loss = 3904.310768437893
Validation Loss = 3904.310768437893
Iteration 1433/10000: Training Loss = 3903.719072064309
Validation Loss = 3903.719072064309
Iteration 1434/10000: Training Loss = 3903.149348428864
Validation Loss = 3903.149348428864
Iteration 1435/10000: Training Loss = 3902.5751471161725
Validation Loss = 3902.5751471161725
Iteration 1436/10000: Training Loss = 3901.98407551709
Validation Loss = 3901.98407551709
Iteration 1437/10000: Training Loss = 3901.406013860566
Validation Loss = 3901.406013860566
Iteration 1438/10000: Training Loss = 3900.8240439641436
Validation Loss = 3900.8240439641436
Iteration 1439/10000: Training Loss = 3900.2496495290106
Validation Loss = 3900.2496495290106
Iteration 1440/10000: Training Loss = 3899.6803427039276
Validation Loss = 3899.6803427039276
Iteration 1441/10000: Training Loss = 3899.1076623879617
Validation Loss = 3899.1076623879617
Iteration 1442/10000: Training Loss = 3898.5223325247484
Validation Loss = 3898.5223325247484
Iteration 1443/10000: Training Loss = 3897.9307269998876
Validation Loss = 3897.9307269998876
Iteration 1444/10000: Training Loss = 3897.3716499396533
Validation Loss = 3897.3716499396533
Iteration 1445/10000: Training Loss = 3896.8075198937195
Validation Loss = 3896.8075198937195
Iteration 1446/10000: Training Loss = 3896.2249800382297
Validation Loss = 3896.2249800382297
Iteration 1447/10000: Training Loss = 3895.6679634474267
Validation Loss = 3895.6679634474267
Iteration 1448/10000: Training Loss = 3895.0772463419585
Validation Loss = 3895.0772463419585
Iteration 1449/10000: Training Loss = 3894.492625896223
Validation Loss = 3894.492625896223
Iteration 1450/10000: Training Loss = 3893.8951610697814
Validation Loss = 3893.8951610697814
Iteration 1451/10000: Training Loss = 3893.31855402638
Validation Loss = 3893.31855402638
Iteration 1452/10000: Training Loss = 3892.7409229414525
Validation Loss = 3892.7409229414525
Iteration 1453/10000: Training Loss = 3892.1748326838433
Validation Loss = 3892.1748326838433
Iteration 1454/10000: Training Loss = 3891.604364813461
Validation Loss = 3891.604364813461
Iteration 1455/10000: Training Loss = 3891.0685944086167
Validation Loss = 3891.0685944086167
Iteration 1456/10000: Training Loss = 3890.4704712000603
Validation Loss = 3890.4704712000603
Iteration 1457/10000: Training Loss = 3889.909552780827
Validation Loss = 3889.909552780827
Iteration 1458/10000: Training Loss = 3889.334061543902
Validation Loss = 3889.334061543902
Iteration 1459/10000: Training Loss = 3888.7558153157993
Validation Loss = 3888.7558153157993
Iteration 1460/10000: Training Loss = 3888.1782013678103
Validation Loss = 3888.1782013678103
Iteration 1461/10000: Training Loss = 3887.604050334597
Validation Loss = 3887.604050334597
Iteration 1462/10000: Training Loss = 3887.0316466498484
Validation Loss = 3887.0316466498484
Iteration 1463/10000: Training Loss = 3886.457336739549
Validation Loss = 3886.457336739549
Iteration 1464/10000: Training Loss = 3885.887437507743
Validation Loss = 3885.887437507743
Iteration 1465/10000: Training Loss = 3885.3162869368903
Validation Loss = 3885.3162869368903
Iteration 1466/10000: Training Loss = 3884.7504032914208
Validation Loss = 3884.7504032914208
Iteration 1467/10000: Training Loss = 3884.192993815655
Validation Loss = 3884.192993815655
Iteration 1468/10000: Training Loss = 3883.6416851835443
Validation Loss = 3883.6416851835443
Iteration 1469/10000: Training Loss = 3883.078941159801
Validation Loss = 3883.078941159801
Iteration 1470/10000: Training Loss = 3882.519695642884
Validation Loss = 3882.519695642884
Iteration 1471/10000: Training Loss = 3881.954404609577
Validation Loss = 3881.954404609577
Iteration 1472/10000: Training Loss = 3881.3799136923403
Validation Loss = 3881.3799136923403
Iteration 1473/10000: Training Loss = 3880.813783473267
Validation Loss = 3880.813783473267
Iteration 1474/10000: Training Loss = 3880.257791680198
Validation Loss = 3880.257791680198
Iteration 1475/10000: Training Loss = 3879.6898051069684
Validation Loss = 3879.6898051069684
Iteration 1476/10000: Training Loss = 3879.117954041908
Validation Loss = 3879.117954041908
Iteration 1477/10000: Training Loss = 3878.5537720566467
Validation Loss = 3878.5537720566467
Iteration 1478/10000: Training Loss = 3877.991422160721
Validation Loss = 3877.991422160721
Iteration 1479/10000: Training Loss = 3877.428857594792
Validation Loss = 3877.428857594792
Iteration 1480/10000: Training Loss = 3876.8639302144943
Validation Loss = 3876.8639302144943
Iteration 1481/10000: Training Loss = 3876.3149401467317
Validation Loss = 3876.3149401467317
Iteration 1482/10000: Training Loss = 3875.7476506875937
Validation Loss = 3875.7476506875937
Iteration 1483/10000: Training Loss = 3875.1768230172265
Validation Loss = 3875.1768230172265
Iteration 1484/10000: Training Loss = 3874.6204785654763
Validation Loss = 3874.6204785654763
Iteration 1485/10000: Training Loss = 3874.056973278861
Validation Loss = 3874.056973278861
Iteration 1486/10000: Training Loss = 3873.497022523011
Validation Loss = 3873.497022523011
Iteration 1487/10000: Training Loss = 3872.941408700271
Validation Loss = 3872.941408700271
Iteration 1488/10000: Training Loss = 3872.3853353673067
Validation Loss = 3872.3853353673067
Iteration 1489/10000: Training Loss = 3871.8331586565037
Validation Loss = 3871.8331586565037
Iteration 1490/10000: Training Loss = 3871.2837792346954
Validation Loss = 3871.2837792346954
Iteration 1491/10000: Training Loss = 3870.724917191352
Validation Loss = 3870.724917191352
Iteration 1492/10000: Training Loss = 3870.1716105975347
Validation Loss = 3870.1716105975347
Iteration 1493/10000: Training Loss = 3869.6302033455195
Validation Loss = 3869.6302033455195
Iteration 1494/10000: Training Loss = 3869.079467139854
Validation Loss = 3869.079467139854
Iteration 1495/10000: Training Loss = 3868.5249248748596
Validation Loss = 3868.5249248748596
Iteration 1496/10000: Training Loss = 3867.977118168241
Validation Loss = 3867.977118168241
Iteration 1497/10000: Training Loss = 3867.430135866911
Validation Loss = 3867.430135866911
Iteration 1498/10000: Training Loss = 3866.8756727657196
Validation Loss = 3866.8756727657196
Iteration 1499/10000: Training Loss = 3866.321309600863
Validation Loss = 3866.321309600863
Iteration 1500/10000: Training Loss = 3865.761889124032
Validation Loss = 3865.761889124032
Iteration 1501/10000: Training Loss = 3865.2189304889635
Validation Loss = 3865.2189304889635
Iteration 1502/10000: Training Loss = 3864.655786853656
Validation Loss = 3864.655786853656
Iteration 1503/10000: Training Loss = 3864.0964808510894
Validation Loss = 3864.0964808510894
Iteration 1504/10000: Training Loss = 3863.5470878675947
Validation Loss = 3863.5470878675947
Iteration 1505/10000: Training Loss = 3862.991436583159
Validation Loss = 3862.991436583159
Iteration 1506/10000: Training Loss = 3862.44372004808
Validation Loss = 3862.44372004808
Iteration 1507/10000: Training Loss = 3861.8916788005563
Validation Loss = 3861.8916788005563
Iteration 1508/10000: Training Loss = 3861.3509136199177
Validation Loss = 3861.3509136199177
Iteration 1509/10000: Training Loss = 3860.8081440302435
Validation Loss = 3860.8081440302435
Iteration 1510/10000: Training Loss = 3860.258320819795
Validation Loss = 3860.258320819795
Iteration 1511/10000: Training Loss = 3859.7180431612683
Validation Loss = 3859.7180431612683
Iteration 1512/10000: Training Loss = 3859.1724777709437
Validation Loss = 3859.1724777709437
Iteration 1513/10000: Training Loss = 3858.6314221670423
Validation Loss = 3858.6314221670423
Iteration 1514/10000: Training Loss = 3858.0836577162177
Validation Loss = 3858.0836577162177
Iteration 1515/10000: Training Loss = 3857.542226562596
Validation Loss = 3857.542226562596
Iteration 1516/10000: Training Loss = 3857.000477569794
Validation Loss = 3857.000477569794
Iteration 1517/10000: Training Loss = 3856.4611060468947
Validation Loss = 3856.4611060468947
Iteration 1518/10000: Training Loss = 3855.921804431311
Validation Loss = 3855.921804431311
Iteration 1519/10000: Training Loss = 3855.381590999354
Validation Loss = 3855.381590999354
Iteration 1520/10000: Training Loss = 3854.8363193512596
Validation Loss = 3854.8363193512596
Iteration 1521/10000: Training Loss = 3854.2905293047415
Validation Loss = 3854.2905293047415
Iteration 1522/10000: Training Loss = 3853.763093853313
Validation Loss = 3853.763093853313
Iteration 1523/10000: Training Loss = 3853.2311130557127
Validation Loss = 3853.2311130557127
Iteration 1524/10000: Training Loss = 3852.6892055257604
Validation Loss = 3852.6892055257604
Iteration 1525/10000: Training Loss = 3852.156807332111
Validation Loss = 3852.156807332111
Iteration 1526/10000: Training Loss = 3851.617156280701
Validation Loss = 3851.617156280701
Iteration 1527/10000: Training Loss = 3851.076203896172
Validation Loss = 3851.076203896172
Iteration 1528/10000: Training Loss = 3850.531211447874
Validation Loss = 3850.531211447874
Iteration 1529/10000: Training Loss = 3849.99265153805
Validation Loss = 3849.99265153805
Iteration 1530/10000: Training Loss = 3849.4601359378094
Validation Loss = 3849.4601359378094
Iteration 1531/10000: Training Loss = 3848.9280887792174
Validation Loss = 3848.9280887792174
Iteration 1532/10000: Training Loss = 3848.3928956734585
Validation Loss = 3848.3928956734585
Iteration 1533/10000: Training Loss = 3847.8504128123004
Validation Loss = 3847.8504128123004
Iteration 1534/10000: Training Loss = 3847.3164320271376
Validation Loss = 3847.3164320271376
Iteration 1535/10000: Training Loss = 3846.7822519797255
Validation Loss = 3846.7822519797255
Iteration 1536/10000: Training Loss = 3846.2490041898523
Validation Loss = 3846.2490041898523
Iteration 1537/10000: Training Loss = 3845.7069319504526
Validation Loss = 3845.7069319504526
Iteration 1538/10000: Training Loss = 3845.182272013033
Validation Loss = 3845.182272013033
Iteration 1539/10000: Training Loss = 3844.652518092641
Validation Loss = 3844.652518092641
Iteration 1540/10000: Training Loss = 3844.122644196319
Validation Loss = 3844.122644196319
Iteration 1541/10000: Training Loss = 3843.6094133493225
Validation Loss = 3843.6094133493225
Iteration 1542/10000: Training Loss = 3843.089733633338
Validation Loss = 3843.089733633338
Iteration 1543/10000: Training Loss = 3842.5713667022123
Validation Loss = 3842.5713667022123
Iteration 1544/10000: Training Loss = 3842.039283182146
Validation Loss = 3842.039283182146
Iteration 1545/10000: Training Loss = 3841.5101426151473
Validation Loss = 3841.5101426151473
Iteration 1546/10000: Training Loss = 3840.997517504736
Validation Loss = 3840.997517504736
Iteration 1547/10000: Training Loss = 3840.479668570339
Validation Loss = 3840.479668570339
Iteration 1548/10000: Training Loss = 3839.9331410646696
Validation Loss = 3839.9331410646696
Iteration 1549/10000: Training Loss = 3839.3977185234176
Validation Loss = 3839.3977185234176
Iteration 1550/10000: Training Loss = 3838.8730081876333
Validation Loss = 3838.8730081876333
Iteration 1551/10000: Training Loss = 3838.337106975551
Validation Loss = 3838.337106975551
Iteration 1552/10000: Training Loss = 3837.8170178785595
Validation Loss = 3837.8170178785595
Iteration 1553/10000: Training Loss = 3837.287987862162
Validation Loss = 3837.287987862162
Iteration 1554/10000: Training Loss = 3836.758158706859
Validation Loss = 3836.758158706859
Iteration 1555/10000: Training Loss = 3836.2297386023324
Validation Loss = 3836.2297386023324
Iteration 1556/10000: Training Loss = 3835.706933967715
Validation Loss = 3835.706933967715
Iteration 1557/10000: Training Loss = 3835.1938341443715
Validation Loss = 3835.1938341443715
Iteration 1558/10000: Training Loss = 3834.668822432972
Validation Loss = 3834.668822432972
Iteration 1559/10000: Training Loss = 3834.1490177604346
Validation Loss = 3834.1490177604346
Iteration 1560/10000: Training Loss = 3833.6298657851844
Validation Loss = 3833.6298657851844
Iteration 1561/10000: Training Loss = 3833.1080915309317
Validation Loss = 3833.1080915309317
Iteration 1562/10000: Training Loss = 3832.5812707435143
Validation Loss = 3832.5812707435143
Iteration 1563/10000: Training Loss = 3832.0641160289706
Validation Loss = 3832.0641160289706
Iteration 1564/10000: Training Loss = 3831.532358792799
Validation Loss = 3831.532358792799
Iteration 1565/10000: Training Loss = 3831.0121441218284
Validation Loss = 3831.0121441218284
Iteration 1566/10000: Training Loss = 3830.4927750969355
Validation Loss = 3830.4927750969355
Iteration 1567/10000: Training Loss = 3829.971435391916
Validation Loss = 3829.971435391916
Iteration 1568/10000: Training Loss = 3829.449426481319
Validation Loss = 3829.449426481319
Iteration 1569/10000: Training Loss = 3828.936492743739
Validation Loss = 3828.936492743739
Iteration 1570/10000: Training Loss = 3828.4197419054376
Validation Loss = 3828.4197419054376
Iteration 1571/10000: Training Loss = 3827.9079758547823
Validation Loss = 3827.9079758547823
Iteration 1572/10000: Training Loss = 3827.392091944374
Validation Loss = 3827.392091944374
Iteration 1573/10000: Training Loss = 3826.875081294618
Validation Loss = 3826.875081294618
Iteration 1574/10000: Training Loss = 3826.360163949514
Validation Loss = 3826.360163949514
Iteration 1575/10000: Training Loss = 3825.849672245183
Validation Loss = 3825.849672245183
Iteration 1576/10000: Training Loss = 3825.341608111946
Validation Loss = 3825.341608111946
Iteration 1577/10000: Training Loss = 3824.8241275992123
Validation Loss = 3824.8241275992123
Iteration 1578/10000: Training Loss = 3824.3200709476832
Validation Loss = 3824.3200709476832
Iteration 1579/10000: Training Loss = 3823.816363681005
Validation Loss = 3823.816363681005
Iteration 1580/10000: Training Loss = 3823.3073503408655
Validation Loss = 3823.3073503408655
Iteration 1581/10000: Training Loss = 3822.7907472513084
Validation Loss = 3822.7907472513084
Iteration 1582/10000: Training Loss = 3822.2812083622575
Validation Loss = 3822.2812083622575
Iteration 1583/10000: Training Loss = 3821.777930786272
Validation Loss = 3821.777930786272
Iteration 1584/10000: Training Loss = 3821.2710164123705
Validation Loss = 3821.2710164123705
Iteration 1585/10000: Training Loss = 3820.765803268574
Validation Loss = 3820.765803268574
Iteration 1586/10000: Training Loss = 3820.243915040942
Validation Loss = 3820.243915040942
Iteration 1587/10000: Training Loss = 3819.746500875208
Validation Loss = 3819.746500875208
Iteration 1588/10000: Training Loss = 3819.2252898815223
Validation Loss = 3819.2252898815223
Iteration 1589/10000: Training Loss = 3818.725919933339
Validation Loss = 3818.725919933339
Iteration 1590/10000: Training Loss = 3818.206093715505
Validation Loss = 3818.206093715505
Iteration 1591/10000: Training Loss = 3817.6912858182154
Validation Loss = 3817.6912858182154
Iteration 1592/10000: Training Loss = 3817.186571656709
Validation Loss = 3817.186571656709
Iteration 1593/10000: Training Loss = 3816.674239624105
Validation Loss = 3816.674239624105
Iteration 1594/10000: Training Loss = 3816.167848279934
Validation Loss = 3816.167848279934
Iteration 1595/10000: Training Loss = 3815.652355542795
Validation Loss = 3815.652355542795
Iteration 1596/10000: Training Loss = 3815.1433581951364
Validation Loss = 3815.1433581951364
Iteration 1597/10000: Training Loss = 3814.645202848
Validation Loss = 3814.645202848
Iteration 1598/10000: Training Loss = 3814.1495443899908
Validation Loss = 3814.1495443899908
Iteration 1599/10000: Training Loss = 3813.6375362547496
Validation Loss = 3813.6375362547496
Iteration 1600/10000: Training Loss = 3813.1306598285987
Validation Loss = 3813.1306598285987
Iteration 1601/10000: Training Loss = 3812.6283117033327
Validation Loss = 3812.6283117033327
Iteration 1602/10000: Training Loss = 3812.13441587174
Validation Loss = 3812.13441587174
Iteration 1603/10000: Training Loss = 3811.6256711406204
Validation Loss = 3811.6256711406204
Iteration 1604/10000: Training Loss = 3811.126967934434
Validation Loss = 3811.126967934434
Iteration 1605/10000: Training Loss = 3810.639002613674
Validation Loss = 3810.639002613674
Iteration 1606/10000: Training Loss = 3810.141815303908
Validation Loss = 3810.141815303908
Iteration 1607/10000: Training Loss = 3809.6327057160524
Validation Loss = 3809.6327057160524
Iteration 1608/10000: Training Loss = 3809.1587285912533
Validation Loss = 3809.1587285912533
Iteration 1609/10000: Training Loss = 3808.6625591772186
Validation Loss = 3808.6625591772186
Iteration 1610/10000: Training Loss = 3808.17808880782
Validation Loss = 3808.17808880782
Iteration 1611/10000: Training Loss = 3807.674051039052
Validation Loss = 3807.674051039052
Iteration 1612/10000: Training Loss = 3807.166822132344
Validation Loss = 3807.166822132344
Iteration 1613/10000: Training Loss = 3806.662209680038
Validation Loss = 3806.662209680038
Iteration 1614/10000: Training Loss = 3806.166695998777
Validation Loss = 3806.166695998777
Iteration 1615/10000: Training Loss = 3805.672766278669
Validation Loss = 3805.672766278669
Iteration 1616/10000: Training Loss = 3805.1749865649654
Validation Loss = 3805.1749865649654
Iteration 1617/10000: Training Loss = 3804.685531903896
Validation Loss = 3804.685531903896
Iteration 1618/10000: Training Loss = 3804.183635514625
Validation Loss = 3804.183635514625
Iteration 1619/10000: Training Loss = 3803.692375029822
Validation Loss = 3803.692375029822
Iteration 1620/10000: Training Loss = 3803.2003953719145
Validation Loss = 3803.2003953719145
Iteration 1621/10000: Training Loss = 3802.712422169521
Validation Loss = 3802.712422169521
Iteration 1622/10000: Training Loss = 3802.2301127284118
Validation Loss = 3802.2301127284118
Iteration 1623/10000: Training Loss = 3801.7321422013097
Validation Loss = 3801.7321422013097
Iteration 1624/10000: Training Loss = 3801.2402413178233
Validation Loss = 3801.2402413178233
Iteration 1625/10000: Training Loss = 3800.7440803510644
Validation Loss = 3800.7440803510644
Iteration 1626/10000: Training Loss = 3800.2558188964135
Validation Loss = 3800.2558188964135
Iteration 1627/10000: Training Loss = 3799.7686316200056
Validation Loss = 3799.7686316200056
Iteration 1628/10000: Training Loss = 3799.26371733047
Validation Loss = 3799.26371733047
Iteration 1629/10000: Training Loss = 3798.7741142942004
Validation Loss = 3798.7741142942004
Iteration 1630/10000: Training Loss = 3798.277186411174
Validation Loss = 3798.277186411174
Iteration 1631/10000: Training Loss = 3797.784765025634
Validation Loss = 3797.784765025634
Iteration 1632/10000: Training Loss = 3797.3038209677397
Validation Loss = 3797.3038209677397
Iteration 1633/10000: Training Loss = 3796.815201988468
Validation Loss = 3796.815201988468
Iteration 1634/10000: Training Loss = 3796.331556481373
Validation Loss = 3796.331556481373
Iteration 1635/10000: Training Loss = 3795.846877648888
Validation Loss = 3795.846877648888
Iteration 1636/10000: Training Loss = 3795.3600067397224
Validation Loss = 3795.3600067397224
Iteration 1637/10000: Training Loss = 3794.8785958857266
Validation Loss = 3794.8785958857266
Iteration 1638/10000: Training Loss = 3794.3931728438683
Validation Loss = 3794.3931728438683
Iteration 1639/10000: Training Loss = 3793.914020089802
Validation Loss = 3793.914020089802
Iteration 1640/10000: Training Loss = 3793.4386563107055
Validation Loss = 3793.4386563107055
Iteration 1641/10000: Training Loss = 3792.956713785358
Validation Loss = 3792.956713785358
Iteration 1642/10000: Training Loss = 3792.4755423984066
Validation Loss = 3792.4755423984066
Iteration 1643/10000: Training Loss = 3791.9995275537312
Validation Loss = 3791.9995275537312
Iteration 1644/10000: Training Loss = 3791.5356312600657
Validation Loss = 3791.5356312600657
Iteration 1645/10000: Training Loss = 3791.0531717838903
Validation Loss = 3791.0531717838903
Iteration 1646/10000: Training Loss = 3790.5598946849846
Validation Loss = 3790.5598946849846
Iteration 1647/10000: Training Loss = 3790.0677118689973
Validation Loss = 3790.0677118689973
Iteration 1648/10000: Training Loss = 3789.5845848672516
Validation Loss = 3789.5845848672516
Iteration 1649/10000: Training Loss = 3789.0997001234773
Validation Loss = 3789.0997001234773
Iteration 1650/10000: Training Loss = 3788.6198526423914
Validation Loss = 3788.6198526423914
Iteration 1651/10000: Training Loss = 3788.1406819008566
Validation Loss = 3788.1406819008566
Iteration 1652/10000: Training Loss = 3787.66810819048
Validation Loss = 3787.66810819048
Iteration 1653/10000: Training Loss = 3787.197226920195
Validation Loss = 3787.197226920195
Iteration 1654/10000: Training Loss = 3786.7188579074323
Validation Loss = 3786.7188579074323
Iteration 1655/10000: Training Loss = 3786.238122037864
Validation Loss = 3786.238122037864
Iteration 1656/10000: Training Loss = 3785.7636374626286
Validation Loss = 3785.7636374626286
Iteration 1657/10000: Training Loss = 3785.285259202325
Validation Loss = 3785.285259202325
Iteration 1658/10000: Training Loss = 3784.811453501302
Validation Loss = 3784.811453501302
Iteration 1659/10000: Training Loss = 3784.3487949006344
Validation Loss = 3784.3487949006344
Iteration 1660/10000: Training Loss = 3783.861073782443
Validation Loss = 3783.861073782443
Iteration 1661/10000: Training Loss = 3783.396835967711
Validation Loss = 3783.396835967711
Iteration 1662/10000: Training Loss = 3782.9184546383467
Validation Loss = 3782.9184546383467
Iteration 1663/10000: Training Loss = 3782.4388084465936
Validation Loss = 3782.4388084465936
Iteration 1664/10000: Training Loss = 3781.9631378558665
Validation Loss = 3781.9631378558665
Iteration 1665/10000: Training Loss = 3781.4871277956445
Validation Loss = 3781.4871277956445
Iteration 1666/10000: Training Loss = 3781.014072547864
Validation Loss = 3781.014072547864
Iteration 1667/10000: Training Loss = 3780.5297068103646
Validation Loss = 3780.5297068103646
Iteration 1668/10000: Training Loss = 3780.0529666688153
Validation Loss = 3780.0529666688153
Iteration 1669/10000: Training Loss = 3779.57650095832
Validation Loss = 3779.57650095832
Iteration 1670/10000: Training Loss = 3779.100535512948
Validation Loss = 3779.100535512948
Iteration 1671/10000: Training Loss = 3778.6284753321816
Validation Loss = 3778.6284753321816
Iteration 1672/10000: Training Loss = 3778.1617350644865
Validation Loss = 3778.1617350644865
Iteration 1673/10000: Training Loss = 3777.7024452494643
Validation Loss = 3777.7024452494643
Iteration 1674/10000: Training Loss = 3777.2254779873097
Validation Loss = 3777.2254779873097
Iteration 1675/10000: Training Loss = 3776.7616131471564
Validation Loss = 3776.7616131471564
Iteration 1676/10000: Training Loss = 3776.287145116635
Validation Loss = 3776.287145116635
Iteration 1677/10000: Training Loss = 3775.8146270404754
Validation Loss = 3775.8146270404754
Iteration 1678/10000: Training Loss = 3775.3507370993507
Validation Loss = 3775.3507370993507
Iteration 1679/10000: Training Loss = 3774.8684073189966
Validation Loss = 3774.8684073189966
Iteration 1680/10000: Training Loss = 3774.4013550113064
Validation Loss = 3774.4013550113064
Iteration 1681/10000: Training Loss = 3773.9412718453627
Validation Loss = 3773.9412718453627
Iteration 1682/10000: Training Loss = 3773.468545863841
Validation Loss = 3773.468545863841
Iteration 1683/10000: Training Loss = 3773.00355790958
Validation Loss = 3773.00355790958
Iteration 1684/10000: Training Loss = 3772.5328194389463
Validation Loss = 3772.5328194389463
Iteration 1685/10000: Training Loss = 3772.068986169842
Validation Loss = 3772.068986169842
Iteration 1686/10000: Training Loss = 3771.606334579894
Validation Loss = 3771.606334579894
Iteration 1687/10000: Training Loss = 3771.1503369324723
Validation Loss = 3771.1503369324723
Iteration 1688/10000: Training Loss = 3770.681406446947
Validation Loss = 3770.681406446947
Iteration 1689/10000: Training Loss = 3770.2261503831005
Validation Loss = 3770.2261503831005
Iteration 1690/10000: Training Loss = 3769.752527142067
Validation Loss = 3769.752527142067
Iteration 1691/10000: Training Loss = 3769.2841110884247
Validation Loss = 3769.2841110884247
Iteration 1692/10000: Training Loss = 3768.820441997078
Validation Loss = 3768.820441997078
Iteration 1693/10000: Training Loss = 3768.349946255528
Validation Loss = 3768.349946255528
Iteration 1694/10000: Training Loss = 3767.882659944326
Validation Loss = 3767.882659944326
Iteration 1695/10000: Training Loss = 3767.4233693782708
Validation Loss = 3767.4233693782708
Iteration 1696/10000: Training Loss = 3766.965140052979
Validation Loss = 3766.965140052979
Iteration 1697/10000: Training Loss = 3766.5018615988333
Validation Loss = 3766.5018615988333
Iteration 1698/10000: Training Loss = 3766.039747156106
Validation Loss = 3766.039747156106
Iteration 1699/10000: Training Loss = 3765.589284996208
Validation Loss = 3765.589284996208
Iteration 1700/10000: Training Loss = 3765.1280418420433
Validation Loss = 3765.1280418420433
Iteration 1701/10000: Training Loss = 3764.677508407133
Validation Loss = 3764.677508407133
Iteration 1702/10000: Training Loss = 3764.213597259229
Validation Loss = 3764.213597259229
Iteration 1703/10000: Training Loss = 3763.753914556524
Validation Loss = 3763.753914556524
Iteration 1704/10000: Training Loss = 3763.295066812098
Validation Loss = 3763.295066812098
Iteration 1705/10000: Training Loss = 3762.842133093467
Validation Loss = 3762.842133093467
Iteration 1706/10000: Training Loss = 3762.3786008049483
Validation Loss = 3762.3786008049483
Iteration 1707/10000: Training Loss = 3761.927573881449
Validation Loss = 3761.927573881449
Iteration 1708/10000: Training Loss = 3761.4699308633144
Validation Loss = 3761.4699308633144
Iteration 1709/10000: Training Loss = 3761.0166135891573
Validation Loss = 3761.0166135891573
Iteration 1710/10000: Training Loss = 3760.5639326555292
Validation Loss = 3760.5639326555292
Iteration 1711/10000: Training Loss = 3760.1121362826966
Validation Loss = 3760.1121362826966
Iteration 1712/10000: Training Loss = 3759.655725467873
Validation Loss = 3759.655725467873
Iteration 1713/10000: Training Loss = 3759.1962832883623
Validation Loss = 3759.1962832883623
Iteration 1714/10000: Training Loss = 3758.73465085466
Validation Loss = 3758.73465085466
Iteration 1715/10000: Training Loss = 3758.288121741166
Validation Loss = 3758.288121741166
Iteration 1716/10000: Training Loss = 3757.8397401850902
Validation Loss = 3757.8397401850902
Iteration 1717/10000: Training Loss = 3757.392681102267
Validation Loss = 3757.392681102267
Iteration 1718/10000: Training Loss = 3756.955749112001
Validation Loss = 3756.955749112001
Iteration 1719/10000: Training Loss = 3756.504851207675
Validation Loss = 3756.504851207675
Iteration 1720/10000: Training Loss = 3756.04432464011
Validation Loss = 3756.04432464011
Iteration 1721/10000: Training Loss = 3755.5945045460726
Validation Loss = 3755.5945045460726
Iteration 1722/10000: Training Loss = 3755.135963464103
Validation Loss = 3755.135963464103
Iteration 1723/10000: Training Loss = 3754.6865628370965
Validation Loss = 3754.6865628370965
Iteration 1724/10000: Training Loss = 3754.2402777995344
Validation Loss = 3754.2402777995344
Iteration 1725/10000: Training Loss = 3753.787042671583
Validation Loss = 3753.787042671583
Iteration 1726/10000: Training Loss = 3753.350343170827
Validation Loss = 3753.350343170827
Iteration 1727/10000: Training Loss = 3752.9023331599974
Validation Loss = 3752.9023331599974
Iteration 1728/10000: Training Loss = 3752.452034492134
Validation Loss = 3752.452034492134
Iteration 1729/10000: Training Loss = 3752.002133194302
Validation Loss = 3752.002133194302
Iteration 1730/10000: Training Loss = 3751.551742881528
Validation Loss = 3751.551742881528
Iteration 1731/10000: Training Loss = 3751.106368302383
Validation Loss = 3751.106368302383
Iteration 1732/10000: Training Loss = 3750.6554743026727
Validation Loss = 3750.6554743026727
Iteration 1733/10000: Training Loss = 3750.203250154827
Validation Loss = 3750.203250154827
Iteration 1734/10000: Training Loss = 3749.7639735604503
Validation Loss = 3749.7639735604503
Iteration 1735/10000: Training Loss = 3749.3125350526293
Validation Loss = 3749.3125350526293
Iteration 1736/10000: Training Loss = 3748.859936425321
Validation Loss = 3748.859936425321
Iteration 1737/10000: Training Loss = 3748.4121884070437
Validation Loss = 3748.4121884070437
Iteration 1738/10000: Training Loss = 3747.9672490634575
Validation Loss = 3747.9672490634575
Iteration 1739/10000: Training Loss = 3747.524487533098
Validation Loss = 3747.524487533098
Iteration 1740/10000: Training Loss = 3747.0835799001734
Validation Loss = 3747.0835799001734
Iteration 1741/10000: Training Loss = 3746.6456545327546
Validation Loss = 3746.6456545327546
Iteration 1742/10000: Training Loss = 3746.2129242260326
Validation Loss = 3746.2129242260326
Iteration 1743/10000: Training Loss = 3745.770977127099
Validation Loss = 3745.770977127099
Iteration 1744/10000: Training Loss = 3745.3243418222783
Validation Loss = 3745.3243418222783
Iteration 1745/10000: Training Loss = 3744.875587780514
Validation Loss = 3744.875587780514
Iteration 1746/10000: Training Loss = 3744.4384864534127
Validation Loss = 3744.4384864534127
Iteration 1747/10000: Training Loss = 3744.0002866888235
Validation Loss = 3744.0002866888235
Iteration 1748/10000: Training Loss = 3743.5639421164465
Validation Loss = 3743.5639421164465
Iteration 1749/10000: Training Loss = 3743.1239773974607
Validation Loss = 3743.1239773974607
Iteration 1750/10000: Training Loss = 3742.6853233572588
Validation Loss = 3742.6853233572588
Iteration 1751/10000: Training Loss = 3742.2423513841545
Validation Loss = 3742.2423513841545
Iteration 1752/10000: Training Loss = 3741.803169561512
Validation Loss = 3741.803169561512
Iteration 1753/10000: Training Loss = 3741.37257239086
Validation Loss = 3741.37257239086
Iteration 1754/10000: Training Loss = 3740.932060917693
Validation Loss = 3740.932060917693
Iteration 1755/10000: Training Loss = 3740.500662694869
Validation Loss = 3740.500662694869
Iteration 1756/10000: Training Loss = 3740.0647506284467
Validation Loss = 3740.0647506284467
Iteration 1757/10000: Training Loss = 3739.628469835046
Validation Loss = 3739.628469835046
Iteration 1758/10000: Training Loss = 3739.1857471426415
Validation Loss = 3739.1857471426415
Iteration 1759/10000: Training Loss = 3738.752327789691
Validation Loss = 3738.752327789691
Iteration 1760/10000: Training Loss = 3738.316942363932
Validation Loss = 3738.316942363932
Iteration 1761/10000: Training Loss = 3737.879682145863
Validation Loss = 3737.879682145863
Iteration 1762/10000: Training Loss = 3737.4258355471993
Validation Loss = 3737.4258355471993
Iteration 1763/10000: Training Loss = 3736.994912222956
Validation Loss = 3736.994912222956
Iteration 1764/10000: Training Loss = 3736.559479519217
Validation Loss = 3736.559479519217
Iteration 1765/10000: Training Loss = 3736.1204798632593
Validation Loss = 3736.1204798632593
Iteration 1766/10000: Training Loss = 3735.6926896334885
Validation Loss = 3735.6926896334885
Iteration 1767/10000: Training Loss = 3735.265033822351
Validation Loss = 3735.265033822351
Iteration 1768/10000: Training Loss = 3734.844673529652
Validation Loss = 3734.844673529652
Iteration 1769/10000: Training Loss = 3734.4070193809775
Validation Loss = 3734.4070193809775
Iteration 1770/10000: Training Loss = 3733.976922901255
Validation Loss = 3733.976922901255
Iteration 1771/10000: Training Loss = 3733.5400545673274
Validation Loss = 3733.5400545673274
Iteration 1772/10000: Training Loss = 3733.110710972686
Validation Loss = 3733.110710972686
Iteration 1773/10000: Training Loss = 3732.677624047018
Validation Loss = 3732.677624047018
Iteration 1774/10000: Training Loss = 3732.246153679674
Validation Loss = 3732.246153679674
Iteration 1775/10000: Training Loss = 3731.8132351850477
Validation Loss = 3731.8132351850477
Iteration 1776/10000: Training Loss = 3731.3831487786256
Validation Loss = 3731.3831487786256
Iteration 1777/10000: Training Loss = 3730.951218668929
Validation Loss = 3730.951218668929
Iteration 1778/10000: Training Loss = 3730.5188901398296
Validation Loss = 3730.5188901398296
Iteration 1779/10000: Training Loss = 3730.084696900634
Validation Loss = 3730.084696900634
Iteration 1780/10000: Training Loss = 3729.6645472367773
Validation Loss = 3729.6645472367773
Iteration 1781/10000: Training Loss = 3729.2356421403647
Validation Loss = 3729.2356421403647
Iteration 1782/10000: Training Loss = 3728.806603038844
Validation Loss = 3728.806603038844
Iteration 1783/10000: Training Loss = 3728.373685722398
Validation Loss = 3728.373685722398
Iteration 1784/10000: Training Loss = 3727.9515555418625
Validation Loss = 3727.9515555418625
Iteration 1785/10000: Training Loss = 3727.527469416131
Validation Loss = 3727.527469416131
Iteration 1786/10000: Training Loss = 3727.0991847723644
Validation Loss = 3727.0991847723644
Iteration 1787/10000: Training Loss = 3726.6710325603117
Validation Loss = 3726.6710325603117
Iteration 1788/10000: Training Loss = 3726.2473671795997
Validation Loss = 3726.2473671795997
Iteration 1789/10000: Training Loss = 3725.8225651116627
Validation Loss = 3725.8225651116627
Iteration 1790/10000: Training Loss = 3725.400936551087
Validation Loss = 3725.400936551087
Iteration 1791/10000: Training Loss = 3724.9718507290163
Validation Loss = 3724.9718507290163
Iteration 1792/10000: Training Loss = 3724.5465611011673
Validation Loss = 3724.5465611011673
Iteration 1793/10000: Training Loss = 3724.1274335331987
Validation Loss = 3724.1274335331987
Iteration 1794/10000: Training Loss = 3723.715562471958
Validation Loss = 3723.715562471958
Iteration 1795/10000: Training Loss = 3723.309482732223
Validation Loss = 3723.309482732223
Iteration 1796/10000: Training Loss = 3722.8745811972167
Validation Loss = 3722.8745811972167
Iteration 1797/10000: Training Loss = 3722.4392107234416
Validation Loss = 3722.4392107234416
Iteration 1798/10000: Training Loss = 3722.0105112046836
Validation Loss = 3722.0105112046836
Iteration 1799/10000: Training Loss = 3721.5907619572968
Validation Loss = 3721.5907619572968
Iteration 1800/10000: Training Loss = 3721.17427752636
Validation Loss = 3721.17427752636
Iteration 1801/10000: Training Loss = 3720.7519544367433
Validation Loss = 3720.7519544367433
Iteration 1802/10000: Training Loss = 3720.332256590994
Validation Loss = 3720.332256590994
Iteration 1803/10000: Training Loss = 3719.9141909971663
Validation Loss = 3719.9141909971663
Iteration 1804/10000: Training Loss = 3719.498940341319
Validation Loss = 3719.498940341319
Iteration 1805/10000: Training Loss = 3719.078626784822
Validation Loss = 3719.078626784822
Iteration 1806/10000: Training Loss = 3718.6636946190138
Validation Loss = 3718.6636946190138
Iteration 1807/10000: Training Loss = 3718.246848303408
Validation Loss = 3718.246848303408
Iteration 1808/10000: Training Loss = 3717.8384641365305
Validation Loss = 3717.8384641365305
Iteration 1809/10000: Training Loss = 3717.4122804373565
Validation Loss = 3717.4122804373565
Iteration 1810/10000: Training Loss = 3716.9892171767024
Validation Loss = 3716.9892171767024
Iteration 1811/10000: Training Loss = 3716.5637276117213
Validation Loss = 3716.5637276117213
Iteration 1812/10000: Training Loss = 3716.147252301352
Validation Loss = 3716.147252301352
Iteration 1813/10000: Training Loss = 3715.7249816837366
Validation Loss = 3715.7249816837366
Iteration 1814/10000: Training Loss = 3715.3131316416616
Validation Loss = 3715.3131316416616
Iteration 1815/10000: Training Loss = 3714.8937914665526
Validation Loss = 3714.8937914665526
Iteration 1816/10000: Training Loss = 3714.4786085526353
Validation Loss = 3714.4786085526353
Iteration 1817/10000: Training Loss = 3714.0614392121875
Validation Loss = 3714.0614392121875
Iteration 1818/10000: Training Loss = 3713.645579783974
Validation Loss = 3713.645579783974
Iteration 1819/10000: Training Loss = 3713.2279573760766
Validation Loss = 3713.2279573760766
Iteration 1820/10000: Training Loss = 3712.8216440106376
Validation Loss = 3712.8216440106376
Iteration 1821/10000: Training Loss = 3712.40863178098
Validation Loss = 3712.40863178098
Iteration 1822/10000: Training Loss = 3711.9961534736753
Validation Loss = 3711.9961534736753
Iteration 1823/10000: Training Loss = 3711.587528410503
Validation Loss = 3711.587528410503
Iteration 1824/10000: Training Loss = 3711.171991833012
Validation Loss = 3711.171991833012
Iteration 1825/10000: Training Loss = 3710.7538399748564
Validation Loss = 3710.7538399748564
Iteration 1826/10000: Training Loss = 3710.338142985039
Validation Loss = 3710.338142985039
Iteration 1827/10000: Training Loss = 3709.921846081873
Validation Loss = 3709.921846081873
Iteration 1828/10000: Training Loss = 3709.516224798083
Validation Loss = 3709.516224798083
Iteration 1829/10000: Training Loss = 3709.102651236382
Validation Loss = 3709.102651236382
Iteration 1830/10000: Training Loss = 3708.693585850768
Validation Loss = 3708.693585850768
Iteration 1831/10000: Training Loss = 3708.279395062659
Validation Loss = 3708.279395062659
Iteration 1832/10000: Training Loss = 3707.8681359471907
Validation Loss = 3707.8681359471907
Iteration 1833/10000: Training Loss = 3707.459110360861
Validation Loss = 3707.459110360861
Iteration 1834/10000: Training Loss = 3707.053416109711
Validation Loss = 3707.053416109711
Iteration 1835/10000: Training Loss = 3706.6416877538895
Validation Loss = 3706.6416877538895
Iteration 1836/10000: Training Loss = 3706.2456490387613
Validation Loss = 3706.2456490387613
Iteration 1837/10000: Training Loss = 3705.8464450517486
Validation Loss = 3705.8464450517486
Iteration 1838/10000: Training Loss = 3705.4359007237817
Validation Loss = 3705.4359007237817
Iteration 1839/10000: Training Loss = 3705.0298544023267
Validation Loss = 3705.0298544023267
Iteration 1840/10000: Training Loss = 3704.614006031993
Validation Loss = 3704.614006031993
Iteration 1841/10000: Training Loss = 3704.1999579858493
Validation Loss = 3704.1999579858493
Iteration 1842/10000: Training Loss = 3703.8028248903343
Validation Loss = 3703.8028248903343
Iteration 1843/10000: Training Loss = 3703.387635847577
Validation Loss = 3703.387635847577
Iteration 1844/10000: Training Loss = 3702.9801613741984
Validation Loss = 3702.9801613741984
Iteration 1845/10000: Training Loss = 3702.576673177958
Validation Loss = 3702.576673177958
Iteration 1846/10000: Training Loss = 3702.161774004046
Validation Loss = 3702.161774004046
Iteration 1847/10000: Training Loss = 3701.7552174852653
Validation Loss = 3701.7552174852653
Iteration 1848/10000: Training Loss = 3701.348289862181
Validation Loss = 3701.348289862181
Iteration 1849/10000: Training Loss = 3700.933490183387
Validation Loss = 3700.933490183387
Iteration 1850/10000: Training Loss = 3700.5356926131994
Validation Loss = 3700.5356926131994
Iteration 1851/10000: Training Loss = 3700.117257295272
Validation Loss = 3700.117257295272
Iteration 1852/10000: Training Loss = 3699.7105242158464
Validation Loss = 3699.7105242158464
Iteration 1853/10000: Training Loss = 3699.307665303995
Validation Loss = 3699.307665303995
Iteration 1854/10000: Training Loss = 3698.902158019997
Validation Loss = 3698.902158019997
Iteration 1855/10000: Training Loss = 3698.496721410789
Validation Loss = 3698.496721410789
Iteration 1856/10000: Training Loss = 3698.100312271282
Validation Loss = 3698.100312271282
Iteration 1857/10000: Training Loss = 3697.7020624227944
Validation Loss = 3697.7020624227944
Iteration 1858/10000: Training Loss = 3697.294119249539
Validation Loss = 3697.294119249539
Iteration 1859/10000: Training Loss = 3696.896341493548
Validation Loss = 3696.896341493548
Iteration 1860/10000: Training Loss = 3696.483342198927
Validation Loss = 3696.483342198927
Iteration 1861/10000: Training Loss = 3696.0846440419596
Validation Loss = 3696.0846440419596
Iteration 1862/10000: Training Loss = 3695.6750225701476
Validation Loss = 3695.6750225701476
Iteration 1863/10000: Training Loss = 3695.281209009892
Validation Loss = 3695.281209009892
Iteration 1864/10000: Training Loss = 3694.8860747047465
Validation Loss = 3694.8860747047465
Iteration 1865/10000: Training Loss = 3694.4729338936595
Validation Loss = 3694.4729338936595
Iteration 1866/10000: Training Loss = 3694.073930120838
Validation Loss = 3694.073930120838
Iteration 1867/10000: Training Loss = 3693.67472746839
Validation Loss = 3693.67472746839
Iteration 1868/10000: Training Loss = 3693.275247397469
Validation Loss = 3693.275247397469
Iteration 1869/10000: Training Loss = 3692.875814155481
Validation Loss = 3692.875814155481
Iteration 1870/10000: Training Loss = 3692.4781256114384
Validation Loss = 3692.4781256114384
Iteration 1871/10000: Training Loss = 3692.0842817311345
Validation Loss = 3692.0842817311345
Iteration 1872/10000: Training Loss = 3691.685540197137
Validation Loss = 3691.685540197137
Iteration 1873/10000: Training Loss = 3691.293671963169
Validation Loss = 3691.293671963169
Iteration 1874/10000: Training Loss = 3690.888304904041
Validation Loss = 3690.888304904041
Iteration 1875/10000: Training Loss = 3690.4952565877406
Validation Loss = 3690.4952565877406
Iteration 1876/10000: Training Loss = 3690.109111140427
Validation Loss = 3690.109111140427
Iteration 1877/10000: Training Loss = 3689.7156195782622
Validation Loss = 3689.7156195782622
Iteration 1878/10000: Training Loss = 3689.327626486492
Validation Loss = 3689.327626486492
Iteration 1879/10000: Training Loss = 3688.9386620706414
Validation Loss = 3688.9386620706414
Iteration 1880/10000: Training Loss = 3688.5409651838177
Validation Loss = 3688.5409651838177
Iteration 1881/10000: Training Loss = 3688.1530837124424
Validation Loss = 3688.1530837124424
Iteration 1882/10000: Training Loss = 3687.750156264122
Validation Loss = 3687.750156264122
Iteration 1883/10000: Training Loss = 3687.359779719173
Validation Loss = 3687.359779719173
Iteration 1884/10000: Training Loss = 3686.968691667152
Validation Loss = 3686.968691667152
Iteration 1885/10000: Training Loss = 3686.576540224168
Validation Loss = 3686.576540224168
Iteration 1886/10000: Training Loss = 3686.190190156901
Validation Loss = 3686.190190156901
Iteration 1887/10000: Training Loss = 3685.7922747336474
Validation Loss = 3685.7922747336474
Iteration 1888/10000: Training Loss = 3685.4002389438883
Validation Loss = 3685.4002389438883
Iteration 1889/10000: Training Loss = 3685.012314072517
Validation Loss = 3685.012314072517
Iteration 1890/10000: Training Loss = 3684.619589160116
Validation Loss = 3684.619589160116
Iteration 1891/10000: Training Loss = 3684.215704289917
Validation Loss = 3684.215704289917
Iteration 1892/10000: Training Loss = 3683.825928373778
Validation Loss = 3683.825928373778
Iteration 1893/10000: Training Loss = 3683.4323894164936
Validation Loss = 3683.4323894164936
Iteration 1894/10000: Training Loss = 3683.052166281379
Validation Loss = 3683.052166281379
Iteration 1895/10000: Training Loss = 3682.672101243187
Validation Loss = 3682.672101243187
Iteration 1896/10000: Training Loss = 3682.307190152199
Validation Loss = 3682.307190152199
Iteration 1897/10000: Training Loss = 3681.9102765302364
Validation Loss = 3681.9102765302364
Iteration 1898/10000: Training Loss = 3681.5312951300543
Validation Loss = 3681.5312951300543
Iteration 1899/10000: Training Loss = 3681.144665104989
Validation Loss = 3681.144665104989
Iteration 1900/10000: Training Loss = 3680.7624263827256
Validation Loss = 3680.7624263827256
Iteration 1901/10000: Training Loss = 3680.3636168584217
Validation Loss = 3680.3636168584217
Iteration 1902/10000: Training Loss = 3679.9871254423215
Validation Loss = 3679.9871254423215
Iteration 1903/10000: Training Loss = 3679.596634488509
Validation Loss = 3679.596634488509
Iteration 1904/10000: Training Loss = 3679.200547074758
Validation Loss = 3679.200547074758
Iteration 1905/10000: Training Loss = 3678.8213926481544
Validation Loss = 3678.8213926481544
Iteration 1906/10000: Training Loss = 3678.4299042781204
Validation Loss = 3678.4299042781204
Iteration 1907/10000: Training Loss = 3678.0293334842245
Validation Loss = 3678.0293334842245
Iteration 1908/10000: Training Loss = 3677.6442007866553
Validation Loss = 3677.6442007866553
Iteration 1909/10000: Training Loss = 3677.2707480037793
Validation Loss = 3677.2707480037793
Iteration 1910/10000: Training Loss = 3676.884314322779
Validation Loss = 3676.884314322779
Iteration 1911/10000: Training Loss = 3676.5037578925717
Validation Loss = 3676.5037578925717
Iteration 1912/10000: Training Loss = 3676.117823199773
Validation Loss = 3676.117823199773
Iteration 1913/10000: Training Loss = 3675.72642018308
Validation Loss = 3675.72642018308
Iteration 1914/10000: Training Loss = 3675.350841291929
Validation Loss = 3675.350841291929
Iteration 1915/10000: Training Loss = 3674.968896858051
Validation Loss = 3674.968896858051
Iteration 1916/10000: Training Loss = 3674.5875690975863
Validation Loss = 3674.5875690975863
Iteration 1917/10000: Training Loss = 3674.20938495911
Validation Loss = 3674.20938495911
Iteration 1918/10000: Training Loss = 3673.821092695271
Validation Loss = 3673.821092695271
Iteration 1919/10000: Training Loss = 3673.4308446440177
Validation Loss = 3673.4308446440177
Iteration 1920/10000: Training Loss = 3673.053022557182
Validation Loss = 3673.053022557182
Iteration 1921/10000: Training Loss = 3672.6693805267582
Validation Loss = 3672.6693805267582
Iteration 1922/10000: Training Loss = 3672.287331183409
Validation Loss = 3672.287331183409
Iteration 1923/10000: Training Loss = 3671.899770345267
Validation Loss = 3671.899770345267
Iteration 1924/10000: Training Loss = 3671.517387787751
Validation Loss = 3671.517387787751
Iteration 1925/10000: Training Loss = 3671.1273478457397
Validation Loss = 3671.1273478457397
Iteration 1926/10000: Training Loss = 3670.7448858736034
Validation Loss = 3670.7448858736034
Iteration 1927/10000: Training Loss = 3670.3768896263987
Validation Loss = 3670.3768896263987
Iteration 1928/10000: Training Loss = 3670.0046433867196
Validation Loss = 3670.0046433867196
Iteration 1929/10000: Training Loss = 3669.6257209937835
Validation Loss = 3669.6257209937835
Iteration 1930/10000: Training Loss = 3669.253232157196
Validation Loss = 3669.253232157196
Iteration 1931/10000: Training Loss = 3668.87673912088
Validation Loss = 3668.87673912088
Iteration 1932/10000: Training Loss = 3668.497487846198
Validation Loss = 3668.497487846198
Iteration 1933/10000: Training Loss = 3668.1165599718775
Validation Loss = 3668.1165599718775
Iteration 1934/10000: Training Loss = 3667.7419890081505
Validation Loss = 3667.7419890081505
Iteration 1935/10000: Training Loss = 3667.365153472895
Validation Loss = 3667.365153472895
Iteration 1936/10000: Training Loss = 3666.99128911951
Validation Loss = 3666.99128911951
Iteration 1937/10000: Training Loss = 3666.6195238107202
Validation Loss = 3666.6195238107202
Iteration 1938/10000: Training Loss = 3666.243321202833
Validation Loss = 3666.243321202833
Iteration 1939/10000: Training Loss = 3665.8749648584567
Validation Loss = 3665.8749648584567
Iteration 1940/10000: Training Loss = 3665.496778826287
Validation Loss = 3665.496778826287
Iteration 1941/10000: Training Loss = 3665.1268623443448
Validation Loss = 3665.1268623443448
Iteration 1942/10000: Training Loss = 3664.754323880029
Validation Loss = 3664.754323880029
Iteration 1943/10000: Training Loss = 3664.3821248382415
Validation Loss = 3664.3821248382415
Iteration 1944/10000: Training Loss = 3664.0102192350087
Validation Loss = 3664.0102192350087
Iteration 1945/10000: Training Loss = 3663.633495598432
Validation Loss = 3663.633495598432
Iteration 1946/10000: Training Loss = 3663.2655300944043
Validation Loss = 3663.2655300944043
Iteration 1947/10000: Training Loss = 3662.891088301099
Validation Loss = 3662.891088301099
Iteration 1948/10000: Training Loss = 3662.523881382695
Validation Loss = 3662.523881382695
Iteration 1949/10000: Training Loss = 3662.1443723075595
Validation Loss = 3662.1443723075595
Iteration 1950/10000: Training Loss = 3661.773629822055
Validation Loss = 3661.773629822055
Iteration 1951/10000: Training Loss = 3661.4009431703544
Validation Loss = 3661.4009431703544
Iteration 1952/10000: Training Loss = 3661.0275388599025
Validation Loss = 3661.0275388599025
Iteration 1953/10000: Training Loss = 3660.661376060323
Validation Loss = 3660.661376060323
Iteration 1954/10000: Training Loss = 3660.286943850833
Validation Loss = 3660.286943850833
Iteration 1955/10000: Training Loss = 3659.9206674411193
Validation Loss = 3659.9206674411193
Iteration 1956/10000: Training Loss = 3659.547371125692
Validation Loss = 3659.547371125692
Iteration 1957/10000: Training Loss = 3659.172918664912
Validation Loss = 3659.172918664912
Iteration 1958/10000: Training Loss = 3658.800493278248
Validation Loss = 3658.800493278248
Iteration 1959/10000: Training Loss = 3658.4327005035134
Validation Loss = 3658.4327005035134
Iteration 1960/10000: Training Loss = 3658.064143469774
Validation Loss = 3658.064143469774
Iteration 1961/10000: Training Loss = 3657.707437964893
Validation Loss = 3657.707437964893
Iteration 1962/10000: Training Loss = 3657.335763324248
Validation Loss = 3657.335763324248
Iteration 1963/10000: Training Loss = 3656.9660683593197
Validation Loss = 3656.9660683593197
Iteration 1964/10000: Training Loss = 3656.5940572095888
Validation Loss = 3656.5940572095888
Iteration 1965/10000: Training Loss = 3656.229632320242
Validation Loss = 3656.229632320242
Iteration 1966/10000: Training Loss = 3655.86177158092
Validation Loss = 3655.86177158092
Iteration 1967/10000: Training Loss = 3655.4956938665446
Validation Loss = 3655.4956938665446
Iteration 1968/10000: Training Loss = 3655.122076330211
Validation Loss = 3655.122076330211
Iteration 1969/10000: Training Loss = 3654.763513843093
Validation Loss = 3654.763513843093
Iteration 1970/10000: Training Loss = 3654.39025531334
Validation Loss = 3654.39025531334
Iteration 1971/10000: Training Loss = 3654.0182527175393
Validation Loss = 3654.0182527175393
Iteration 1972/10000: Training Loss = 3653.655347594086
Validation Loss = 3653.655347594086
Iteration 1973/10000: Training Loss = 3653.2963304952405
Validation Loss = 3653.2963304952405
Iteration 1974/10000: Training Loss = 3652.936225878406
Validation Loss = 3652.936225878406
Iteration 1975/10000: Training Loss = 3652.566927736908
Validation Loss = 3652.566927736908
Iteration 1976/10000: Training Loss = 3652.197933599363
Validation Loss = 3652.197933599363
Iteration 1977/10000: Training Loss = 3651.8283820495067
Validation Loss = 3651.8283820495067
Iteration 1978/10000: Training Loss = 3651.4621346326207
Validation Loss = 3651.4621346326207
Iteration 1979/10000: Training Loss = 3651.097150579487
Validation Loss = 3651.097150579487
Iteration 1980/10000: Training Loss = 3650.73714136116
Validation Loss = 3650.73714136116
Iteration 1981/10000: Training Loss = 3650.372362180814
Validation Loss = 3650.372362180814
Iteration 1982/10000: Training Loss = 3650.0059581655005
Validation Loss = 3650.0059581655005
Iteration 1983/10000: Training Loss = 3649.6388097908593
Validation Loss = 3649.6388097908593
Iteration 1984/10000: Training Loss = 3649.2778241561605
Validation Loss = 3649.2778241561605
Iteration 1985/10000: Training Loss = 3648.9202081734675
Validation Loss = 3648.9202081734675
Iteration 1986/10000: Training Loss = 3648.5581396663183
Validation Loss = 3648.5581396663183
Iteration 1987/10000: Training Loss = 3648.208974606977
Validation Loss = 3648.208974606977
Iteration 1988/10000: Training Loss = 3647.8457769225406
Validation Loss = 3647.8457769225406
Iteration 1989/10000: Training Loss = 3647.4778549197185
Validation Loss = 3647.4778549197185
Iteration 1990/10000: Training Loss = 3647.1056437528605
Validation Loss = 3647.1056437528605
Iteration 1991/10000: Training Loss = 3646.734020860052
Validation Loss = 3646.734020860052
Iteration 1992/10000: Training Loss = 3646.380003092027
Validation Loss = 3646.380003092027
Iteration 1993/10000: Training Loss = 3646.028900838626
Validation Loss = 3646.028900838626
Iteration 1994/10000: Training Loss = 3645.6633086938627
Validation Loss = 3645.6633086938627
Iteration 1995/10000: Training Loss = 3645.298270680033
Validation Loss = 3645.298270680033
Iteration 1996/10000: Training Loss = 3644.929091512578
Validation Loss = 3644.929091512578
Iteration 1997/10000: Training Loss = 3644.5655269554036
Validation Loss = 3644.5655269554036
Iteration 1998/10000: Training Loss = 3644.213899950457
Validation Loss = 3644.213899950457
Iteration 1999/10000: Training Loss = 3643.849088745223
Validation Loss = 3643.849088745223
Iteration 2000/10000: Training Loss = 3643.4834173208365
Validation Loss = 3643.4834173208365
Iteration 2001/10000: Training Loss = 3643.1333176088183
Validation Loss = 3643.1333176088183
Iteration 2002/10000: Training Loss = 3642.7777204355407
Validation Loss = 3642.7777204355407
Iteration 2003/10000: Training Loss = 3642.4235210548645
Validation Loss = 3642.4235210548645
Iteration 2004/10000: Training Loss = 3642.0695960077323
Validation Loss = 3642.0695960077323
Iteration 2005/10000: Training Loss = 3641.7043090457414
Validation Loss = 3641.7043090457414
Iteration 2006/10000: Training Loss = 3641.354261291704
Validation Loss = 3641.354261291704
Iteration 2007/10000: Training Loss = 3641.006141699381
Validation Loss = 3641.006141699381
Iteration 2008/10000: Training Loss = 3640.6539536685905
Validation Loss = 3640.6539536685905
Iteration 2009/10000: Training Loss = 3640.3065959683877
Validation Loss = 3640.3065959683877
Iteration 2010/10000: Training Loss = 3639.9513249010374
Validation Loss = 3639.9513249010374
Iteration 2011/10000: Training Loss = 3639.591530980823
Validation Loss = 3639.591530980823
Iteration 2012/10000: Training Loss = 3639.2250103284764
Validation Loss = 3639.2250103284764
Iteration 2013/10000: Training Loss = 3638.880102911243
Validation Loss = 3638.880102911243
Iteration 2014/10000: Training Loss = 3638.5197695897054
Validation Loss = 3638.5197695897054
Iteration 2015/10000: Training Loss = 3638.16734554845
Validation Loss = 3638.16734554845
Iteration 2016/10000: Training Loss = 3637.8097959436564
Validation Loss = 3637.8097959436564
Iteration 2017/10000: Training Loss = 3637.4542114785577
Validation Loss = 3637.4542114785577
Iteration 2018/10000: Training Loss = 3637.099192129498
Validation Loss = 3637.099192129498
Iteration 2019/10000: Training Loss = 3636.7489163068217
Validation Loss = 3636.7489163068217
Iteration 2020/10000: Training Loss = 3636.394614208938
Validation Loss = 3636.394614208938
Iteration 2021/10000: Training Loss = 3636.044482180388
Validation Loss = 3636.044482180388
Iteration 2022/10000: Training Loss = 3635.685227518264
Validation Loss = 3635.685227518264
Iteration 2023/10000: Training Loss = 3635.333098275273
Validation Loss = 3635.333098275273
Iteration 2024/10000: Training Loss = 3634.9859986806646
Validation Loss = 3634.9859986806646
Iteration 2025/10000: Training Loss = 3634.6334366440833
Validation Loss = 3634.6334366440833
Iteration 2026/10000: Training Loss = 3634.286504737811
Validation Loss = 3634.286504737811
Iteration 2027/10000: Training Loss = 3633.954653633734
Validation Loss = 3633.954653633734
Iteration 2028/10000: Training Loss = 3633.5972816940616
Validation Loss = 3633.5972816940616
Iteration 2029/10000: Training Loss = 3633.233416695701
Validation Loss = 3633.233416695701
Iteration 2030/10000: Training Loss = 3632.8830871114005
Validation Loss = 3632.8830871114005
Iteration 2031/10000: Training Loss = 3632.5410194526135
Validation Loss = 3632.5410194526135
Iteration 2032/10000: Training Loss = 3632.199040181026
Validation Loss = 3632.199040181026
Iteration 2033/10000: Training Loss = 3631.845951764414
Validation Loss = 3631.845951764414
Iteration 2034/10000: Training Loss = 3631.4989407176136
Validation Loss = 3631.4989407176136
Iteration 2035/10000: Training Loss = 3631.149261454589
Validation Loss = 3631.149261454589
Iteration 2036/10000: Training Loss = 3630.803028849996
Validation Loss = 3630.803028849996
Iteration 2037/10000: Training Loss = 3630.452461341689
Validation Loss = 3630.452461341689
Iteration 2038/10000: Training Loss = 3630.1115218674513
Validation Loss = 3630.1115218674513
Iteration 2039/10000: Training Loss = 3629.770847306153
Validation Loss = 3629.770847306153
Iteration 2040/10000: Training Loss = 3629.415210348913
Validation Loss = 3629.415210348913
Iteration 2041/10000: Training Loss = 3629.0729728963765
Validation Loss = 3629.0729728963765
Iteration 2042/10000: Training Loss = 3628.725394602471
Validation Loss = 3628.725394602471
Iteration 2043/10000: Training Loss = 3628.3790192120537
Validation Loss = 3628.3790192120537
Iteration 2044/10000: Training Loss = 3628.0354161553905
Validation Loss = 3628.0354161553905
Iteration 2045/10000: Training Loss = 3627.689585501676
Validation Loss = 3627.689585501676
Iteration 2046/10000: Training Loss = 3627.341534663971
Validation Loss = 3627.341534663971
Iteration 2047/10000: Training Loss = 3626.9882438671066
Validation Loss = 3626.9882438671066
Iteration 2048/10000: Training Loss = 3626.6396695488243
Validation Loss = 3626.6396695488243
Iteration 2049/10000: Training Loss = 3626.2967637670104
Validation Loss = 3626.2967637670104
Iteration 2050/10000: Training Loss = 3625.959379639551
Validation Loss = 3625.959379639551
Iteration 2051/10000: Training Loss = 3625.6176962960767
Validation Loss = 3625.6176962960767
Iteration 2052/10000: Training Loss = 3625.2712504865085
Validation Loss = 3625.2712504865085
Iteration 2053/10000: Training Loss = 3624.929312482078
Validation Loss = 3624.929312482078
Iteration 2054/10000: Training Loss = 3624.5810365352863
Validation Loss = 3624.5810365352863
Iteration 2055/10000: Training Loss = 3624.241136244999
Validation Loss = 3624.241136244999
Iteration 2056/10000: Training Loss = 3623.9027369175096
Validation Loss = 3623.9027369175096
Iteration 2057/10000: Training Loss = 3623.569455474304
Validation Loss = 3623.569455474304
Iteration 2058/10000: Training Loss = 3623.22506087909
Validation Loss = 3623.22506087909
Iteration 2059/10000: Training Loss = 3622.8798100239965
Validation Loss = 3622.8798100239965
Iteration 2060/10000: Training Loss = 3622.555024099944
Validation Loss = 3622.555024099944
Iteration 2061/10000: Training Loss = 3622.2200934606085
Validation Loss = 3622.2200934606085
Iteration 2062/10000: Training Loss = 3621.8799080829135
Validation Loss = 3621.8799080829135
Iteration 2063/10000: Training Loss = 3621.5525359213834
Validation Loss = 3621.5525359213834
Iteration 2064/10000: Training Loss = 3621.199413183569
Validation Loss = 3621.199413183569
Iteration 2065/10000: Training Loss = 3620.867422116488
Validation Loss = 3620.867422116488
Iteration 2066/10000: Training Loss = 3620.5204682325907
Validation Loss = 3620.5204682325907
Iteration 2067/10000: Training Loss = 3620.185251168155
Validation Loss = 3620.185251168155
Iteration 2068/10000: Training Loss = 3619.8383852924608
Validation Loss = 3619.8383852924608
Iteration 2069/10000: Training Loss = 3619.496951786484
Validation Loss = 3619.496951786484
Iteration 2070/10000: Training Loss = 3619.1582131814357
Validation Loss = 3619.1582131814357
Iteration 2071/10000: Training Loss = 3618.8179955279816
Validation Loss = 3618.8179955279816
Iteration 2072/10000: Training Loss = 3618.483945849045
Validation Loss = 3618.483945849045
Iteration 2073/10000: Training Loss = 3618.1496458511774
Validation Loss = 3618.1496458511774
Iteration 2074/10000: Training Loss = 3617.804927968336
Validation Loss = 3617.804927968336
Iteration 2075/10000: Training Loss = 3617.471020752075
Validation Loss = 3617.471020752075
Iteration 2076/10000: Training Loss = 3617.1372911990584
Validation Loss = 3617.1372911990584
Iteration 2077/10000: Training Loss = 3616.8067353883243
Validation Loss = 3616.8067353883243
Iteration 2078/10000: Training Loss = 3616.4796229555277
Validation Loss = 3616.4796229555277
Iteration 2079/10000: Training Loss = 3616.1481724083333
Validation Loss = 3616.1481724083333
Iteration 2080/10000: Training Loss = 3615.825139554958
Validation Loss = 3615.825139554958
Iteration 2081/10000: Training Loss = 3615.4688055290817
Validation Loss = 3615.4688055290817
Iteration 2082/10000: Training Loss = 3615.1315716217396
Validation Loss = 3615.1315716217396
Iteration 2083/10000: Training Loss = 3614.7910638777835
Validation Loss = 3614.7910638777835
Iteration 2084/10000: Training Loss = 3614.452084480014
Validation Loss = 3614.452084480014
Iteration 2085/10000: Training Loss = 3614.115635522313
Validation Loss = 3614.115635522313
Iteration 2086/10000: Training Loss = 3613.784472699408
Validation Loss = 3613.784472699408
Iteration 2087/10000: Training Loss = 3613.4469413415086
Validation Loss = 3613.4469413415086
Iteration 2088/10000: Training Loss = 3613.111830336252
Validation Loss = 3613.111830336252
Iteration 2089/10000: Training Loss = 3612.7783295542245
Validation Loss = 3612.7783295542245
Iteration 2090/10000: Training Loss = 3612.4474960662155
Validation Loss = 3612.4474960662155
Iteration 2091/10000: Training Loss = 3612.1133939069437
Validation Loss = 3612.1133939069437
Iteration 2092/10000: Training Loss = 3611.7885697116503
Validation Loss = 3611.7885697116503
Iteration 2093/10000: Training Loss = 3611.4612774817333
Validation Loss = 3611.4612774817333
Iteration 2094/10000: Training Loss = 3611.12332885624
Validation Loss = 3611.12332885624
Iteration 2095/10000: Training Loss = 3610.7969500279237
Validation Loss = 3610.7969500279237
Iteration 2096/10000: Training Loss = 3610.4646874563623
Validation Loss = 3610.4646874563623
Iteration 2097/10000: Training Loss = 3610.132949686881
Validation Loss = 3610.132949686881
Iteration 2098/10000: Training Loss = 3609.7883468534355
Validation Loss = 3609.7883468534355
Iteration 2099/10000: Training Loss = 3609.46688778679
Validation Loss = 3609.46688778679
Iteration 2100/10000: Training Loss = 3609.1350234098045
Validation Loss = 3609.1350234098045
Iteration 2101/10000: Training Loss = 3608.805613707326
Validation Loss = 3608.805613707326
Iteration 2102/10000: Training Loss = 3608.4821033357575
Validation Loss = 3608.4821033357575
Iteration 2103/10000: Training Loss = 3608.148024281074
Validation Loss = 3608.148024281074
Iteration 2104/10000: Training Loss = 3607.8197688034275
Validation Loss = 3607.8197688034275
Iteration 2105/10000: Training Loss = 3607.493204067759
Validation Loss = 3607.493204067759
Iteration 2106/10000: Training Loss = 3607.1628628189196
Validation Loss = 3607.1628628189196
Iteration 2107/10000: Training Loss = 3606.819538357893
Validation Loss = 3606.819538357893
Iteration 2108/10000: Training Loss = 3606.495905842243
Validation Loss = 3606.495905842243
Iteration 2109/10000: Training Loss = 3606.165249118387
Validation Loss = 3606.165249118387
Iteration 2110/10000: Training Loss = 3605.8439161369847
Validation Loss = 3605.8439161369847
Iteration 2111/10000: Training Loss = 3605.523694028676
Validation Loss = 3605.523694028676
Iteration 2112/10000: Training Loss = 3605.2084565530113
Validation Loss = 3605.2084565530113
Iteration 2113/10000: Training Loss = 3604.8809039243315
Validation Loss = 3604.8809039243315
Iteration 2114/10000: Training Loss = 3604.542688267057
Validation Loss = 3604.542688267057
Iteration 2115/10000: Training Loss = 3604.23208704955
Validation Loss = 3604.23208704955
Iteration 2116/10000: Training Loss = 3603.9077916231013
Validation Loss = 3603.9077916231013
Iteration 2117/10000: Training Loss = 3603.5877586546067
Validation Loss = 3603.5877586546067
Iteration 2118/10000: Training Loss = 3603.239637935984
Validation Loss = 3603.239637935984
Iteration 2119/10000: Training Loss = 3602.8987316847374
Validation Loss = 3602.8987316847374
Iteration 2120/10000: Training Loss = 3602.574605798926
Validation Loss = 3602.574605798926
Iteration 2121/10000: Training Loss = 3602.249737548513
Validation Loss = 3602.249737548513
Iteration 2122/10000: Training Loss = 3601.9342442209245
Validation Loss = 3601.9342442209245
Iteration 2123/10000: Training Loss = 3601.6159910348447
Validation Loss = 3601.6159910348447
Iteration 2124/10000: Training Loss = 3601.275490384468
Validation Loss = 3601.275490384468
Iteration 2125/10000: Training Loss = 3600.9534514425536
Validation Loss = 3600.9534514425536
Iteration 2126/10000: Training Loss = 3600.6252662851357
Validation Loss = 3600.6252662851357
Iteration 2127/10000: Training Loss = 3600.3022526760906
Validation Loss = 3600.3022526760906
Iteration 2128/10000: Training Loss = 3599.98455784608
Validation Loss = 3599.98455784608
Iteration 2129/10000: Training Loss = 3599.6637662532985
Validation Loss = 3599.6637662532985
Iteration 2130/10000: Training Loss = 3599.3351712638046
Validation Loss = 3599.3351712638046
Iteration 2131/10000: Training Loss = 3599.004353834752
Validation Loss = 3599.004353834752
Iteration 2132/10000: Training Loss = 3598.6835314601794
Validation Loss = 3598.6835314601794
Iteration 2133/10000: Training Loss = 3598.3619657738072
Validation Loss = 3598.3619657738072
Iteration 2134/10000: Training Loss = 3598.040191021402
Validation Loss = 3598.040191021402
Iteration 2135/10000: Training Loss = 3597.7142109296988
Validation Loss = 3597.7142109296988
Iteration 2136/10000: Training Loss = 3597.389141207967
Validation Loss = 3597.389141207967
Iteration 2137/10000: Training Loss = 3597.0584795600917
Validation Loss = 3597.0584795600917
Iteration 2138/10000: Training Loss = 3596.7391005049326
Validation Loss = 3596.7391005049326
Iteration 2139/10000: Training Loss = 3596.4148726785875
Validation Loss = 3596.4148726785875
Iteration 2140/10000: Training Loss = 3596.094088507027
Validation Loss = 3596.094088507027
Iteration 2141/10000: Training Loss = 3595.7759557797526
Validation Loss = 3595.7759557797526
Iteration 2142/10000: Training Loss = 3595.4559510392583
Validation Loss = 3595.4559510392583
Iteration 2143/10000: Training Loss = 3595.1313757673893
Validation Loss = 3595.1313757673893
Iteration 2144/10000: Training Loss = 3594.806279666386
Validation Loss = 3594.806279666386
Iteration 2145/10000: Training Loss = 3594.485823779619
Validation Loss = 3594.485823779619
Iteration 2146/10000: Training Loss = 3594.173841488869
Validation Loss = 3594.173841488869
Iteration 2147/10000: Training Loss = 3593.865862041691
Validation Loss = 3593.865862041691
Iteration 2148/10000: Training Loss = 3593.53834871628
Validation Loss = 3593.53834871628
Iteration 2149/10000: Training Loss = 3593.22102819447
Validation Loss = 3593.22102819447
Iteration 2150/10000: Training Loss = 3592.913003495294
Validation Loss = 3592.913003495294
Iteration 2151/10000: Training Loss = 3592.596051152352
Validation Loss = 3592.596051152352
Iteration 2152/10000: Training Loss = 3592.278742928726
Validation Loss = 3592.278742928726
Iteration 2153/10000: Training Loss = 3591.953320174635
Validation Loss = 3591.953320174635
Iteration 2154/10000: Training Loss = 3591.6330186875634
Validation Loss = 3591.6330186875634
Iteration 2155/10000: Training Loss = 3591.315022560558
Validation Loss = 3591.315022560558
Iteration 2156/10000: Training Loss = 3590.9913640584587
Validation Loss = 3590.9913640584587
Iteration 2157/10000: Training Loss = 3590.6850077985473
Validation Loss = 3590.6850077985473
Iteration 2158/10000: Training Loss = 3590.3767741848064
Validation Loss = 3590.3767741848064
Iteration 2159/10000: Training Loss = 3590.06054776517
Validation Loss = 3590.06054776517
Iteration 2160/10000: Training Loss = 3589.756306894675
Validation Loss = 3589.756306894675
Iteration 2161/10000: Training Loss = 3589.444605518173
Validation Loss = 3589.444605518173
Iteration 2162/10000: Training Loss = 3589.1141003910748
Validation Loss = 3589.1141003910748
Iteration 2163/10000: Training Loss = 3588.779588536678
Validation Loss = 3588.779588536678
Iteration 2164/10000: Training Loss = 3588.463227665885
Validation Loss = 3588.463227665885
Iteration 2165/10000: Training Loss = 3588.1473124748377
Validation Loss = 3588.1473124748377
Iteration 2166/10000: Training Loss = 3587.838499725338
Validation Loss = 3587.838499725338
Iteration 2167/10000: Training Loss = 3587.521505317436
Validation Loss = 3587.521505317436
Iteration 2168/10000: Training Loss = 3587.199408749803
Validation Loss = 3587.199408749803
Iteration 2169/10000: Training Loss = 3586.891934795927
Validation Loss = 3586.891934795927
Iteration 2170/10000: Training Loss = 3586.5767789846445
Validation Loss = 3586.5767789846445
Iteration 2171/10000: Training Loss = 3586.264421113677
Validation Loss = 3586.264421113677
Iteration 2172/10000: Training Loss = 3585.956786115706
Validation Loss = 3585.956786115706
Iteration 2173/10000: Training Loss = 3585.6498476896563
Validation Loss = 3585.6498476896563
Iteration 2174/10000: Training Loss = 3585.3360271827
Validation Loss = 3585.3360271827
Iteration 2175/10000: Training Loss = 3585.030156124334
Validation Loss = 3585.030156124334
Iteration 2176/10000: Training Loss = 3584.729847581714
Validation Loss = 3584.729847581714
Iteration 2177/10000: Training Loss = 3584.420429491999
Validation Loss = 3584.420429491999
Iteration 2178/10000: Training Loss = 3584.1024175587427
Validation Loss = 3584.1024175587427
Iteration 2179/10000: Training Loss = 3583.7915764765307
Validation Loss = 3583.7915764765307
Iteration 2180/10000: Training Loss = 3583.4789006458204
Validation Loss = 3583.4789006458204
Iteration 2181/10000: Training Loss = 3583.1679837987526
Validation Loss = 3583.1679837987526
Iteration 2182/10000: Training Loss = 3582.854345553705
Validation Loss = 3582.854345553705
Iteration 2183/10000: Training Loss = 3582.5448738599366
Validation Loss = 3582.5448738599366
Iteration 2184/10000: Training Loss = 3582.229831366622
Validation Loss = 3582.229831366622
Iteration 2185/10000: Training Loss = 3581.916411821179
Validation Loss = 3581.916411821179
Iteration 2186/10000: Training Loss = 3581.609990208875
Validation Loss = 3581.609990208875
Iteration 2187/10000: Training Loss = 3581.296432825603
Validation Loss = 3581.296432825603
Iteration 2188/10000: Training Loss = 3580.9906357555183
Validation Loss = 3580.9906357555183
Iteration 2189/10000: Training Loss = 3580.677516237715
Validation Loss = 3580.677516237715
Iteration 2190/10000: Training Loss = 3580.3667889252233
Validation Loss = 3580.3667889252233
Iteration 2191/10000: Training Loss = 3580.0593264215804
Validation Loss = 3580.0593264215804
Iteration 2192/10000: Training Loss = 3579.7542283243297
Validation Loss = 3579.7542283243297
Iteration 2193/10000: Training Loss = 3579.441823003522
Validation Loss = 3579.441823003522
Iteration 2194/10000: Training Loss = 3579.137020025997
Validation Loss = 3579.137020025997
Iteration 2195/10000: Training Loss = 3578.830573464029
Validation Loss = 3578.830573464029
Iteration 2196/10000: Training Loss = 3578.5238734231366
Validation Loss = 3578.5238734231366
Iteration 2197/10000: Training Loss = 3578.2334313611645
Validation Loss = 3578.2334313611645
Iteration 2198/10000: Training Loss = 3577.923672242847
Validation Loss = 3577.923672242847
Iteration 2199/10000: Training Loss = 3577.616786652236
Validation Loss = 3577.616786652236
Iteration 2200/10000: Training Loss = 3577.303017964468
Validation Loss = 3577.303017964468
Iteration 2201/10000: Training Loss = 3576.990791610022
Validation Loss = 3576.990791610022
Iteration 2202/10000: Training Loss = 3576.6776332378045
Validation Loss = 3576.6776332378045
Iteration 2203/10000: Training Loss = 3576.373259095572
Validation Loss = 3576.373259095572
Iteration 2204/10000: Training Loss = 3576.0687023770415
Validation Loss = 3576.0687023770415
Iteration 2205/10000: Training Loss = 3575.7689867624276
Validation Loss = 3575.7689867624276
Iteration 2206/10000: Training Loss = 3575.466163361946
Validation Loss = 3575.466163361946
Iteration 2207/10000: Training Loss = 3575.155027967908
Validation Loss = 3575.155027967908
Iteration 2208/10000: Training Loss = 3574.8516286317185
Validation Loss = 3574.8516286317185
Iteration 2209/10000: Training Loss = 3574.5488619842604
Validation Loss = 3574.5488619842604
Iteration 2210/10000: Training Loss = 3574.244362601563
Validation Loss = 3574.244362601563
Iteration 2211/10000: Training Loss = 3573.944973735673
Validation Loss = 3573.944973735673
Iteration 2212/10000: Training Loss = 3573.6434963984684
Validation Loss = 3573.6434963984684
Iteration 2213/10000: Training Loss = 3573.342189752618
Validation Loss = 3573.342189752618
Iteration 2214/10000: Training Loss = 3573.0331876012897
Validation Loss = 3573.0331876012897
Iteration 2215/10000: Training Loss = 3572.7275893261094
Validation Loss = 3572.7275893261094
Iteration 2216/10000: Training Loss = 3572.4278528296327
Validation Loss = 3572.4278528296327
Iteration 2217/10000: Training Loss = 3572.1340865839347
Validation Loss = 3572.1340865839347
Iteration 2218/10000: Training Loss = 3571.8258254962066
Validation Loss = 3571.8258254962066
Iteration 2219/10000: Training Loss = 3571.5284227092493
Validation Loss = 3571.5284227092493
Iteration 2220/10000: Training Loss = 3571.224081569343
Validation Loss = 3571.224081569343
Iteration 2221/10000: Training Loss = 3570.9309424070866
Validation Loss = 3570.9309424070866
Iteration 2222/10000: Training Loss = 3570.616423780818
Validation Loss = 3570.616423780818
Iteration 2223/10000: Training Loss = 3570.3087913114045
Validation Loss = 3570.3087913114045
Iteration 2224/10000: Training Loss = 3570.0108856622173
Validation Loss = 3570.0108856622173
Iteration 2225/10000: Training Loss = 3569.7027964174777
Validation Loss = 3569.7027964174777
Iteration 2226/10000: Training Loss = 3569.4017795191244
Validation Loss = 3569.4017795191244
Iteration 2227/10000: Training Loss = 3569.1042864741635
Validation Loss = 3569.1042864741635
Iteration 2228/10000: Training Loss = 3568.8018865204954
Validation Loss = 3568.8018865204954
Iteration 2229/10000: Training Loss = 3568.4993799434565
Validation Loss = 3568.4993799434565
Iteration 2230/10000: Training Loss = 3568.1967688134255
Validation Loss = 3568.1967688134255
Iteration 2231/10000: Training Loss = 3567.896967356276
Validation Loss = 3567.896967356276
Iteration 2232/10000: Training Loss = 3567.599453536552
Validation Loss = 3567.599453536552
Iteration 2233/10000: Training Loss = 3567.298849755479
Validation Loss = 3567.298849755479
Iteration 2234/10000: Training Loss = 3566.999426527444
Validation Loss = 3566.999426527444
Iteration 2235/10000: Training Loss = 3566.696945548468
Validation Loss = 3566.696945548468
Iteration 2236/10000: Training Loss = 3566.3936898430247
Validation Loss = 3566.3936898430247
Iteration 2237/10000: Training Loss = 3566.097793021462
Validation Loss = 3566.097793021462
Iteration 2238/10000: Training Loss = 3565.796963345912
Validation Loss = 3565.796963345912
Iteration 2239/10000: Training Loss = 3565.4956054686313
Validation Loss = 3565.4956054686313
Iteration 2240/10000: Training Loss = 3565.2065461674497
Validation Loss = 3565.2065461674497
Iteration 2241/10000: Training Loss = 3564.907654627104
Validation Loss = 3564.907654627104
Iteration 2242/10000: Training Loss = 3564.6049545501014
Validation Loss = 3564.6049545501014
Iteration 2243/10000: Training Loss = 3564.305168257795
Validation Loss = 3564.305168257795
Iteration 2244/10000: Training Loss = 3563.99868678986
Validation Loss = 3563.99868678986
Iteration 2245/10000: Training Loss = 3563.7034335117023
Validation Loss = 3563.7034335117023
Iteration 2246/10000: Training Loss = 3563.405202297261
Validation Loss = 3563.405202297261
Iteration 2247/10000: Training Loss = 3563.1063685574463
Validation Loss = 3563.1063685574463
Iteration 2248/10000: Training Loss = 3562.8058840759186
Validation Loss = 3562.8058840759186
Iteration 2249/10000: Training Loss = 3562.5093053319765
Validation Loss = 3562.5093053319765
Iteration 2250/10000: Training Loss = 3562.2152268829836
Validation Loss = 3562.2152268829836
Iteration 2251/10000: Training Loss = 3561.9283136789822
Validation Loss = 3561.9283136789822
Iteration 2252/10000: Training Loss = 3561.6338307194756
Validation Loss = 3561.6338307194756
Iteration 2253/10000: Training Loss = 3561.328433759985
Validation Loss = 3561.328433759985
Iteration 2254/10000: Training Loss = 3561.0344036044494
Validation Loss = 3561.0344036044494
Iteration 2255/10000: Training Loss = 3560.7387303364953
Validation Loss = 3560.7387303364953
Iteration 2256/10000: Training Loss = 3560.4395396466766
Validation Loss = 3560.4395396466766
Iteration 2257/10000: Training Loss = 3560.1428624937353
Validation Loss = 3560.1428624937353
Iteration 2258/10000: Training Loss = 3559.8455118982383
Validation Loss = 3559.8455118982383
Iteration 2259/10000: Training Loss = 3559.550039862955
Validation Loss = 3559.550039862955
Iteration 2260/10000: Training Loss = 3559.263695550791
Validation Loss = 3559.263695550791
Iteration 2261/10000: Training Loss = 3558.973857486868
Validation Loss = 3558.973857486868
Iteration 2262/10000: Training Loss = 3558.6811864738365
Validation Loss = 3558.6811864738365
Iteration 2263/10000: Training Loss = 3558.384559720056
Validation Loss = 3558.384559720056
Iteration 2264/10000: Training Loss = 3558.1025985216684
Validation Loss = 3558.1025985216684
Iteration 2265/10000: Training Loss = 3557.7989291989857
Validation Loss = 3557.7989291989857
Iteration 2266/10000: Training Loss = 3557.5062444625974
Validation Loss = 3557.5062444625974
Iteration 2267/10000: Training Loss = 3557.211494154038
Validation Loss = 3557.211494154038
Iteration 2268/10000: Training Loss = 3556.924191153697
Validation Loss = 3556.924191153697
Iteration 2269/10000: Training Loss = 3556.6341970209064
Validation Loss = 3556.6341970209064
Iteration 2270/10000: Training Loss = 3556.3378243785733
Validation Loss = 3556.3378243785733
Iteration 2271/10000: Training Loss = 3556.0366395749984
Validation Loss = 3556.0366395749984
Iteration 2272/10000: Training Loss = 3555.7429314598253
Validation Loss = 3555.7429314598253
Iteration 2273/10000: Training Loss = 3555.4511614437447
Validation Loss = 3555.4511614437447
Iteration 2274/10000: Training Loss = 3555.162399585996
Validation Loss = 3555.162399585996
Iteration 2275/10000: Training Loss = 3554.870752958406
Validation Loss = 3554.870752958406
Iteration 2276/10000: Training Loss = 3554.5797531107432
Validation Loss = 3554.5797531107432
Iteration 2277/10000: Training Loss = 3554.291532748444
Validation Loss = 3554.291532748444
Iteration 2278/10000: Training Loss = 3554.0024586240447
Validation Loss = 3554.0024586240447
Iteration 2279/10000: Training Loss = 3553.7185502393318
Validation Loss = 3553.7185502393318
Iteration 2280/10000: Training Loss = 3553.424798981221
Validation Loss = 3553.424798981221
Iteration 2281/10000: Training Loss = 3553.149755746588
Validation Loss = 3553.149755746588
Iteration 2282/10000: Training Loss = 3552.85554535167
Validation Loss = 3552.85554535167
Iteration 2283/10000: Training Loss = 3552.56677635759
Validation Loss = 3552.56677635759
Iteration 2284/10000: Training Loss = 3552.273360007196
Validation Loss = 3552.273360007196
Iteration 2285/10000: Training Loss = 3551.9899800938774
Validation Loss = 3551.9899800938774
Iteration 2286/10000: Training Loss = 3551.7020309109853
Validation Loss = 3551.7020309109853
Iteration 2287/10000: Training Loss = 3551.4165995615294
Validation Loss = 3551.4165995615294
Iteration 2288/10000: Training Loss = 3551.134327214707
Validation Loss = 3551.134327214707
Iteration 2289/10000: Training Loss = 3550.8505957777056
Validation Loss = 3550.8505957777056
Iteration 2290/10000: Training Loss = 3550.559582145078
Validation Loss = 3550.559582145078
Iteration 2291/10000: Training Loss = 3550.2719211217127
Validation Loss = 3550.2719211217127
Iteration 2292/10000: Training Loss = 3549.981154247143
Validation Loss = 3549.981154247143
Iteration 2293/10000: Training Loss = 3549.69584858296
Validation Loss = 3549.69584858296
Iteration 2294/10000: Training Loss = 3549.4041226233276
Validation Loss = 3549.4041226233276
Iteration 2295/10000: Training Loss = 3549.1238544403295
Validation Loss = 3549.1238544403295
Iteration 2296/10000: Training Loss = 3548.839773624137
Validation Loss = 3548.839773624137
Iteration 2297/10000: Training Loss = 3548.5591338452737
Validation Loss = 3548.5591338452737
Iteration 2298/10000: Training Loss = 3548.2703670070628
Validation Loss = 3548.2703670070628
Iteration 2299/10000: Training Loss = 3547.974989700818
Validation Loss = 3547.974989700818
Iteration 2300/10000: Training Loss = 3547.6875945779607
Validation Loss = 3547.6875945779607
Iteration 2301/10000: Training Loss = 3547.402579426475
Validation Loss = 3547.402579426475
Iteration 2302/10000: Training Loss = 3547.1149328850684
Validation Loss = 3547.1149328850684
Iteration 2303/10000: Training Loss = 3546.829135870234
Validation Loss = 3546.829135870234
Iteration 2304/10000: Training Loss = 3546.547821129192
Validation Loss = 3546.547821129192
Iteration 2305/10000: Training Loss = 3546.2621514227535
Validation Loss = 3546.2621514227535
Iteration 2306/10000: Training Loss = 3545.9834941200097
Validation Loss = 3545.9834941200097
Iteration 2307/10000: Training Loss = 3545.7078611630127
Validation Loss = 3545.7078611630127
Iteration 2308/10000: Training Loss = 3545.4197104526916
Validation Loss = 3545.4197104526916
Iteration 2309/10000: Training Loss = 3545.1466463149723
Validation Loss = 3545.1466463149723
Iteration 2310/10000: Training Loss = 3544.868773694503
Validation Loss = 3544.868773694503
Iteration 2311/10000: Training Loss = 3544.588722256619
Validation Loss = 3544.588722256619
Iteration 2312/10000: Training Loss = 3544.3081008957233
Validation Loss = 3544.3081008957233
Iteration 2313/10000: Training Loss = 3544.0168809103457
Validation Loss = 3544.0168809103457
Iteration 2314/10000: Training Loss = 3543.735569867162
Validation Loss = 3543.735569867162
Iteration 2315/10000: Training Loss = 3543.446445481962
Validation Loss = 3543.446445481962
Iteration 2316/10000: Training Loss = 3543.1678943149855
Validation Loss = 3543.1678943149855
Iteration 2317/10000: Training Loss = 3542.881223620912
Validation Loss = 3542.881223620912
Iteration 2318/10000: Training Loss = 3542.592726017082
Validation Loss = 3542.592726017082
Iteration 2319/10000: Training Loss = 3542.3106930880613
Validation Loss = 3542.3106930880613
Iteration 2320/10000: Training Loss = 3542.0239972394847
Validation Loss = 3542.0239972394847
Iteration 2321/10000: Training Loss = 3541.74898799543
Validation Loss = 3541.74898799543
Iteration 2322/10000: Training Loss = 3541.4621721452986
Validation Loss = 3541.4621721452986
Iteration 2323/10000: Training Loss = 3541.1777367655504
Validation Loss = 3541.1777367655504
Iteration 2324/10000: Training Loss = 3540.89641253525
Validation Loss = 3540.89641253525
Iteration 2325/10000: Training Loss = 3540.617086194517
Validation Loss = 3540.617086194517
Iteration 2326/10000: Training Loss = 3540.329602311526
Validation Loss = 3540.329602311526
Iteration 2327/10000: Training Loss = 3540.054543973272
Validation Loss = 3540.054543973272
Iteration 2328/10000: Training Loss = 3539.773941000844
Validation Loss = 3539.773941000844
Iteration 2329/10000: Training Loss = 3539.496130135051
Validation Loss = 3539.496130135051
Iteration 2330/10000: Training Loss = 3539.2255079139923
Validation Loss = 3539.2255079139923
Iteration 2331/10000: Training Loss = 3538.94280563326
Validation Loss = 3538.94280563326
Iteration 2332/10000: Training Loss = 3538.6616389891065
Validation Loss = 3538.6616389891065
Iteration 2333/10000: Training Loss = 3538.3782151359683
Validation Loss = 3538.3782151359683
Iteration 2334/10000: Training Loss = 3538.1026382176096
Validation Loss = 3538.1026382176096
Iteration 2335/10000: Training Loss = 3537.8262824563426
Validation Loss = 3537.8262824563426
Iteration 2336/10000: Training Loss = 3537.551503107046
Validation Loss = 3537.551503107046
Iteration 2337/10000: Training Loss = 3537.276033185356
Validation Loss = 3537.276033185356
Iteration 2338/10000: Training Loss = 3536.99419170004
Validation Loss = 3536.99419170004
Iteration 2339/10000: Training Loss = 3536.709567550099
Validation Loss = 3536.709567550099
Iteration 2340/10000: Training Loss = 3536.4322209423517
Validation Loss = 3536.4322209423517
Iteration 2341/10000: Training Loss = 3536.153847785351
Validation Loss = 3536.153847785351
Iteration 2342/10000: Training Loss = 3535.8792242783147
Validation Loss = 3535.8792242783147
Iteration 2343/10000: Training Loss = 3535.5970822850004
Validation Loss = 3535.5970822850004
Iteration 2344/10000: Training Loss = 3535.318818551446
Validation Loss = 3535.318818551446
Iteration 2345/10000: Training Loss = 3535.041554031956
Validation Loss = 3535.041554031956
Iteration 2346/10000: Training Loss = 3534.764008474192
Validation Loss = 3534.764008474192
Iteration 2347/10000: Training Loss = 3534.498922327715
Validation Loss = 3534.498922327715
Iteration 2348/10000: Training Loss = 3534.22149995317
Validation Loss = 3534.22149995317
Iteration 2349/10000: Training Loss = 3533.952236865435
Validation Loss = 3533.952236865435
Iteration 2350/10000: Training Loss = 3533.671805777522
Validation Loss = 3533.671805777522
Iteration 2351/10000: Training Loss = 3533.4010143773203
Validation Loss = 3533.4010143773203
Iteration 2352/10000: Training Loss = 3533.124047014624
Validation Loss = 3533.124047014624
Iteration 2353/10000: Training Loss = 3532.845316313446
Validation Loss = 3532.845316313446
Iteration 2354/10000: Training Loss = 3532.576334231339
Validation Loss = 3532.576334231339
Iteration 2355/10000: Training Loss = 3532.2948401015424
Validation Loss = 3532.2948401015424
Iteration 2356/10000: Training Loss = 3532.020000263339
Validation Loss = 3532.020000263339
Iteration 2357/10000: Training Loss = 3531.7430455431577
Validation Loss = 3531.7430455431577
Iteration 2358/10000: Training Loss = 3531.4661068582895
Validation Loss = 3531.4661068582895
Iteration 2359/10000: Training Loss = 3531.189819366119
Validation Loss = 3531.189819366119
Iteration 2360/10000: Training Loss = 3530.9174132469207
Validation Loss = 3530.9174132469207
Iteration 2361/10000: Training Loss = 3530.648705353142
Validation Loss = 3530.648705353142
Iteration 2362/10000: Training Loss = 3530.372781575307
Validation Loss = 3530.372781575307
Iteration 2363/10000: Training Loss = 3530.098316765466
Validation Loss = 3530.098316765466
Iteration 2364/10000: Training Loss = 3529.8203424402354
Validation Loss = 3529.8203424402354
Iteration 2365/10000: Training Loss = 3529.54899970651
Validation Loss = 3529.54899970651
Iteration 2366/10000: Training Loss = 3529.2767361995398
Validation Loss = 3529.2767361995398
Iteration 2367/10000: Training Loss = 3529.0054450383936
Validation Loss = 3529.0054450383936
Iteration 2368/10000: Training Loss = 3528.732163181586
Validation Loss = 3528.732163181586
Iteration 2369/10000: Training Loss = 3528.453095279673
Validation Loss = 3528.453095279673
Iteration 2370/10000: Training Loss = 3528.1812720659004
Validation Loss = 3528.1812720659004
Iteration 2371/10000: Training Loss = 3527.9127318904675
Validation Loss = 3527.9127318904675
Iteration 2372/10000: Training Loss = 3527.643597816296
Validation Loss = 3527.643597816296
Iteration 2373/10000: Training Loss = 3527.3659668033943
Validation Loss = 3527.3659668033943
Iteration 2374/10000: Training Loss = 3527.0950367310697
Validation Loss = 3527.0950367310697
Iteration 2375/10000: Training Loss = 3526.8223798824106
Validation Loss = 3526.8223798824106
Iteration 2376/10000: Training Loss = 3526.553297683416
Validation Loss = 3526.553297683416
Iteration 2377/10000: Training Loss = 3526.282283882603
Validation Loss = 3526.282283882603
Iteration 2378/10000: Training Loss = 3526.0229903174722
Validation Loss = 3526.0229903174722
Iteration 2379/10000: Training Loss = 3525.7499442785042
Validation Loss = 3525.7499442785042
Iteration 2380/10000: Training Loss = 3525.482176874336
Validation Loss = 3525.482176874336
Iteration 2381/10000: Training Loss = 3525.2142191652883
Validation Loss = 3525.2142191652883
Iteration 2382/10000: Training Loss = 3524.948708975478
Validation Loss = 3524.948708975478
Iteration 2383/10000: Training Loss = 3524.6778893431556
Validation Loss = 3524.6778893431556
Iteration 2384/10000: Training Loss = 3524.409804073365
Validation Loss = 3524.409804073365
Iteration 2385/10000: Training Loss = 3524.1449941177975
Validation Loss = 3524.1449941177975
Iteration 2386/10000: Training Loss = 3523.8741928941067
Validation Loss = 3523.8741928941067
Iteration 2387/10000: Training Loss = 3523.612437674418
Validation Loss = 3523.612437674418
Iteration 2388/10000: Training Loss = 3523.3423607568957
Validation Loss = 3523.3423607568957
Iteration 2389/10000: Training Loss = 3523.0754626532466
Validation Loss = 3523.0754626532466
Iteration 2390/10000: Training Loss = 3522.805218631239
Validation Loss = 3522.805218631239
Iteration 2391/10000: Training Loss = 3522.534243826372
Validation Loss = 3522.534243826372
Iteration 2392/10000: Training Loss = 3522.2680475420907
Validation Loss = 3522.2680475420907
Iteration 2393/10000: Training Loss = 3522.0013987785387
Validation Loss = 3522.0013987785387
Iteration 2394/10000: Training Loss = 3521.7372919921813
Validation Loss = 3521.7372919921813
Iteration 2395/10000: Training Loss = 3521.47436384828
Validation Loss = 3521.47436384828
Iteration 2396/10000: Training Loss = 3521.204838958299
Validation Loss = 3521.204838958299
Iteration 2397/10000: Training Loss = 3520.94397247622
Validation Loss = 3520.94397247622
Iteration 2398/10000: Training Loss = 3520.6788076021057
Validation Loss = 3520.6788076021057
Iteration 2399/10000: Training Loss = 3520.405113410515
Validation Loss = 3520.405113410515
Iteration 2400/10000: Training Loss = 3520.1454936323603
Validation Loss = 3520.1454936323603
Iteration 2401/10000: Training Loss = 3519.8781469363785
Validation Loss = 3519.8781469363785
Iteration 2402/10000: Training Loss = 3519.609205546066
Validation Loss = 3519.609205546066
Iteration 2403/10000: Training Loss = 3519.338987605657
Validation Loss = 3519.338987605657
Iteration 2404/10000: Training Loss = 3519.0738308364002
Validation Loss = 3519.0738308364002
Iteration 2405/10000: Training Loss = 3518.8078357271374
Validation Loss = 3518.8078357271374
Iteration 2406/10000: Training Loss = 3518.5385680161403
Validation Loss = 3518.5385680161403
Iteration 2407/10000: Training Loss = 3518.2681578293823
Validation Loss = 3518.2681578293823
Iteration 2408/10000: Training Loss = 3518.00607835711
Validation Loss = 3518.00607835711
Iteration 2409/10000: Training Loss = 3517.7374295260647
Validation Loss = 3517.7374295260647
Iteration 2410/10000: Training Loss = 3517.4760282061834
Validation Loss = 3517.4760282061834
Iteration 2411/10000: Training Loss = 3517.210445466038
Validation Loss = 3517.210445466038
Iteration 2412/10000: Training Loss = 3516.950188784813
Validation Loss = 3516.950188784813
Iteration 2413/10000: Training Loss = 3516.6868560782755
Validation Loss = 3516.6868560782755
Iteration 2414/10000: Training Loss = 3516.4220242484976
Validation Loss = 3516.4220242484976
Iteration 2415/10000: Training Loss = 3516.163418822775
Validation Loss = 3516.163418822775
Iteration 2416/10000: Training Loss = 3515.9025700970833
Validation Loss = 3515.9025700970833
Iteration 2417/10000: Training Loss = 3515.6410644903804
Validation Loss = 3515.6410644903804
Iteration 2418/10000: Training Loss = 3515.3811676311393
Validation Loss = 3515.3811676311393
Iteration 2419/10000: Training Loss = 3515.119610121041
Validation Loss = 3515.119610121041
Iteration 2420/10000: Training Loss = 3514.8567212498806
Validation Loss = 3514.8567212498806
Iteration 2421/10000: Training Loss = 3514.588921289674
Validation Loss = 3514.588921289674
Iteration 2422/10000: Training Loss = 3514.329941457091
Validation Loss = 3514.329941457091
Iteration 2423/10000: Training Loss = 3514.073926509755
Validation Loss = 3514.073926509755
Iteration 2424/10000: Training Loss = 3513.815286463435
Validation Loss = 3513.815286463435
Iteration 2425/10000: Training Loss = 3513.5557124416796
Validation Loss = 3513.5557124416796
Iteration 2426/10000: Training Loss = 3513.2949168223404
Validation Loss = 3513.2949168223404
Iteration 2427/10000: Training Loss = 3513.0323281790675
Validation Loss = 3513.0323281790675
Iteration 2428/10000: Training Loss = 3512.7642242829966
Validation Loss = 3512.7642242829966
Iteration 2429/10000: Training Loss = 3512.50853426181
Validation Loss = 3512.50853426181
Iteration 2430/10000: Training Loss = 3512.247110449725
Validation Loss = 3512.247110449725
Iteration 2431/10000: Training Loss = 3511.9898806151823
Validation Loss = 3511.9898806151823
Iteration 2432/10000: Training Loss = 3511.729811636405
Validation Loss = 3511.729811636405
Iteration 2433/10000: Training Loss = 3511.4721961536115
Validation Loss = 3511.4721961536115
Iteration 2434/10000: Training Loss = 3511.2181541477257
Validation Loss = 3511.2181541477257
Iteration 2435/10000: Training Loss = 3510.9576610226723
Validation Loss = 3510.9576610226723
Iteration 2436/10000: Training Loss = 3510.709758906636
Validation Loss = 3510.709758906636
Iteration 2437/10000: Training Loss = 3510.46044445509
Validation Loss = 3510.46044445509
Iteration 2438/10000: Training Loss = 3510.212467956098
Validation Loss = 3510.212467956098
Iteration 2439/10000: Training Loss = 3509.953028520084
Validation Loss = 3509.953028520084
Iteration 2440/10000: Training Loss = 3509.7004110761704
Validation Loss = 3509.7004110761704
Iteration 2441/10000: Training Loss = 3509.425822566821
Validation Loss = 3509.425822566821
Iteration 2442/10000: Training Loss = 3509.164576660641
Validation Loss = 3509.164576660641
Iteration 2443/10000: Training Loss = 3508.9043121040067
Validation Loss = 3508.9043121040067
Iteration 2444/10000: Training Loss = 3508.648204232642
Validation Loss = 3508.648204232642
Iteration 2445/10000: Training Loss = 3508.3892491978836
Validation Loss = 3508.3892491978836
Iteration 2446/10000: Training Loss = 3508.137996058901
Validation Loss = 3508.137996058901
Iteration 2447/10000: Training Loss = 3507.8827585821873
Validation Loss = 3507.8827585821873
Iteration 2448/10000: Training Loss = 3507.6228872489705
Validation Loss = 3507.6228872489705
Iteration 2449/10000: Training Loss = 3507.3612045962877
Validation Loss = 3507.3612045962877
Iteration 2450/10000: Training Loss = 3507.1042892250475
Validation Loss = 3507.1042892250475
Iteration 2451/10000: Training Loss = 3506.846656515072
Validation Loss = 3506.846656515072
Iteration 2452/10000: Training Loss = 3506.5884105200703
Validation Loss = 3506.5884105200703
Iteration 2453/10000: Training Loss = 3506.3407299924042
Validation Loss = 3506.3407299924042
Iteration 2454/10000: Training Loss = 3506.0787813063844
Validation Loss = 3506.0787813063844
Iteration 2455/10000: Training Loss = 3505.829552549274
Validation Loss = 3505.829552549274
Iteration 2456/10000: Training Loss = 3505.572164842725
Validation Loss = 3505.572164842725
Iteration 2457/10000: Training Loss = 3505.313236128823
Validation Loss = 3505.313236128823
Iteration 2458/10000: Training Loss = 3505.0594846743284
Validation Loss = 3505.0594846743284
Iteration 2459/10000: Training Loss = 3504.811084947904
Validation Loss = 3504.811084947904
Iteration 2460/10000: Training Loss = 3504.550487543021
Validation Loss = 3504.550487543021
Iteration 2461/10000: Training Loss = 3504.29657804005
Validation Loss = 3504.29657804005
Iteration 2462/10000: Training Loss = 3504.0421959034516
Validation Loss = 3504.0421959034516
Iteration 2463/10000: Training Loss = 3503.7869732067566
Validation Loss = 3503.7869732067566
Iteration 2464/10000: Training Loss = 3503.535000457298
Validation Loss = 3503.535000457298
Iteration 2465/10000: Training Loss = 3503.2785508622096
Validation Loss = 3503.2785508622096
Iteration 2466/10000: Training Loss = 3503.0288995621927
Validation Loss = 3503.0288995621927
Iteration 2467/10000: Training Loss = 3502.772139559675
Validation Loss = 3502.772139559675
Iteration 2468/10000: Training Loss = 3502.52718040835
Validation Loss = 3502.52718040835
Iteration 2469/10000: Training Loss = 3502.2665580732405
Validation Loss = 3502.2665580732405
Iteration 2470/10000: Training Loss = 3502.015211454296
Validation Loss = 3502.015211454296
Iteration 2471/10000: Training Loss = 3501.765476285579
Validation Loss = 3501.765476285579
Iteration 2472/10000: Training Loss = 3501.5128327626235
Validation Loss = 3501.5128327626235
Iteration 2473/10000: Training Loss = 3501.26262160785
Validation Loss = 3501.26262160785
Iteration 2474/10000: Training Loss = 3501.013551702342
Validation Loss = 3501.013551702342
Iteration 2475/10000: Training Loss = 3500.7576854403646
Validation Loss = 3500.7576854403646
Iteration 2476/10000: Training Loss = 3500.504450471867
Validation Loss = 3500.504450471867
Iteration 2477/10000: Training Loss = 3500.243376494503
Validation Loss = 3500.243376494503
Iteration 2478/10000: Training Loss = 3499.999288520305
Validation Loss = 3499.999288520305
Iteration 2479/10000: Training Loss = 3499.753039963385
Validation Loss = 3499.753039963385
Iteration 2480/10000: Training Loss = 3499.502062839783
Validation Loss = 3499.502062839783
Iteration 2481/10000: Training Loss = 3499.252922135112
Validation Loss = 3499.252922135112
Iteration 2482/10000: Training Loss = 3499.002117918915
Validation Loss = 3499.002117918915
Iteration 2483/10000: Training Loss = 3498.7574678937613
Validation Loss = 3498.7574678937613
Iteration 2484/10000: Training Loss = 3498.5073885294364
Validation Loss = 3498.5073885294364
Iteration 2485/10000: Training Loss = 3498.262419105213
Validation Loss = 3498.262419105213
Iteration 2486/10000: Training Loss = 3498.007087239815
Validation Loss = 3498.007087239815
Iteration 2487/10000: Training Loss = 3497.769201770296
Validation Loss = 3497.769201770296
Iteration 2488/10000: Training Loss = 3497.5245816416373
Validation Loss = 3497.5245816416373
Iteration 2489/10000: Training Loss = 3497.282289549049
Validation Loss = 3497.282289549049
Iteration 2490/10000: Training Loss = 3497.0129737156735
Validation Loss = 3497.0129737156735
Iteration 2491/10000: Training Loss = 3496.7609432507356
Validation Loss = 3496.7609432507356
Iteration 2492/10000: Training Loss = 3496.497051363866
Validation Loss = 3496.497051363866
Iteration 2493/10000: Training Loss = 3496.2424968127993
Validation Loss = 3496.2424968127993
Iteration 2494/10000: Training Loss = 3495.996986677532
Validation Loss = 3495.996986677532
Iteration 2495/10000: Training Loss = 3495.7504566832263
Validation Loss = 3495.7504566832263
Iteration 2496/10000: Training Loss = 3495.4952726789884
Validation Loss = 3495.4952726789884
Iteration 2497/10000: Training Loss = 3495.240871863087
Validation Loss = 3495.240871863087
Iteration 2498/10000: Training Loss = 3494.993859586318
Validation Loss = 3494.993859586318
Iteration 2499/10000: Training Loss = 3494.7416687825616
Validation Loss = 3494.7416687825616
Iteration 2500/10000: Training Loss = 3494.491047577245
Validation Loss = 3494.491047577245
Iteration 2501/10000: Training Loss = 3494.2479017190317
Validation Loss = 3494.2479017190317
Iteration 2502/10000: Training Loss = 3494.00251345192
Validation Loss = 3494.00251345192
Iteration 2503/10000: Training Loss = 3493.754362295568
Validation Loss = 3493.754362295568
Iteration 2504/10000: Training Loss = 3493.5056163650584
Validation Loss = 3493.5056163650584
Iteration 2505/10000: Training Loss = 3493.258350133298
Validation Loss = 3493.258350133298
Iteration 2506/10000: Training Loss = 3493.0094907056196
Validation Loss = 3493.0094907056196
Iteration 2507/10000: Training Loss = 3492.759832593259
Validation Loss = 3492.759832593259
Iteration 2508/10000: Training Loss = 3492.5149184263514
Validation Loss = 3492.5149184263514
Iteration 2509/10000: Training Loss = 3492.270437010603
Validation Loss = 3492.270437010603
Iteration 2510/10000: Training Loss = 3492.027744834452
Validation Loss = 3492.027744834452
Iteration 2511/10000: Training Loss = 3491.7824340311977
Validation Loss = 3491.7824340311977
Iteration 2512/10000: Training Loss = 3491.5303319845398
Validation Loss = 3491.5303319845398
Iteration 2513/10000: Training Loss = 3491.283421952642
Validation Loss = 3491.283421952642
Iteration 2514/10000: Training Loss = 3491.0410211543644
Validation Loss = 3491.0410211543644
Iteration 2515/10000: Training Loss = 3490.7919197470324
Validation Loss = 3490.7919197470324
Iteration 2516/10000: Training Loss = 3490.546770298478
Validation Loss = 3490.546770298478
Iteration 2517/10000: Training Loss = 3490.305594464981
Validation Loss = 3490.305594464981
Iteration 2518/10000: Training Loss = 3490.0612040259516
Validation Loss = 3490.0612040259516
Iteration 2519/10000: Training Loss = 3489.818482976782
Validation Loss = 3489.818482976782
Iteration 2520/10000: Training Loss = 3489.5785226831144
Validation Loss = 3489.5785226831144
Iteration 2521/10000: Training Loss = 3489.33356390087
Validation Loss = 3489.33356390087
Iteration 2522/10000: Training Loss = 3489.090982142016
Validation Loss = 3489.090982142016
Iteration 2523/10000: Training Loss = 3488.8373296989544
Validation Loss = 3488.8373296989544
Iteration 2524/10000: Training Loss = 3488.586796900614
Validation Loss = 3488.586796900614
Iteration 2525/10000: Training Loss = 3488.3418106709546
Validation Loss = 3488.3418106709546
Iteration 2526/10000: Training Loss = 3488.0968189188934
Validation Loss = 3488.0968189188934
Iteration 2527/10000: Training Loss = 3487.859684612895
Validation Loss = 3487.859684612895
Iteration 2528/10000: Training Loss = 3487.6174193939905
Validation Loss = 3487.6174193939905
Iteration 2529/10000: Training Loss = 3487.3741645510995
Validation Loss = 3487.3741645510995
Iteration 2530/10000: Training Loss = 3487.1311504100445
Validation Loss = 3487.1311504100445
Iteration 2531/10000: Training Loss = 3486.8855668354295
Validation Loss = 3486.8855668354295
Iteration 2532/10000: Training Loss = 3486.6435131396524
Validation Loss = 3486.6435131396524
Iteration 2533/10000: Training Loss = 3486.4001721438312
Validation Loss = 3486.4001721438312
Iteration 2534/10000: Training Loss = 3486.1601506845855
Validation Loss = 3486.1601506845855
Iteration 2535/10000: Training Loss = 3485.917739237567
Validation Loss = 3485.917739237567
Iteration 2536/10000: Training Loss = 3485.6781179227582
Validation Loss = 3485.6781179227582
Iteration 2537/10000: Training Loss = 3485.435644168533
Validation Loss = 3485.435644168533
Iteration 2538/10000: Training Loss = 3485.185175217981
Validation Loss = 3485.185175217981
Iteration 2539/10000: Training Loss = 3484.9441524813024
Validation Loss = 3484.9441524813024
Iteration 2540/10000: Training Loss = 3484.7006216440245
Validation Loss = 3484.7006216440245
Iteration 2541/10000: Training Loss = 3484.4615247584493
Validation Loss = 3484.4615247584493
Iteration 2542/10000: Training Loss = 3484.223933037199
Validation Loss = 3484.223933037199
Iteration 2543/10000: Training Loss = 3483.985101424649
Validation Loss = 3483.985101424649
Iteration 2544/10000: Training Loss = 3483.7430776922633
Validation Loss = 3483.7430776922633
Iteration 2545/10000: Training Loss = 3483.498204586253
Validation Loss = 3483.498204586253
Iteration 2546/10000: Training Loss = 3483.251132164717
Validation Loss = 3483.251132164717
Iteration 2547/10000: Training Loss = 3483.0138528051298
Validation Loss = 3483.0138528051298
Iteration 2548/10000: Training Loss = 3482.771455845141
Validation Loss = 3482.771455845141
Iteration 2549/10000: Training Loss = 3482.525385064635
Validation Loss = 3482.525385064635
Iteration 2550/10000: Training Loss = 3482.2818513697025
Validation Loss = 3482.2818513697025
Iteration 2551/10000: Training Loss = 3482.0430768088486
Validation Loss = 3482.0430768088486
Iteration 2552/10000: Training Loss = 3481.7999044030275
Validation Loss = 3481.7999044030275
Iteration 2553/10000: Training Loss = 3481.562585430118
Validation Loss = 3481.562585430118
Iteration 2554/10000: Training Loss = 3481.3242322922774
Validation Loss = 3481.3242322922774
Iteration 2555/10000: Training Loss = 3481.0808794120794
Validation Loss = 3481.0808794120794
Iteration 2556/10000: Training Loss = 3480.837081244333
Validation Loss = 3480.837081244333
Iteration 2557/10000: Training Loss = 3480.5923928598004
Validation Loss = 3480.5923928598004
Iteration 2558/10000: Training Loss = 3480.3570449460212
Validation Loss = 3480.3570449460212
Iteration 2559/10000: Training Loss = 3480.1201963762537
Validation Loss = 3480.1201963762537
Iteration 2560/10000: Training Loss = 3479.8824687395727
Validation Loss = 3479.8824687395727
Iteration 2561/10000: Training Loss = 3479.647789837931
Validation Loss = 3479.647789837931
Iteration 2562/10000: Training Loss = 3479.404244041516
Validation Loss = 3479.404244041516
Iteration 2563/10000: Training Loss = 3479.1687972440254
Validation Loss = 3479.1687972440254
Iteration 2564/10000: Training Loss = 3478.9313087139644
Validation Loss = 3478.9313087139644
Iteration 2565/10000: Training Loss = 3478.6870274702937
Validation Loss = 3478.6870274702937
Iteration 2566/10000: Training Loss = 3478.448723731288
Validation Loss = 3478.448723731288
Iteration 2567/10000: Training Loss = 3478.210329221175
Validation Loss = 3478.210329221175
Iteration 2568/10000: Training Loss = 3477.9738116809904
Validation Loss = 3477.9738116809904
Iteration 2569/10000: Training Loss = 3477.733843001999
Validation Loss = 3477.733843001999
Iteration 2570/10000: Training Loss = 3477.5005961490074
Validation Loss = 3477.5005961490074
Iteration 2571/10000: Training Loss = 3477.263247039756
Validation Loss = 3477.263247039756
Iteration 2572/10000: Training Loss = 3477.0308461697796
Validation Loss = 3477.0308461697796
Iteration 2573/10000: Training Loss = 3476.7984446420155
Validation Loss = 3476.7984446420155
Iteration 2574/10000: Training Loss = 3476.5625635875977
Validation Loss = 3476.5625635875977
Iteration 2575/10000: Training Loss = 3476.325127084576
Validation Loss = 3476.325127084576
Iteration 2576/10000: Training Loss = 3476.0945813362387
Validation Loss = 3476.0945813362387
Iteration 2577/10000: Training Loss = 3475.8604185464596
Validation Loss = 3475.8604185464596
Iteration 2578/10000: Training Loss = 3475.6271291734724
Validation Loss = 3475.6271291734724
Iteration 2579/10000: Training Loss = 3475.3929573225096
Validation Loss = 3475.3929573225096
Iteration 2580/10000: Training Loss = 3475.1578708123957
Validation Loss = 3475.1578708123957
Iteration 2581/10000: Training Loss = 3474.9229142582103
Validation Loss = 3474.9229142582103
Iteration 2582/10000: Training Loss = 3474.686649816437
Validation Loss = 3474.686649816437
Iteration 2583/10000: Training Loss = 3474.4551186719573
Validation Loss = 3474.4551186719573
Iteration 2584/10000: Training Loss = 3474.2206125037133
Validation Loss = 3474.2206125037133
Iteration 2585/10000: Training Loss = 3473.9887582587353
Validation Loss = 3473.9887582587353
Iteration 2586/10000: Training Loss = 3473.7467798977955
Validation Loss = 3473.7467798977955
Iteration 2587/10000: Training Loss = 3473.520806543109
Validation Loss = 3473.520806543109
Iteration 2588/10000: Training Loss = 3473.2721785322633
Validation Loss = 3473.2721785322633
Iteration 2589/10000: Training Loss = 3473.0361318215905
Validation Loss = 3473.0361318215905
Iteration 2590/10000: Training Loss = 3472.8038055926177
Validation Loss = 3472.8038055926177
Iteration 2591/10000: Training Loss = 3472.5715700159512
Validation Loss = 3472.5715700159512
Iteration 2592/10000: Training Loss = 3472.336000645141
Validation Loss = 3472.336000645141
Iteration 2593/10000: Training Loss = 3472.103232305756
Validation Loss = 3472.103232305756
Iteration 2594/10000: Training Loss = 3471.872162597662
Validation Loss = 3471.872162597662
Iteration 2595/10000: Training Loss = 3471.6410949824976
Validation Loss = 3471.6410949824976
Iteration 2596/10000: Training Loss = 3471.4047581367377
Validation Loss = 3471.4047581367377
Iteration 2597/10000: Training Loss = 3471.1700918093097
Validation Loss = 3471.1700918093097
Iteration 2598/10000: Training Loss = 3470.9352711380857
Validation Loss = 3470.9352711380857
Iteration 2599/10000: Training Loss = 3470.70191836724
Validation Loss = 3470.70191836724
Iteration 2600/10000: Training Loss = 3470.47685432022
Validation Loss = 3470.47685432022
Iteration 2601/10000: Training Loss = 3470.251434702176
Validation Loss = 3470.251434702176
Iteration 2602/10000: Training Loss = 3470.0134961857734
Validation Loss = 3470.0134961857734
Iteration 2603/10000: Training Loss = 3469.783117650331
Validation Loss = 3469.783117650331
Iteration 2604/10000: Training Loss = 3469.5489918634944
Validation Loss = 3469.5489918634944
Iteration 2605/10000: Training Loss = 3469.3169948651203
Validation Loss = 3469.3169948651203
Iteration 2606/10000: Training Loss = 3469.084068494074
Validation Loss = 3469.084068494074
Iteration 2607/10000: Training Loss = 3468.853144573011
Validation Loss = 3468.853144573011
Iteration 2608/10000: Training Loss = 3468.620293746485
Validation Loss = 3468.620293746485
Iteration 2609/10000: Training Loss = 3468.393392243101
Validation Loss = 3468.393392243101
Iteration 2610/10000: Training Loss = 3468.164587666489
Validation Loss = 3468.164587666489
Iteration 2611/10000: Training Loss = 3467.930704561216
Validation Loss = 3467.930704561216
Iteration 2612/10000: Training Loss = 3467.6988140162134
Validation Loss = 3467.6988140162134
Iteration 2613/10000: Training Loss = 3467.46856733341
Validation Loss = 3467.46856733341
Iteration 2614/10000: Training Loss = 3467.2413216493896
Validation Loss = 3467.2413216493896
Iteration 2615/10000: Training Loss = 3467.006924184745
Validation Loss = 3467.006924184745
Iteration 2616/10000: Training Loss = 3466.775855560445
Validation Loss = 3466.775855560445
Iteration 2617/10000: Training Loss = 3466.545800745149
Validation Loss = 3466.545800745149
Iteration 2618/10000: Training Loss = 3466.3149460228774
Validation Loss = 3466.3149460228774
Iteration 2619/10000: Training Loss = 3466.088087176288
Validation Loss = 3466.088087176288
Iteration 2620/10000: Training Loss = 3465.853836180341
Validation Loss = 3465.853836180341
Iteration 2621/10000: Training Loss = 3465.619342530004
Validation Loss = 3465.619342530004
Iteration 2622/10000: Training Loss = 3465.387994290203
Validation Loss = 3465.387994290203
Iteration 2623/10000: Training Loss = 3465.1578863866516
Validation Loss = 3465.1578863866516
Iteration 2624/10000: Training Loss = 3464.9321789898568
Validation Loss = 3464.9321789898568
Iteration 2625/10000: Training Loss = 3464.702594197507
Validation Loss = 3464.702594197507
Iteration 2626/10000: Training Loss = 3464.472163945862
Validation Loss = 3464.472163945862
Iteration 2627/10000: Training Loss = 3464.23788483251
Validation Loss = 3464.23788483251
Iteration 2628/10000: Training Loss = 3464.0117732506774
Validation Loss = 3464.0117732506774
Iteration 2629/10000: Training Loss = 3463.7865569904375
Validation Loss = 3463.7865569904375
Iteration 2630/10000: Training Loss = 3463.5590220509703
Validation Loss = 3463.5590220509703
Iteration 2631/10000: Training Loss = 3463.3345829510413
Validation Loss = 3463.3345829510413
Iteration 2632/10000: Training Loss = 3463.1040918711856
Validation Loss = 3463.1040918711856
Iteration 2633/10000: Training Loss = 3462.872766096015
Validation Loss = 3462.872766096015
Iteration 2634/10000: Training Loss = 3462.6468446000113
Validation Loss = 3462.6468446000113
Iteration 2635/10000: Training Loss = 3462.418504367216
Validation Loss = 3462.418504367216
Iteration 2636/10000: Training Loss = 3462.192762655539
Validation Loss = 3462.192762655539
Iteration 2637/10000: Training Loss = 3461.9686607725894
Validation Loss = 3461.9686607725894
Iteration 2638/10000: Training Loss = 3461.747266873613
Validation Loss = 3461.747266873613
Iteration 2639/10000: Training Loss = 3461.513562296132
Validation Loss = 3461.513562296132
Iteration 2640/10000: Training Loss = 3461.280746809898
Validation Loss = 3461.280746809898
Iteration 2641/10000: Training Loss = 3461.0572086676843
Validation Loss = 3461.0572086676843
Iteration 2642/10000: Training Loss = 3460.8297113132694
Validation Loss = 3460.8297113132694
Iteration 2643/10000: Training Loss = 3460.6027859628707
Validation Loss = 3460.6027859628707
Iteration 2644/10000: Training Loss = 3460.376413182602
Validation Loss = 3460.376413182602
Iteration 2645/10000: Training Loss = 3460.153171342856
Validation Loss = 3460.153171342856
Iteration 2646/10000: Training Loss = 3459.9309262899937
Validation Loss = 3459.9309262899937
Iteration 2647/10000: Training Loss = 3459.69919998067
Validation Loss = 3459.69919998067
Iteration 2648/10000: Training Loss = 3459.4740904749087
Validation Loss = 3459.4740904749087
Iteration 2649/10000: Training Loss = 3459.24801122662
Validation Loss = 3459.24801122662
Iteration 2650/10000: Training Loss = 3459.0205838780653
Validation Loss = 3459.0205838780653
Iteration 2651/10000: Training Loss = 3458.798763345638
Validation Loss = 3458.798763345638
Iteration 2652/10000: Training Loss = 3458.5735183257702
Validation Loss = 3458.5735183257702
Iteration 2653/10000: Training Loss = 3458.3511959517964
Validation Loss = 3458.3511959517964
Iteration 2654/10000: Training Loss = 3458.1232067551477
Validation Loss = 3458.1232067551477
Iteration 2655/10000: Training Loss = 3457.899536656179
Validation Loss = 3457.899536656179
Iteration 2656/10000: Training Loss = 3457.681088374689
Validation Loss = 3457.681088374689
Iteration 2657/10000: Training Loss = 3457.453925290379
Validation Loss = 3457.453925290379
Iteration 2658/10000: Training Loss = 3457.2338753883078
Validation Loss = 3457.2338753883078
Iteration 2659/10000: Training Loss = 3457.006237395354
Validation Loss = 3457.006237395354
Iteration 2660/10000: Training Loss = 3456.7799733998854
Validation Loss = 3456.7799733998854
Iteration 2661/10000: Training Loss = 3456.555085060309
Validation Loss = 3456.555085060309
Iteration 2662/10000: Training Loss = 3456.335655068144
Validation Loss = 3456.335655068144
Iteration 2663/10000: Training Loss = 3456.1092958483305
Validation Loss = 3456.1092958483305
Iteration 2664/10000: Training Loss = 3455.8911142037323
Validation Loss = 3455.8911142037323
Iteration 2665/10000: Training Loss = 3455.670752434794
Validation Loss = 3455.670752434794
Iteration 2666/10000: Training Loss = 3455.4490446887003
Validation Loss = 3455.4490446887003
Iteration 2667/10000: Training Loss = 3455.22459053512
Validation Loss = 3455.22459053512
Iteration 2668/10000: Training Loss = 3454.9952551784386
Validation Loss = 3454.9952551784386
Iteration 2669/10000: Training Loss = 3454.7708421806847
Validation Loss = 3454.7708421806847
Iteration 2670/10000: Training Loss = 3454.5514266899763
Validation Loss = 3454.5514266899763
Iteration 2671/10000: Training Loss = 3454.3302034730964
Validation Loss = 3454.3302034730964
Iteration 2672/10000: Training Loss = 3454.1090386231294
Validation Loss = 3454.1090386231294
Iteration 2673/10000: Training Loss = 3453.8834078893433
Validation Loss = 3453.8834078893433
Iteration 2674/10000: Training Loss = 3453.6611959703114
Validation Loss = 3453.6611959703114
Iteration 2675/10000: Training Loss = 3453.4414346263866
Validation Loss = 3453.4414346263866
Iteration 2676/10000: Training Loss = 3453.2196904549337
Validation Loss = 3453.2196904549337
Iteration 2677/10000: Training Loss = 3452.9943698162488
Validation Loss = 3452.9943698162488
Iteration 2678/10000: Training Loss = 3452.7747927462196
Validation Loss = 3452.7747927462196
Iteration 2679/10000: Training Loss = 3452.5529816232192
Validation Loss = 3452.5529816232192
Iteration 2680/10000: Training Loss = 3452.3297081715114
Validation Loss = 3452.3297081715114
Iteration 2681/10000: Training Loss = 3452.110989332053
Validation Loss = 3452.110989332053
Iteration 2682/10000: Training Loss = 3451.8916959732833
Validation Loss = 3451.8916959732833
Iteration 2683/10000: Training Loss = 3451.6776243180907
Validation Loss = 3451.6776243180907
Iteration 2684/10000: Training Loss = 3451.4551206906167
Validation Loss = 3451.4551206906167
Iteration 2685/10000: Training Loss = 3451.237155975398
Validation Loss = 3451.237155975398
Iteration 2686/10000: Training Loss = 3451.0104491484426
Validation Loss = 3451.0104491484426
Iteration 2687/10000: Training Loss = 3450.800529735891
Validation Loss = 3450.800529735891
Iteration 2688/10000: Training Loss = 3450.5901885762396
Validation Loss = 3450.5901885762396
Iteration 2689/10000: Training Loss = 3450.379025634031
Validation Loss = 3450.379025634031
Iteration 2690/10000: Training Loss = 3450.146482709469
Validation Loss = 3450.146482709469
Iteration 2691/10000: Training Loss = 3449.9511589714834
Validation Loss = 3449.9511589714834
Iteration 2692/10000: Training Loss = 3449.736380334628
Validation Loss = 3449.736380334628
Iteration 2693/10000: Training Loss = 3449.5103111162293
Validation Loss = 3449.5103111162293
Iteration 2694/10000: Training Loss = 3449.2992669633077
Validation Loss = 3449.2992669633077
Iteration 2695/10000: Training Loss = 3449.085492140557
Validation Loss = 3449.085492140557
Iteration 2696/10000: Training Loss = 3448.8448974393846
Validation Loss = 3448.8448974393846
Iteration 2697/10000: Training Loss = 3448.611798307763
Validation Loss = 3448.611798307763
Iteration 2698/10000: Training Loss = 3448.3891666439986
Validation Loss = 3448.3891666439986
Iteration 2699/10000: Training Loss = 3448.1824188140495
Validation Loss = 3448.1824188140495
Iteration 2700/10000: Training Loss = 3447.957611110657
Validation Loss = 3447.957611110657
Iteration 2701/10000: Training Loss = 3447.737938300323
Validation Loss = 3447.737938300323
Iteration 2702/10000: Training Loss = 3447.514953806979
Validation Loss = 3447.514953806979
Iteration 2703/10000: Training Loss = 3447.2957026002637
Validation Loss = 3447.2957026002637
Iteration 2704/10000: Training Loss = 3447.077579428508
Validation Loss = 3447.077579428508
Iteration 2705/10000: Training Loss = 3446.857470142099
Validation Loss = 3446.857470142099
Iteration 2706/10000: Training Loss = 3446.631716675165
Validation Loss = 3446.631716675165
Iteration 2707/10000: Training Loss = 3446.4098038281977
Validation Loss = 3446.4098038281977
Iteration 2708/10000: Training Loss = 3446.1946738893507
Validation Loss = 3446.1946738893507
Iteration 2709/10000: Training Loss = 3445.9753676681744
Validation Loss = 3445.9753676681744
Iteration 2710/10000: Training Loss = 3445.7594546626165
Validation Loss = 3445.7594546626165
Iteration 2711/10000: Training Loss = 3445.546442698315
Validation Loss = 3445.546442698315
Iteration 2712/10000: Training Loss = 3445.3304089946805
Validation Loss = 3445.3304089946805
Iteration 2713/10000: Training Loss = 3445.1107728439947
Validation Loss = 3445.1107728439947
Iteration 2714/10000: Training Loss = 3444.901775980352
Validation Loss = 3444.901775980352
Iteration 2715/10000: Training Loss = 3444.6825145659172
Validation Loss = 3444.6825145659172
Iteration 2716/10000: Training Loss = 3444.465313849152
Validation Loss = 3444.465313849152
Iteration 2717/10000: Training Loss = 3444.2531303224614
Validation Loss = 3444.2531303224614
Iteration 2718/10000: Training Loss = 3444.0343385078822
Validation Loss = 3444.0343385078822
Iteration 2719/10000: Training Loss = 3443.8192592461214
Validation Loss = 3443.8192592461214
Iteration 2720/10000: Training Loss = 3443.6000888543144
Validation Loss = 3443.6000888543144
Iteration 2721/10000: Training Loss = 3443.387442980154
Validation Loss = 3443.387442980154
Iteration 2722/10000: Training Loss = 3443.1681850893656
Validation Loss = 3443.1681850893656
Iteration 2723/10000: Training Loss = 3442.9550985053147
Validation Loss = 3442.9550985053147
Iteration 2724/10000: Training Loss = 3442.7411076482745
Validation Loss = 3442.7411076482745
Iteration 2725/10000: Training Loss = 3442.521721493894
Validation Loss = 3442.521721493894
Iteration 2726/10000: Training Loss = 3442.3084016799025
Validation Loss = 3442.3084016799025
Iteration 2727/10000: Training Loss = 3442.0985215130995
Validation Loss = 3442.0985215130995
Iteration 2728/10000: Training Loss = 3441.886630182004
Validation Loss = 3441.886630182004
Iteration 2729/10000: Training Loss = 3441.674017953492
Validation Loss = 3441.674017953492
Iteration 2730/10000: Training Loss = 3441.453095803858
Validation Loss = 3441.453095803858
Iteration 2731/10000: Training Loss = 3441.2457364801294
Validation Loss = 3441.2457364801294
Iteration 2732/10000: Training Loss = 3441.024589809094
Validation Loss = 3441.024589809094
Iteration 2733/10000: Training Loss = 3440.811421764355
Validation Loss = 3440.811421764355
Iteration 2734/10000: Training Loss = 3440.5988248147173
Validation Loss = 3440.5988248147173
Iteration 2735/10000: Training Loss = 3440.384854266953
Validation Loss = 3440.384854266953
Iteration 2736/10000: Training Loss = 3440.1733147193468
Validation Loss = 3440.1733147193468
Iteration 2737/10000: Training Loss = 3439.9617118205383
Validation Loss = 3439.9617118205383
Iteration 2738/10000: Training Loss = 3439.7480672453958
Validation Loss = 3439.7480672453958
Iteration 2739/10000: Training Loss = 3439.5421623931825
Validation Loss = 3439.5421623931825
Iteration 2740/10000: Training Loss = 3439.325664417157
Validation Loss = 3439.325664417157
Iteration 2741/10000: Training Loss = 3439.1165121481054
Validation Loss = 3439.1165121481054
Iteration 2742/10000: Training Loss = 3438.89785976501
Validation Loss = 3438.89785976501
Iteration 2743/10000: Training Loss = 3438.6797554169216
Validation Loss = 3438.6797554169216
Iteration 2744/10000: Training Loss = 3438.4733715678326
Validation Loss = 3438.4733715678326
Iteration 2745/10000: Training Loss = 3438.2610611794307
Validation Loss = 3438.2610611794307
Iteration 2746/10000: Training Loss = 3438.0438742234255
Validation Loss = 3438.0438742234255
Iteration 2747/10000: Training Loss = 3437.8307279745895
Validation Loss = 3437.8307279745895
Iteration 2748/10000: Training Loss = 3437.6212148951586
Validation Loss = 3437.6212148951586
Iteration 2749/10000: Training Loss = 3437.4101286508644
Validation Loss = 3437.4101286508644
Iteration 2750/10000: Training Loss = 3437.196020273007
Validation Loss = 3437.196020273007
Iteration 2751/10000: Training Loss = 3436.9805507799806
Validation Loss = 3436.9805507799806
Iteration 2752/10000: Training Loss = 3436.777508428824
Validation Loss = 3436.777508428824
Iteration 2753/10000: Training Loss = 3436.569834203018
Validation Loss = 3436.569834203018
Iteration 2754/10000: Training Loss = 3436.362294234336
Validation Loss = 3436.362294234336
Iteration 2755/10000: Training Loss = 3436.167540194561
Validation Loss = 3436.167540194561
Iteration 2756/10000: Training Loss = 3435.9451245410232
Validation Loss = 3435.9451245410232
Iteration 2757/10000: Training Loss = 3435.739175326057
Validation Loss = 3435.739175326057
Iteration 2758/10000: Training Loss = 3435.540287556927
Validation Loss = 3435.540287556927
Iteration 2759/10000: Training Loss = 3435.3258492876507
Validation Loss = 3435.3258492876507
Iteration 2760/10000: Training Loss = 3435.1182829900977
Validation Loss = 3435.1182829900977
Iteration 2761/10000: Training Loss = 3434.922153889914
Validation Loss = 3434.922153889914
Iteration 2762/10000: Training Loss = 3434.720298471254
Validation Loss = 3434.720298471254
Iteration 2763/10000: Training Loss = 3434.5148595308
Validation Loss = 3434.5148595308
Iteration 2764/10000: Training Loss = 3434.2940938708466
Validation Loss = 3434.2940938708466
Iteration 2765/10000: Training Loss = 3434.0816055259065
Validation Loss = 3434.0816055259065
Iteration 2766/10000: Training Loss = 3433.8626198821466
Validation Loss = 3433.8626198821466
Iteration 2767/10000: Training Loss = 3433.6573126539406
Validation Loss = 3433.6573126539406
Iteration 2768/10000: Training Loss = 3433.445653177145
Validation Loss = 3433.445653177145
Iteration 2769/10000: Training Loss = 3433.232335875412
Validation Loss = 3433.232335875412
Iteration 2770/10000: Training Loss = 3433.0173628374046
Validation Loss = 3433.0173628374046
Iteration 2771/10000: Training Loss = 3432.81158698629
Validation Loss = 3432.81158698629
Iteration 2772/10000: Training Loss = 3432.6029427748867
Validation Loss = 3432.6029427748867
Iteration 2773/10000: Training Loss = 3432.3953379366803
Validation Loss = 3432.3953379366803
Iteration 2774/10000: Training Loss = 3432.1872650968758
Validation Loss = 3432.1872650968758
Iteration 2775/10000: Training Loss = 3431.9769568644024
Validation Loss = 3431.9769568644024
Iteration 2776/10000: Training Loss = 3431.7703387643064
Validation Loss = 3431.7703387643064
Iteration 2777/10000: Training Loss = 3431.564642710881
Validation Loss = 3431.564642710881
Iteration 2778/10000: Training Loss = 3431.362021672397
Validation Loss = 3431.362021672397
Iteration 2779/10000: Training Loss = 3431.157607750323
Validation Loss = 3431.157607750323
Iteration 2780/10000: Training Loss = 3430.947506675883
Validation Loss = 3430.947506675883
Iteration 2781/10000: Training Loss = 3430.7444782090042
Validation Loss = 3430.7444782090042
Iteration 2782/10000: Training Loss = 3430.5364343045458
Validation Loss = 3430.5364343045458
Iteration 2783/10000: Training Loss = 3430.331905104345
Validation Loss = 3430.331905104345
Iteration 2784/10000: Training Loss = 3430.127858474483
Validation Loss = 3430.127858474483
Iteration 2785/10000: Training Loss = 3429.9184845355535
Validation Loss = 3429.9184845355535
Iteration 2786/10000: Training Loss = 3429.7071955159345
Validation Loss = 3429.7071955159345
Iteration 2787/10000: Training Loss = 3429.502192628318
Validation Loss = 3429.502192628318
Iteration 2788/10000: Training Loss = 3429.2994595998402
Validation Loss = 3429.2994595998402
Iteration 2789/10000: Training Loss = 3429.0924289725704
Validation Loss = 3429.0924289725704
Iteration 2790/10000: Training Loss = 3428.888875648328
Validation Loss = 3428.888875648328
Iteration 2791/10000: Training Loss = 3428.686654171659
Validation Loss = 3428.686654171659
Iteration 2792/10000: Training Loss = 3428.483378430811
Validation Loss = 3428.483378430811
Iteration 2793/10000: Training Loss = 3428.2695803082065
Validation Loss = 3428.2695803082065
Iteration 2794/10000: Training Loss = 3428.074736759141
Validation Loss = 3428.074736759141
Iteration 2795/10000: Training Loss = 3427.863915177854
Validation Loss = 3427.863915177854
Iteration 2796/10000: Training Loss = 3427.662635644524
Validation Loss = 3427.662635644524
Iteration 2797/10000: Training Loss = 3427.4613214299716
Validation Loss = 3427.4613214299716
Iteration 2798/10000: Training Loss = 3427.2558643140082
Validation Loss = 3427.2558643140082
Iteration 2799/10000: Training Loss = 3427.0540546769766
Validation Loss = 3427.0540546769766
Iteration 2800/10000: Training Loss = 3426.8531389475406
Validation Loss = 3426.8531389475406
Iteration 2801/10000: Training Loss = 3426.6444900456545
Validation Loss = 3426.6444900456545
Iteration 2802/10000: Training Loss = 3426.439555599845
Validation Loss = 3426.439555599845
Iteration 2803/10000: Training Loss = 3426.2348109646036
Validation Loss = 3426.2348109646036
Iteration 2804/10000: Training Loss = 3426.0281159641468
Validation Loss = 3426.0281159641468
Iteration 2805/10000: Training Loss = 3425.827559140027
Validation Loss = 3425.827559140027
Iteration 2806/10000: Training Loss = 3425.6242840999125
Validation Loss = 3425.6242840999125
Iteration 2807/10000: Training Loss = 3425.4235762644266
Validation Loss = 3425.4235762644266
Iteration 2808/10000: Training Loss = 3425.2217434459562
Validation Loss = 3425.2217434459562
Iteration 2809/10000: Training Loss = 3425.0202386724795
Validation Loss = 3425.0202386724795
Iteration 2810/10000: Training Loss = 3424.8164725795855
Validation Loss = 3424.8164725795855
Iteration 2811/10000: Training Loss = 3424.617666831815
Validation Loss = 3424.617666831815
Iteration 2812/10000: Training Loss = 3424.4059292499355
Validation Loss = 3424.4059292499355
Iteration 2813/10000: Training Loss = 3424.2037536566368
Validation Loss = 3424.2037536566368
Iteration 2814/10000: Training Loss = 3424.005185193021
Validation Loss = 3424.005185193021
Iteration 2815/10000: Training Loss = 3423.8021769465804
Validation Loss = 3423.8021769465804
Iteration 2816/10000: Training Loss = 3423.598970051446
Validation Loss = 3423.598970051446
Iteration 2817/10000: Training Loss = 3423.3923298996892
Validation Loss = 3423.3923298996892
Iteration 2818/10000: Training Loss = 3423.1999099985082
Validation Loss = 3423.1999099985082
Iteration 2819/10000: Training Loss = 3422.999592235922
Validation Loss = 3422.999592235922
Iteration 2820/10000: Training Loss = 3422.7957014175704
Validation Loss = 3422.7957014175704
Iteration 2821/10000: Training Loss = 3422.5937253615257
Validation Loss = 3422.5937253615257
Iteration 2822/10000: Training Loss = 3422.3819923354345
Validation Loss = 3422.3819923354345
Iteration 2823/10000: Training Loss = 3422.1845070108966
Validation Loss = 3422.1845070108966
Iteration 2824/10000: Training Loss = 3421.98456266513
Validation Loss = 3421.98456266513
Iteration 2825/10000: Training Loss = 3421.791412074161
Validation Loss = 3421.791412074161
Iteration 2826/10000: Training Loss = 3421.587194726706
Validation Loss = 3421.587194726706
Iteration 2827/10000: Training Loss = 3421.372932615552
Validation Loss = 3421.372932615552
Iteration 2828/10000: Training Loss = 3421.1824696367994
Validation Loss = 3421.1824696367994
Iteration 2829/10000: Training Loss = 3420.980205567979
Validation Loss = 3420.980205567979
Iteration 2830/10000: Training Loss = 3420.781290807972
Validation Loss = 3420.781290807972
Iteration 2831/10000: Training Loss = 3420.5793211453656
Validation Loss = 3420.5793211453656
Iteration 2832/10000: Training Loss = 3420.3813778696176
Validation Loss = 3420.3813778696176
Iteration 2833/10000: Training Loss = 3420.1898557340446
Validation Loss = 3420.1898557340446
Iteration 2834/10000: Training Loss = 3419.9794535122765
Validation Loss = 3419.9794535122765
Iteration 2835/10000: Training Loss = 3419.767292126939
Validation Loss = 3419.767292126939
Iteration 2836/10000: Training Loss = 3419.5722992416454
Validation Loss = 3419.5722992416454
Iteration 2837/10000: Training Loss = 3419.375825288386
Validation Loss = 3419.375825288386
Iteration 2838/10000: Training Loss = 3419.172431170487
Validation Loss = 3419.172431170487
Iteration 2839/10000: Training Loss = 3418.972434246464
Validation Loss = 3418.972434246464
Iteration 2840/10000: Training Loss = 3418.7673756104305
Validation Loss = 3418.7673756104305
Iteration 2841/10000: Training Loss = 3418.5630236970946
Validation Loss = 3418.5630236970946
Iteration 2842/10000: Training Loss = 3418.356150378218
Validation Loss = 3418.356150378218
Iteration 2843/10000: Training Loss = 3418.147952164292
Validation Loss = 3418.147952164292
Iteration 2844/10000: Training Loss = 3417.9486743332927
Validation Loss = 3417.9486743332927
Iteration 2845/10000: Training Loss = 3417.757280223438
Validation Loss = 3417.757280223438
Iteration 2846/10000: Training Loss = 3417.558694422101
Validation Loss = 3417.558694422101
Iteration 2847/10000: Training Loss = 3417.358998427043
Validation Loss = 3417.358998427043
Iteration 2848/10000: Training Loss = 3417.153915121095
Validation Loss = 3417.153915121095
Iteration 2849/10000: Training Loss = 3416.9469766544626
Validation Loss = 3416.9469766544626
Iteration 2850/10000: Training Loss = 3416.746317770077
Validation Loss = 3416.746317770077
Iteration 2851/10000: Training Loss = 3416.5516265917645
Validation Loss = 3416.5516265917645
Iteration 2852/10000: Training Loss = 3416.352919184278
Validation Loss = 3416.352919184278
Iteration 2853/10000: Training Loss = 3416.1551754501734
Validation Loss = 3416.1551754501734
Iteration 2854/10000: Training Loss = 3415.9646519968396
Validation Loss = 3415.9646519968396
Iteration 2855/10000: Training Loss = 3415.7637754326133
Validation Loss = 3415.7637754326133
Iteration 2856/10000: Training Loss = 3415.568388938062
Validation Loss = 3415.568388938062
Iteration 2857/10000: Training Loss = 3415.371199015809
Validation Loss = 3415.371199015809
Iteration 2858/10000: Training Loss = 3415.1674775347083
Validation Loss = 3415.1674775347083
Iteration 2859/10000: Training Loss = 3414.973362062404
Validation Loss = 3414.973362062404
Iteration 2860/10000: Training Loss = 3414.775810559404
Validation Loss = 3414.775810559404
Iteration 2861/10000: Training Loss = 3414.5755395668502
Validation Loss = 3414.5755395668502
Iteration 2862/10000: Training Loss = 3414.373841747503
Validation Loss = 3414.373841747503
Iteration 2863/10000: Training Loss = 3414.1765143225325
Validation Loss = 3414.1765143225325
Iteration 2864/10000: Training Loss = 3413.9793105826657
Validation Loss = 3413.9793105826657
Iteration 2865/10000: Training Loss = 3413.7891410229395
Validation Loss = 3413.7891410229395
Iteration 2866/10000: Training Loss = 3413.590115704801
Validation Loss = 3413.590115704801
Iteration 2867/10000: Training Loss = 3413.390542823379
Validation Loss = 3413.390542823379
Iteration 2868/10000: Training Loss = 3413.1909925600175
Validation Loss = 3413.1909925600175
Iteration 2869/10000: Training Loss = 3412.993917417714
Validation Loss = 3412.993917417714
Iteration 2870/10000: Training Loss = 3412.8052885205216
Validation Loss = 3412.8052885205216
Iteration 2871/10000: Training Loss = 3412.612329315727
Validation Loss = 3412.612329315727
Iteration 2872/10000: Training Loss = 3412.4068215633783
Validation Loss = 3412.4068215633783
Iteration 2873/10000: Training Loss = 3412.217071414937
Validation Loss = 3412.217071414937
Iteration 2874/10000: Training Loss = 3412.0287394120724
Validation Loss = 3412.0287394120724
Iteration 2875/10000: Training Loss = 3411.8283931875694
Validation Loss = 3411.8283931875694
Iteration 2876/10000: Training Loss = 3411.6303876072548
Validation Loss = 3411.6303876072548
Iteration 2877/10000: Training Loss = 3411.436903602735
Validation Loss = 3411.436903602735
Iteration 2878/10000: Training Loss = 3411.2490674997075
Validation Loss = 3411.2490674997075
Iteration 2879/10000: Training Loss = 3411.0533092192018
Validation Loss = 3411.0533092192018
Iteration 2880/10000: Training Loss = 3410.8560268429646
Validation Loss = 3410.8560268429646
Iteration 2881/10000: Training Loss = 3410.6613790748247
Validation Loss = 3410.6613790748247
Iteration 2882/10000: Training Loss = 3410.4681598678894
Validation Loss = 3410.4681598678894
Iteration 2883/10000: Training Loss = 3410.2680698627446
Validation Loss = 3410.2680698627446
Iteration 2884/10000: Training Loss = 3410.0802988651894
Validation Loss = 3410.0802988651894
Iteration 2885/10000: Training Loss = 3409.8862426730034
Validation Loss = 3409.8862426730034
Iteration 2886/10000: Training Loss = 3409.692811381208
Validation Loss = 3409.692811381208
Iteration 2887/10000: Training Loss = 3409.5010489512406
Validation Loss = 3409.5010489512406
Iteration 2888/10000: Training Loss = 3409.3076237802675
Validation Loss = 3409.3076237802675
Iteration 2889/10000: Training Loss = 3409.1118852259565
Validation Loss = 3409.1118852259565
Iteration 2890/10000: Training Loss = 3408.9167547781744
Validation Loss = 3408.9167547781744
Iteration 2891/10000: Training Loss = 3408.7218990948554
Validation Loss = 3408.7218990948554
Iteration 2892/10000: Training Loss = 3408.5309980786037
Validation Loss = 3408.5309980786037
Iteration 2893/10000: Training Loss = 3408.3349221808567
Validation Loss = 3408.3349221808567
Iteration 2894/10000: Training Loss = 3408.14761959776
Validation Loss = 3408.14761959776
Iteration 2895/10000: Training Loss = 3407.955868263715
Validation Loss = 3407.955868263715
Iteration 2896/10000: Training Loss = 3407.7651525435695
Validation Loss = 3407.7651525435695
Iteration 2897/10000: Training Loss = 3407.5702297298753
Validation Loss = 3407.5702297298753
Iteration 2898/10000: Training Loss = 3407.3682900732183
Validation Loss = 3407.3682900732183
Iteration 2899/10000: Training Loss = 3407.1733019135095
Validation Loss = 3407.1733019135095
Iteration 2900/10000: Training Loss = 3406.9787264183224
Validation Loss = 3406.9787264183224
Iteration 2901/10000: Training Loss = 3406.7872860144776
Validation Loss = 3406.7872860144776
Iteration 2902/10000: Training Loss = 3406.600002017817
Validation Loss = 3406.600002017817
Iteration 2903/10000: Training Loss = 3406.40465600475
Validation Loss = 3406.40465600475
Iteration 2904/10000: Training Loss = 3406.214113173433
Validation Loss = 3406.214113173433
Iteration 2905/10000: Training Loss = 3406.0236634820253
Validation Loss = 3406.0236634820253
Iteration 2906/10000: Training Loss = 3405.8339999985724
Validation Loss = 3405.8339999985724
Iteration 2907/10000: Training Loss = 3405.6432344057357
Validation Loss = 3405.6432344057357
Iteration 2908/10000: Training Loss = 3405.454025917061
Validation Loss = 3405.454025917061
Iteration 2909/10000: Training Loss = 3405.2614168295463
Validation Loss = 3405.2614168295463
Iteration 2910/10000: Training Loss = 3405.071608424763
Validation Loss = 3405.071608424763
Iteration 2911/10000: Training Loss = 3404.895889590085
Validation Loss = 3404.895889590085
Iteration 2912/10000: Training Loss = 3404.6961655433656
Validation Loss = 3404.6961655433656
Iteration 2913/10000: Training Loss = 3404.506547066422
Validation Loss = 3404.506547066422
Iteration 2914/10000: Training Loss = 3404.319969690228
Validation Loss = 3404.319969690228
Iteration 2915/10000: Training Loss = 3404.1234828450974
Validation Loss = 3404.1234828450974
Iteration 2916/10000: Training Loss = 3403.9289747963144
Validation Loss = 3403.9289747963144
Iteration 2917/10000: Training Loss = 3403.738948277279
Validation Loss = 3403.738948277279
Iteration 2918/10000: Training Loss = 3403.5507295122493
Validation Loss = 3403.5507295122493
Iteration 2919/10000: Training Loss = 3403.363110108063
Validation Loss = 3403.363110108063
Iteration 2920/10000: Training Loss = 3403.1684695100707
Validation Loss = 3403.1684695100707
Iteration 2921/10000: Training Loss = 3402.976873168831
Validation Loss = 3402.976873168831
Iteration 2922/10000: Training Loss = 3402.78860864714
Validation Loss = 3402.78860864714
Iteration 2923/10000: Training Loss = 3402.5947731982305
Validation Loss = 3402.5947731982305
Iteration 2924/10000: Training Loss = 3402.4103738241565
Validation Loss = 3402.4103738241565
Iteration 2925/10000: Training Loss = 3402.2225483455168
Validation Loss = 3402.2225483455168
Iteration 2926/10000: Training Loss = 3402.030121053305
Validation Loss = 3402.030121053305
Iteration 2927/10000: Training Loss = 3401.847915429144
Validation Loss = 3401.847915429144
Iteration 2928/10000: Training Loss = 3401.6539758560457
Validation Loss = 3401.6539758560457
Iteration 2929/10000: Training Loss = 3401.4694056938465
Validation Loss = 3401.4694056938465
Iteration 2930/10000: Training Loss = 3401.2849178992137
Validation Loss = 3401.2849178992137
Iteration 2931/10000: Training Loss = 3401.1038048865876
Validation Loss = 3401.1038048865876
Iteration 2932/10000: Training Loss = 3400.922873454661
Validation Loss = 3400.922873454661
Iteration 2933/10000: Training Loss = 3400.716024483332
Validation Loss = 3400.716024483332
Iteration 2934/10000: Training Loss = 3400.532645327088
Validation Loss = 3400.532645327088
Iteration 2935/10000: Training Loss = 3400.339361516233
Validation Loss = 3400.339361516233
Iteration 2936/10000: Training Loss = 3400.1526288264413
Validation Loss = 3400.1526288264413
Iteration 2937/10000: Training Loss = 3399.9623023680265
Validation Loss = 3399.9623023680265
Iteration 2938/10000: Training Loss = 3399.779274005307
Validation Loss = 3399.779274005307
Iteration 2939/10000: Training Loss = 3399.592339633458
Validation Loss = 3399.592339633458
Iteration 2940/10000: Training Loss = 3399.3977106941747
Validation Loss = 3399.3977106941747
Iteration 2941/10000: Training Loss = 3399.225574441323
Validation Loss = 3399.225574441323
Iteration 2942/10000: Training Loss = 3399.0258869742006
Validation Loss = 3399.0258869742006
Iteration 2943/10000: Training Loss = 3398.8462822156885
Validation Loss = 3398.8462822156885
Iteration 2944/10000: Training Loss = 3398.6507098382817
Validation Loss = 3398.6507098382817
Iteration 2945/10000: Training Loss = 3398.4621641795716
Validation Loss = 3398.4621641795716
Iteration 2946/10000: Training Loss = 3398.2700157523573
Validation Loss = 3398.2700157523573
Iteration 2947/10000: Training Loss = 3398.0816571830583
Validation Loss = 3398.0816571830583
Iteration 2948/10000: Training Loss = 3397.8892689796166
Validation Loss = 3397.8892689796166
Iteration 2949/10000: Training Loss = 3397.7062592007096
Validation Loss = 3397.7062592007096
Iteration 2950/10000: Training Loss = 3397.5195884413515
Validation Loss = 3397.5195884413515
Iteration 2951/10000: Training Loss = 3397.3293267278823
Validation Loss = 3397.3293267278823
Iteration 2952/10000: Training Loss = 3397.138755159057
Validation Loss = 3397.138755159057
Iteration 2953/10000: Training Loss = 3396.9529698194224
Validation Loss = 3396.9529698194224
Iteration 2954/10000: Training Loss = 3396.759898096792
Validation Loss = 3396.759898096792
Iteration 2955/10000: Training Loss = 3396.5732013361626
Validation Loss = 3396.5732013361626
Iteration 2956/10000: Training Loss = 3396.3897597544974
Validation Loss = 3396.3897597544974
Iteration 2957/10000: Training Loss = 3396.2022324293075
Validation Loss = 3396.2022324293075
Iteration 2958/10000: Training Loss = 3396.0142812464123
Validation Loss = 3396.0142812464123
Iteration 2959/10000: Training Loss = 3395.832974176077
Validation Loss = 3395.832974176077
Iteration 2960/10000: Training Loss = 3395.6481010628336
Validation Loss = 3395.6481010628336
Iteration 2961/10000: Training Loss = 3395.462350810643
Validation Loss = 3395.462350810643
Iteration 2962/10000: Training Loss = 3395.275492012921
Validation Loss = 3395.275492012921
Iteration 2963/10000: Training Loss = 3395.092726954959
Validation Loss = 3395.092726954959
Iteration 2964/10000: Training Loss = 3394.9008940990375
Validation Loss = 3394.9008940990375
Iteration 2965/10000: Training Loss = 3394.7131130526227
Validation Loss = 3394.7131130526227
Iteration 2966/10000: Training Loss = 3394.5386315308983
Validation Loss = 3394.5386315308983
Iteration 2967/10000: Training Loss = 3394.3570612278945
Validation Loss = 3394.3570612278945
Iteration 2968/10000: Training Loss = 3394.1717228328926
Validation Loss = 3394.1717228328926
Iteration 2969/10000: Training Loss = 3393.979905055547
Validation Loss = 3393.979905055547
Iteration 2970/10000: Training Loss = 3393.8058458088917
Validation Loss = 3393.8058458088917
Iteration 2971/10000: Training Loss = 3393.6151533496472
Validation Loss = 3393.6151533496472
Iteration 2972/10000: Training Loss = 3393.4356532508054
Validation Loss = 3393.4356532508054
Iteration 2973/10000: Training Loss = 3393.24219276623
Validation Loss = 3393.24219276623
Iteration 2974/10000: Training Loss = 3393.0487568106973
Validation Loss = 3393.0487568106973
Iteration 2975/10000: Training Loss = 3392.867154383711
Validation Loss = 3392.867154383711
Iteration 2976/10000: Training Loss = 3392.687914915557
Validation Loss = 3392.687914915557
Iteration 2977/10000: Training Loss = 3392.5123093004927
Validation Loss = 3392.5123093004927
Iteration 2978/10000: Training Loss = 3392.3244851687114
Validation Loss = 3392.3244851687114
Iteration 2979/10000: Training Loss = 3392.1589193323985
Validation Loss = 3392.1589193323985
Iteration 2980/10000: Training Loss = 3391.9564796068594
Validation Loss = 3391.9564796068594
Iteration 2981/10000: Training Loss = 3391.771125426805
Validation Loss = 3391.771125426805
Iteration 2982/10000: Training Loss = 3391.5804995430804
Validation Loss = 3391.5804995430804
Iteration 2983/10000: Training Loss = 3391.390783530927
Validation Loss = 3391.390783530927
Iteration 2984/10000: Training Loss = 3391.2125214399034
Validation Loss = 3391.2125214399034
Iteration 2985/10000: Training Loss = 3391.0310216954003
Validation Loss = 3391.0310216954003
Iteration 2986/10000: Training Loss = 3390.843163185502
Validation Loss = 3390.843163185502
Iteration 2987/10000: Training Loss = 3390.6623682876834
Validation Loss = 3390.6623682876834
Iteration 2988/10000: Training Loss = 3390.47652491217
Validation Loss = 3390.47652491217
Iteration 2989/10000: Training Loss = 3390.3020302029713
Validation Loss = 3390.3020302029713
Iteration 2990/10000: Training Loss = 3390.120003017098
Validation Loss = 3390.120003017098
Iteration 2991/10000: Training Loss = 3389.9401010466554
Validation Loss = 3389.9401010466554
Iteration 2992/10000: Training Loss = 3389.760307204632
Validation Loss = 3389.760307204632
Iteration 2993/10000: Training Loss = 3389.5827032303378
Validation Loss = 3389.5827032303378
Iteration 2994/10000: Training Loss = 3389.400444415223
Validation Loss = 3389.400444415223
Iteration 2995/10000: Training Loss = 3389.2223886135616
Validation Loss = 3389.2223886135616
Iteration 2996/10000: Training Loss = 3389.0337079138367
Validation Loss = 3389.0337079138367
Iteration 2997/10000: Training Loss = 3388.8398974671104
Validation Loss = 3388.8398974671104
Iteration 2998/10000: Training Loss = 3388.6566788162645
Validation Loss = 3388.6566788162645
Iteration 2999/10000: Training Loss = 3388.4752584002863
Validation Loss = 3388.4752584002863
Iteration 3000/10000: Training Loss = 3388.292082358508
Validation Loss = 3388.292082358508
Iteration 3001/10000: Training Loss = 3388.112892189998
Validation Loss = 3388.112892189998
Iteration 3002/10000: Training Loss = 3387.93489031614
Validation Loss = 3387.93489031614
Iteration 3003/10000: Training Loss = 3387.7511498689846
Validation Loss = 3387.7511498689846
Iteration 3004/10000: Training Loss = 3387.57155349961
Validation Loss = 3387.57155349961
Iteration 3005/10000: Training Loss = 3387.393537518526
Validation Loss = 3387.393537518526
Iteration 3006/10000: Training Loss = 3387.2125341196884
Validation Loss = 3387.2125341196884
Iteration 3007/10000: Training Loss = 3387.029757735829
Validation Loss = 3387.029757735829
Iteration 3008/10000: Training Loss = 3386.8472718835255
Validation Loss = 3386.8472718835255
Iteration 3009/10000: Training Loss = 3386.6720831812463
Validation Loss = 3386.6720831812463
Iteration 3010/10000: Training Loss = 3386.489391238577
Validation Loss = 3386.489391238577
Iteration 3011/10000: Training Loss = 3386.3062057566276
Validation Loss = 3386.3062057566276
Iteration 3012/10000: Training Loss = 3386.1236388566585
Validation Loss = 3386.1236388566585
Iteration 3013/10000: Training Loss = 3385.9447360988383
Validation Loss = 3385.9447360988383
Iteration 3014/10000: Training Loss = 3385.7620431903833
Validation Loss = 3385.7620431903833
Iteration 3015/10000: Training Loss = 3385.5767893196476
Validation Loss = 3385.5767893196476
Iteration 3016/10000: Training Loss = 3385.3987565892626
Validation Loss = 3385.3987565892626
Iteration 3017/10000: Training Loss = 3385.2199823113306
Validation Loss = 3385.2199823113306
Iteration 3018/10000: Training Loss = 3385.044469443016
Validation Loss = 3385.044469443016
Iteration 3019/10000: Training Loss = 3384.8572542959005
Validation Loss = 3384.8572542959005
Iteration 3020/10000: Training Loss = 3384.677503216151
Validation Loss = 3384.677503216151
Iteration 3021/10000: Training Loss = 3384.4929873200867
Validation Loss = 3384.4929873200867
Iteration 3022/10000: Training Loss = 3384.3120069865345
Validation Loss = 3384.3120069865345
Iteration 3023/10000: Training Loss = 3384.129664757877
Validation Loss = 3384.129664757877
Iteration 3024/10000: Training Loss = 3383.9512781846993
Validation Loss = 3383.9512781846993
Iteration 3025/10000: Training Loss = 3383.7700429993124
Validation Loss = 3383.7700429993124
Iteration 3026/10000: Training Loss = 3383.5968902498375
Validation Loss = 3383.5968902498375
Iteration 3027/10000: Training Loss = 3383.4225674675117
Validation Loss = 3383.4225674675117
Iteration 3028/10000: Training Loss = 3383.244717371363
Validation Loss = 3383.244717371363
Iteration 3029/10000: Training Loss = 3383.0723104778203
Validation Loss = 3383.0723104778203
Iteration 3030/10000: Training Loss = 3382.883621284064
Validation Loss = 3382.883621284064
Iteration 3031/10000: Training Loss = 3382.709469702911
Validation Loss = 3382.709469702911
Iteration 3032/10000: Training Loss = 3382.5388489394313
Validation Loss = 3382.5388489394313
Iteration 3033/10000: Training Loss = 3382.36060245853
Validation Loss = 3382.36060245853
Iteration 3034/10000: Training Loss = 3382.1689263636263
Validation Loss = 3382.1689263636263
Iteration 3035/10000: Training Loss = 3381.9963460175836
Validation Loss = 3381.9963460175836
Iteration 3036/10000: Training Loss = 3381.81821067251
Validation Loss = 3381.81821067251
Iteration 3037/10000: Training Loss = 3381.6440519745975
Validation Loss = 3381.6440519745975
Iteration 3038/10000: Training Loss = 3381.4701300254765
Validation Loss = 3381.4701300254765
Iteration 3039/10000: Training Loss = 3381.3037681837272
Validation Loss = 3381.3037681837272
Iteration 3040/10000: Training Loss = 3381.1196165366196
Validation Loss = 3381.1196165366196
Iteration 3041/10000: Training Loss = 3380.9465476496207
Validation Loss = 3380.9465476496207
Iteration 3042/10000: Training Loss = 3380.7616776803766
Validation Loss = 3380.7616776803766
Iteration 3043/10000: Training Loss = 3380.5824033710737
Validation Loss = 3380.5824033710737
Iteration 3044/10000: Training Loss = 3380.400674038157
Validation Loss = 3380.400674038157
Iteration 3045/10000: Training Loss = 3380.2251706739457
Validation Loss = 3380.2251706739457
Iteration 3046/10000: Training Loss = 3380.0490447657266
Validation Loss = 3380.0490447657266
Iteration 3047/10000: Training Loss = 3379.8707478888123
Validation Loss = 3379.8707478888123
Iteration 3048/10000: Training Loss = 3379.689571353199
Validation Loss = 3379.689571353199
Iteration 3049/10000: Training Loss = 3379.516618478678
Validation Loss = 3379.516618478678
Iteration 3050/10000: Training Loss = 3379.329261510979
Validation Loss = 3379.329261510979
Iteration 3051/10000: Training Loss = 3379.154607862995
Validation Loss = 3379.154607862995
Iteration 3052/10000: Training Loss = 3378.9806677751576
Validation Loss = 3378.9806677751576
Iteration 3053/10000: Training Loss = 3378.804620824932
Validation Loss = 3378.804620824932
Iteration 3054/10000: Training Loss = 3378.624532013447
Validation Loss = 3378.624532013447
Iteration 3055/10000: Training Loss = 3378.448534604287
Validation Loss = 3378.448534604287
Iteration 3056/10000: Training Loss = 3378.264881300435
Validation Loss = 3378.264881300435
Iteration 3057/10000: Training Loss = 3378.091247279373
Validation Loss = 3378.091247279373
Iteration 3058/10000: Training Loss = 3377.9135929043246
Validation Loss = 3377.9135929043246
Iteration 3059/10000: Training Loss = 3377.733653749701
Validation Loss = 3377.733653749701
Iteration 3060/10000: Training Loss = 3377.557048701551
Validation Loss = 3377.557048701551
Iteration 3061/10000: Training Loss = 3377.376859113351
Validation Loss = 3377.376859113351
Iteration 3062/10000: Training Loss = 3377.209116840548
Validation Loss = 3377.209116840548
Iteration 3063/10000: Training Loss = 3377.030149432314
Validation Loss = 3377.030149432314
Iteration 3064/10000: Training Loss = 3376.8576819528926
Validation Loss = 3376.8576819528926
Iteration 3065/10000: Training Loss = 3376.683966482832
Validation Loss = 3376.683966482832
Iteration 3066/10000: Training Loss = 3376.510652323903
Validation Loss = 3376.510652323903
Iteration 3067/10000: Training Loss = 3376.3376827083875
Validation Loss = 3376.3376827083875
Iteration 3068/10000: Training Loss = 3376.161655974126
Validation Loss = 3376.161655974126
Iteration 3069/10000: Training Loss = 3375.9879963477947
Validation Loss = 3375.9879963477947
Iteration 3070/10000: Training Loss = 3375.8175384948245
Validation Loss = 3375.8175384948245
Iteration 3071/10000: Training Loss = 3375.640162629509
Validation Loss = 3375.640162629509
Iteration 3072/10000: Training Loss = 3375.465231617257
Validation Loss = 3375.465231617257
Iteration 3073/10000: Training Loss = 3375.290679100384
Validation Loss = 3375.290679100384
Iteration 3074/10000: Training Loss = 3375.1199693727685
Validation Loss = 3375.1199693727685
Iteration 3075/10000: Training Loss = 3374.947277635652
Validation Loss = 3374.947277635652
Iteration 3076/10000: Training Loss = 3374.7788940754976
Validation Loss = 3374.7788940754976
Iteration 3077/10000: Training Loss = 3374.607660510559
Validation Loss = 3374.607660510559
Iteration 3078/10000: Training Loss = 3374.4369698007745
Validation Loss = 3374.4369698007745
Iteration 3079/10000: Training Loss = 3374.2540366992616
Validation Loss = 3374.2540366992616
Iteration 3080/10000: Training Loss = 3374.0766186200426
Validation Loss = 3374.0766186200426
Iteration 3081/10000: Training Loss = 3373.903446507608
Validation Loss = 3373.903446507608
Iteration 3082/10000: Training Loss = 3373.731720349413
Validation Loss = 3373.731720349413
Iteration 3083/10000: Training Loss = 3373.5593637159736
Validation Loss = 3373.5593637159736
Iteration 3084/10000: Training Loss = 3373.3922574192297
Validation Loss = 3373.3922574192297
Iteration 3085/10000: Training Loss = 3373.2190766030494
Validation Loss = 3373.2190766030494
Iteration 3086/10000: Training Loss = 3373.0379840095557
Validation Loss = 3373.0379840095557
Iteration 3087/10000: Training Loss = 3372.8662285430432
Validation Loss = 3372.8662285430432
Iteration 3088/10000: Training Loss = 3372.6964303840477
Validation Loss = 3372.6964303840477
Iteration 3089/10000: Training Loss = 3372.5281574295386
Validation Loss = 3372.5281574295386
Iteration 3090/10000: Training Loss = 3372.3568520558038
Validation Loss = 3372.3568520558038
Iteration 3091/10000: Training Loss = 3372.1865829980457
Validation Loss = 3372.1865829980457
Iteration 3092/10000: Training Loss = 3372.008091402343
Validation Loss = 3372.008091402343
Iteration 3093/10000: Training Loss = 3371.828735814125
Validation Loss = 3371.828735814125
Iteration 3094/10000: Training Loss = 3371.6591972835117
Validation Loss = 3371.6591972835117
Iteration 3095/10000: Training Loss = 3371.4906206143246
Validation Loss = 3371.4906206143246
Iteration 3096/10000: Training Loss = 3371.3185286340217
Validation Loss = 3371.3185286340217
Iteration 3097/10000: Training Loss = 3371.145867665709
Validation Loss = 3371.145867665709
Iteration 3098/10000: Training Loss = 3370.9742081715412
Validation Loss = 3370.9742081715412
Iteration 3099/10000: Training Loss = 3370.803261101955
Validation Loss = 3370.803261101955
Iteration 3100/10000: Training Loss = 3370.6270639808054
Validation Loss = 3370.6270639808054
Iteration 3101/10000: Training Loss = 3370.457375018663
Validation Loss = 3370.457375018663
Iteration 3102/10000: Training Loss = 3370.283520218899
Validation Loss = 3370.283520218899
Iteration 3103/10000: Training Loss = 3370.1126707102967
Validation Loss = 3370.1126707102967
Iteration 3104/10000: Training Loss = 3369.944153799582
Validation Loss = 3369.944153799582
Iteration 3105/10000: Training Loss = 3369.771262389534
Validation Loss = 3369.771262389534
Iteration 3106/10000: Training Loss = 3369.603213311233
Validation Loss = 3369.603213311233
Iteration 3107/10000: Training Loss = 3369.426040227548
Validation Loss = 3369.426040227548
Iteration 3108/10000: Training Loss = 3369.2562612027714
Validation Loss = 3369.2562612027714
Iteration 3109/10000: Training Loss = 3369.085171404503
Validation Loss = 3369.085171404503
Iteration 3110/10000: Training Loss = 3368.9182913380673
Validation Loss = 3368.9182913380673
Iteration 3111/10000: Training Loss = 3368.742781583948
Validation Loss = 3368.742781583948
Iteration 3112/10000: Training Loss = 3368.5707916104725
Validation Loss = 3368.5707916104725
Iteration 3113/10000: Training Loss = 3368.399718338989
Validation Loss = 3368.399718338989
Iteration 3114/10000: Training Loss = 3368.2284109326815
Validation Loss = 3368.2284109326815
Iteration 3115/10000: Training Loss = 3368.058455192816
Validation Loss = 3368.058455192816
Iteration 3116/10000: Training Loss = 3367.889094658495
Validation Loss = 3367.889094658495
Iteration 3117/10000: Training Loss = 3367.7166884209555
Validation Loss = 3367.7166884209555
Iteration 3118/10000: Training Loss = 3367.5494941416796
Validation Loss = 3367.5494941416796
Iteration 3119/10000: Training Loss = 3367.376344290645
Validation Loss = 3367.376344290645
Iteration 3120/10000: Training Loss = 3367.2139553519332
Validation Loss = 3367.2139553519332
Iteration 3121/10000: Training Loss = 3367.0411335549684
Validation Loss = 3367.0411335549684
Iteration 3122/10000: Training Loss = 3366.8729729431157
Validation Loss = 3366.8729729431157
Iteration 3123/10000: Training Loss = 3366.7068412723547
Validation Loss = 3366.7068412723547
Iteration 3124/10000: Training Loss = 3366.5377584265134
Validation Loss = 3366.5377584265134
Iteration 3125/10000: Training Loss = 3366.368391315872
Validation Loss = 3366.368391315872
Iteration 3126/10000: Training Loss = 3366.198140534462
Validation Loss = 3366.198140534462
Iteration 3127/10000: Training Loss = 3366.035134509252
Validation Loss = 3366.035134509252
Iteration 3128/10000: Training Loss = 3365.8677543666704
Validation Loss = 3365.8677543666704
Iteration 3129/10000: Training Loss = 3365.6997201792233
Validation Loss = 3365.6997201792233
Iteration 3130/10000: Training Loss = 3365.5313625536405
Validation Loss = 3365.5313625536405
Iteration 3131/10000: Training Loss = 3365.3603820290914
Validation Loss = 3365.3603820290914
Iteration 3132/10000: Training Loss = 3365.1927761475813
Validation Loss = 3365.1927761475813
Iteration 3133/10000: Training Loss = 3365.0218352289367
Validation Loss = 3365.0218352289367
Iteration 3134/10000: Training Loss = 3364.8604766814965
Validation Loss = 3364.8604766814965
Iteration 3135/10000: Training Loss = 3364.703581124278
Validation Loss = 3364.703581124278
Iteration 3136/10000: Training Loss = 3364.5381722470165
Validation Loss = 3364.5381722470165
Iteration 3137/10000: Training Loss = 3364.3704308698393
Validation Loss = 3364.3704308698393
Iteration 3138/10000: Training Loss = 3364.2044651440883
Validation Loss = 3364.2044651440883
Iteration 3139/10000: Training Loss = 3364.0328989632753
Validation Loss = 3364.0328989632753
Iteration 3140/10000: Training Loss = 3363.86825825897
Validation Loss = 3363.86825825897
Iteration 3141/10000: Training Loss = 3363.700467039299
Validation Loss = 3363.700467039299
Iteration 3142/10000: Training Loss = 3363.5361716560174
Validation Loss = 3363.5361716560174
Iteration 3143/10000: Training Loss = 3363.3731030701615
Validation Loss = 3363.3731030701615
Iteration 3144/10000: Training Loss = 3363.2105415174847
Validation Loss = 3363.2105415174847
Iteration 3145/10000: Training Loss = 3363.046210335888
Validation Loss = 3363.046210335888
Iteration 3146/10000: Training Loss = 3362.8747350904146
Validation Loss = 3362.8747350904146
Iteration 3147/10000: Training Loss = 3362.7043686920188
Validation Loss = 3362.7043686920188
Iteration 3148/10000: Training Loss = 3362.5331701781
Validation Loss = 3362.5331701781
Iteration 3149/10000: Training Loss = 3362.3670624441907
Validation Loss = 3362.3670624441907
Iteration 3150/10000: Training Loss = 3362.203825819137
Validation Loss = 3362.203825819137
Iteration 3151/10000: Training Loss = 3362.0424497059216
Validation Loss = 3362.0424497059216
Iteration 3152/10000: Training Loss = 3361.8697076781737
Validation Loss = 3361.8697076781737
Iteration 3153/10000: Training Loss = 3361.6971066710994
Validation Loss = 3361.6971066710994
Iteration 3154/10000: Training Loss = 3361.5300088474623
Validation Loss = 3361.5300088474623
Iteration 3155/10000: Training Loss = 3361.3667789281694
Validation Loss = 3361.3667789281694
Iteration 3156/10000: Training Loss = 3361.2024281278254
Validation Loss = 3361.2024281278254
Iteration 3157/10000: Training Loss = 3361.0352952579565
Validation Loss = 3361.0352952579565
Iteration 3158/10000: Training Loss = 3360.8684431641414
Validation Loss = 3360.8684431641414
Iteration 3159/10000: Training Loss = 3360.702429339139
Validation Loss = 3360.702429339139
Iteration 3160/10000: Training Loss = 3360.5362965829504
Validation Loss = 3360.5362965829504
Iteration 3161/10000: Training Loss = 3360.3721843743047
Validation Loss = 3360.3721843743047
Iteration 3162/10000: Training Loss = 3360.2066021278624
Validation Loss = 3360.2066021278624
Iteration 3163/10000: Training Loss = 3360.0400710321906
Validation Loss = 3360.0400710321906
Iteration 3164/10000: Training Loss = 3359.8733956362953
Validation Loss = 3359.8733956362953
Iteration 3165/10000: Training Loss = 3359.702932866794
Validation Loss = 3359.702932866794
Iteration 3166/10000: Training Loss = 3359.5316267560015
Validation Loss = 3359.5316267560015
Iteration 3167/10000: Training Loss = 3359.3661319140742
Validation Loss = 3359.3661319140742
Iteration 3168/10000: Training Loss = 3359.1993673080838
Validation Loss = 3359.1993673080838
Iteration 3169/10000: Training Loss = 3359.0359102024536
Validation Loss = 3359.0359102024536
Iteration 3170/10000: Training Loss = 3358.8756837423143
Validation Loss = 3358.8756837423143
Iteration 3171/10000: Training Loss = 3358.711156559773
Validation Loss = 3358.711156559773
Iteration 3172/10000: Training Loss = 3358.5427553734285
Validation Loss = 3358.5427553734285
Iteration 3173/10000: Training Loss = 3358.3765423391387
Validation Loss = 3358.3765423391387
Iteration 3174/10000: Training Loss = 3358.2152663985858
Validation Loss = 3358.2152663985858
Iteration 3175/10000: Training Loss = 3358.0498563548667
Validation Loss = 3358.0498563548667
Iteration 3176/10000: Training Loss = 3357.8896788290344
Validation Loss = 3357.8896788290344
Iteration 3177/10000: Training Loss = 3357.728511200459
Validation Loss = 3357.728511200459
Iteration 3178/10000: Training Loss = 3357.560353110224
Validation Loss = 3357.560353110224
Iteration 3179/10000: Training Loss = 3357.395907455743
Validation Loss = 3357.395907455743
Iteration 3180/10000: Training Loss = 3357.2298493968756
Validation Loss = 3357.2298493968756
Iteration 3181/10000: Training Loss = 3357.0655066699796
Validation Loss = 3357.0655066699796
Iteration 3182/10000: Training Loss = 3356.8994074610173
Validation Loss = 3356.8994074610173
Iteration 3183/10000: Training Loss = 3356.739790421447
Validation Loss = 3356.739790421447
Iteration 3184/10000: Training Loss = 3356.5779281408813
Validation Loss = 3356.5779281408813
Iteration 3185/10000: Training Loss = 3356.4103449769673
Validation Loss = 3356.4103449769673
Iteration 3186/10000: Training Loss = 3356.248896103832
Validation Loss = 3356.248896103832
Iteration 3187/10000: Training Loss = 3356.085152179003
Validation Loss = 3356.085152179003
Iteration 3188/10000: Training Loss = 3355.9264460578293
Validation Loss = 3355.9264460578293
Iteration 3189/10000: Training Loss = 3355.765588878596
Validation Loss = 3355.765588878596
Iteration 3190/10000: Training Loss = 3355.6014915916585
Validation Loss = 3355.6014915916585
Iteration 3191/10000: Training Loss = 3355.440554224262
Validation Loss = 3355.440554224262
Iteration 3192/10000: Training Loss = 3355.279549569864
Validation Loss = 3355.279549569864
Iteration 3193/10000: Training Loss = 3355.1181784515857
Validation Loss = 3355.1181784515857
Iteration 3194/10000: Training Loss = 3354.957326907217
Validation Loss = 3354.957326907217
Iteration 3195/10000: Training Loss = 3354.7956067397154
Validation Loss = 3354.7956067397154
Iteration 3196/10000: Training Loss = 3354.6311290043172
Validation Loss = 3354.6311290043172
Iteration 3197/10000: Training Loss = 3354.4663857635655
Validation Loss = 3354.4663857635655
Iteration 3198/10000: Training Loss = 3354.303366246453
Validation Loss = 3354.303366246453
Iteration 3199/10000: Training Loss = 3354.1409224425374
Validation Loss = 3354.1409224425374
Iteration 3200/10000: Training Loss = 3353.9838477866947
Validation Loss = 3353.9838477866947
Iteration 3201/10000: Training Loss = 3353.8246380410314
Validation Loss = 3353.8246380410314
Iteration 3202/10000: Training Loss = 3353.6669671659897
Validation Loss = 3353.6669671659897
Iteration 3203/10000: Training Loss = 3353.5023692792524
Validation Loss = 3353.5023692792524
Iteration 3204/10000: Training Loss = 3353.3395804163374
Validation Loss = 3353.3395804163374
Iteration 3205/10000: Training Loss = 3353.1822283479178
Validation Loss = 3353.1822283479178
Iteration 3206/10000: Training Loss = 3353.018801269331
Validation Loss = 3353.018801269331
Iteration 3207/10000: Training Loss = 3352.8562588840823
Validation Loss = 3352.8562588840823
Iteration 3208/10000: Training Loss = 3352.6988028032165
Validation Loss = 3352.6988028032165
Iteration 3209/10000: Training Loss = 3352.538428799881
Validation Loss = 3352.538428799881
Iteration 3210/10000: Training Loss = 3352.3757457713414
Validation Loss = 3352.3757457713414
Iteration 3211/10000: Training Loss = 3352.216920984974
Validation Loss = 3352.216920984974
Iteration 3212/10000: Training Loss = 3352.052996187845
Validation Loss = 3352.052996187845
Iteration 3213/10000: Training Loss = 3351.8922356533253
Validation Loss = 3351.8922356533253
Iteration 3214/10000: Training Loss = 3351.7313105716644
Validation Loss = 3351.7313105716644
Iteration 3215/10000: Training Loss = 3351.569958313123
Validation Loss = 3351.569958313123
Iteration 3216/10000: Training Loss = 3351.4136113425093
Validation Loss = 3351.4136113425093
Iteration 3217/10000: Training Loss = 3351.2623295270505
Validation Loss = 3351.2623295270505
Iteration 3218/10000: Training Loss = 3351.1024607014683
Validation Loss = 3351.1024607014683
Iteration 3219/10000: Training Loss = 3350.931809809728
Validation Loss = 3350.931809809728
Iteration 3220/10000: Training Loss = 3350.764530940982
Validation Loss = 3350.764530940982
Iteration 3221/10000: Training Loss = 3350.6068116882534
Validation Loss = 3350.6068116882534
Iteration 3222/10000: Training Loss = 3350.447917722973
Validation Loss = 3350.447917722973
Iteration 3223/10000: Training Loss = 3350.2846585311536
Validation Loss = 3350.2846585311536
Iteration 3224/10000: Training Loss = 3350.1242535927345
Validation Loss = 3350.1242535927345
Iteration 3225/10000: Training Loss = 3349.963000515512
Validation Loss = 3349.963000515512
Iteration 3226/10000: Training Loss = 3349.8016133866886
Validation Loss = 3349.8016133866886
Iteration 3227/10000: Training Loss = 3349.6432890450033
Validation Loss = 3349.6432890450033
Iteration 3228/10000: Training Loss = 3349.4917412788436
Validation Loss = 3349.4917412788436
Iteration 3229/10000: Training Loss = 3349.3311004623047
Validation Loss = 3349.3311004623047
Iteration 3230/10000: Training Loss = 3349.185748474586
Validation Loss = 3349.185748474586
Iteration 3231/10000: Training Loss = 3349.016428988529
Validation Loss = 3349.016428988529
Iteration 3232/10000: Training Loss = 3348.856170436731
Validation Loss = 3348.856170436731
Iteration 3233/10000: Training Loss = 3348.6998991988053
Validation Loss = 3348.6998991988053
Iteration 3234/10000: Training Loss = 3348.5354979824665
Validation Loss = 3348.5354979824665
Iteration 3235/10000: Training Loss = 3348.3730807570078
Validation Loss = 3348.3730807570078
Iteration 3236/10000: Training Loss = 3348.2124762545313
Validation Loss = 3348.2124762545313
Iteration 3237/10000: Training Loss = 3348.048039581461
Validation Loss = 3348.048039581461
Iteration 3238/10000: Training Loss = 3347.891133363396
Validation Loss = 3347.891133363396
Iteration 3239/10000: Training Loss = 3347.7377411781285
Validation Loss = 3347.7377411781285
Iteration 3240/10000: Training Loss = 3347.5862343948556
Validation Loss = 3347.5862343948556
Iteration 3241/10000: Training Loss = 3347.41591334727
Validation Loss = 3347.41591334727
Iteration 3242/10000: Training Loss = 3347.255296031784
Validation Loss = 3347.255296031784
Iteration 3243/10000: Training Loss = 3347.099053589253
Validation Loss = 3347.099053589253
Iteration 3244/10000: Training Loss = 3346.93964634324
Validation Loss = 3346.93964634324
Iteration 3245/10000: Training Loss = 3346.780582939507
Validation Loss = 3346.780582939507
Iteration 3246/10000: Training Loss = 3346.619692219312
Validation Loss = 3346.619692219312
Iteration 3247/10000: Training Loss = 3346.462131680131
Validation Loss = 3346.462131680131
Iteration 3248/10000: Training Loss = 3346.3030240866283
Validation Loss = 3346.3030240866283
Iteration 3249/10000: Training Loss = 3346.144300723057
Validation Loss = 3346.144300723057
Iteration 3250/10000: Training Loss = 3345.9872539518096
Validation Loss = 3345.9872539518096
Iteration 3251/10000: Training Loss = 3345.824319458052
Validation Loss = 3345.824319458052
Iteration 3252/10000: Training Loss = 3345.665126335133
Validation Loss = 3345.665126335133
Iteration 3253/10000: Training Loss = 3345.5020129160994
Validation Loss = 3345.5020129160994
Iteration 3254/10000: Training Loss = 3345.347120927774
Validation Loss = 3345.347120927774
Iteration 3255/10000: Training Loss = 3345.187338093102
Validation Loss = 3345.187338093102
Iteration 3256/10000: Training Loss = 3345.0344533646585
Validation Loss = 3345.0344533646585
Iteration 3257/10000: Training Loss = 3344.875801852042
Validation Loss = 3344.875801852042
Iteration 3258/10000: Training Loss = 3344.7201306531097
Validation Loss = 3344.7201306531097
Iteration 3259/10000: Training Loss = 3344.5674005656833
Validation Loss = 3344.5674005656833
Iteration 3260/10000: Training Loss = 3344.402285323805
Validation Loss = 3344.402285323805
Iteration 3261/10000: Training Loss = 3344.2458067570974
Validation Loss = 3344.2458067570974
Iteration 3262/10000: Training Loss = 3344.090699465451
Validation Loss = 3344.090699465451
Iteration 3263/10000: Training Loss = 3343.940098353491
Validation Loss = 3343.940098353491
Iteration 3264/10000: Training Loss = 3343.782983027494
Validation Loss = 3343.782983027494
Iteration 3265/10000: Training Loss = 3343.627488097199
Validation Loss = 3343.627488097199
Iteration 3266/10000: Training Loss = 3343.4734146482238
Validation Loss = 3343.4734146482238
Iteration 3267/10000: Training Loss = 3343.3159892095778
Validation Loss = 3343.3159892095778
Iteration 3268/10000: Training Loss = 3343.163956862365
Validation Loss = 3343.163956862365
Iteration 3269/10000: Training Loss = 3343.0102052651573
Validation Loss = 3343.0102052651573
Iteration 3270/10000: Training Loss = 3342.85037107823
Validation Loss = 3342.85037107823
Iteration 3271/10000: Training Loss = 3342.6930202381095
Validation Loss = 3342.6930202381095
Iteration 3272/10000: Training Loss = 3342.53764633776
Validation Loss = 3342.53764633776
Iteration 3273/10000: Training Loss = 3342.383130016064
Validation Loss = 3342.383130016064
Iteration 3274/10000: Training Loss = 3342.2217069899175
Validation Loss = 3342.2217069899175
Iteration 3275/10000: Training Loss = 3342.0638900341182
Validation Loss = 3342.0638900341182
Iteration 3276/10000: Training Loss = 3341.9098120433678
Validation Loss = 3341.9098120433678
Iteration 3277/10000: Training Loss = 3341.7541223735843
Validation Loss = 3341.7541223735843
Iteration 3278/10000: Training Loss = 3341.600823892482
Validation Loss = 3341.600823892482
Iteration 3279/10000: Training Loss = 3341.439214735602
Validation Loss = 3341.439214735602
Iteration 3280/10000: Training Loss = 3341.2821996119633
Validation Loss = 3341.2821996119633
Iteration 3281/10000: Training Loss = 3341.1307605040934
Validation Loss = 3341.1307605040934
Iteration 3282/10000: Training Loss = 3340.976231751438
Validation Loss = 3340.976231751438
Iteration 3283/10000: Training Loss = 3340.8215297081765
Validation Loss = 3340.8215297081765
Iteration 3284/10000: Training Loss = 3340.6691556441315
Validation Loss = 3340.6691556441315
Iteration 3285/10000: Training Loss = 3340.510256915212
Validation Loss = 3340.510256915212
Iteration 3286/10000: Training Loss = 3340.3536697065083
Validation Loss = 3340.3536697065083
Iteration 3287/10000: Training Loss = 3340.1985327011107
Validation Loss = 3340.1985327011107
Iteration 3288/10000: Training Loss = 3340.0413786984054
Validation Loss = 3340.0413786984054
Iteration 3289/10000: Training Loss = 3339.8887273146856
Validation Loss = 3339.8887273146856
Iteration 3290/10000: Training Loss = 3339.735711878804
Validation Loss = 3339.735711878804
Iteration 3291/10000: Training Loss = 3339.5797707372694
Validation Loss = 3339.5797707372694
Iteration 3292/10000: Training Loss = 3339.424517963503
Validation Loss = 3339.424517963503
Iteration 3293/10000: Training Loss = 3339.2729683660104
Validation Loss = 3339.2729683660104
Iteration 3294/10000: Training Loss = 3339.1197569345254
Validation Loss = 3339.1197569345254
Iteration 3295/10000: Training Loss = 3338.9662025577427
Validation Loss = 3338.9662025577427
Iteration 3296/10000: Training Loss = 3338.814162478144
Validation Loss = 3338.814162478144
Iteration 3297/10000: Training Loss = 3338.6661937318872
Validation Loss = 3338.6661937318872
Iteration 3298/10000: Training Loss = 3338.505808001613
Validation Loss = 3338.505808001613
Iteration 3299/10000: Training Loss = 3338.3493462207116
Validation Loss = 3338.3493462207116
Iteration 3300/10000: Training Loss = 3338.195036310249
Validation Loss = 3338.195036310249
Iteration 3301/10000: Training Loss = 3338.0426820225744
Validation Loss = 3338.0426820225744
Iteration 3302/10000: Training Loss = 3337.8935989678557
Validation Loss = 3337.8935989678557
Iteration 3303/10000: Training Loss = 3337.7391259234278
Validation Loss = 3337.7391259234278
Iteration 3304/10000: Training Loss = 3337.5863744983485
Validation Loss = 3337.5863744983485
Iteration 3305/10000: Training Loss = 3337.432068571811
Validation Loss = 3337.432068571811
Iteration 3306/10000: Training Loss = 3337.282621604524
Validation Loss = 3337.282621604524
Iteration 3307/10000: Training Loss = 3337.1366140879713
Validation Loss = 3337.1366140879713
Iteration 3308/10000: Training Loss = 3336.9899806939948
Validation Loss = 3336.9899806939948
Iteration 3309/10000: Training Loss = 3336.831386821133
Validation Loss = 3336.831386821133
Iteration 3310/10000: Training Loss = 3336.673118324998
Validation Loss = 3336.673118324998
Iteration 3311/10000: Training Loss = 3336.515883210852
Validation Loss = 3336.515883210852
Iteration 3312/10000: Training Loss = 3336.3658060900434
Validation Loss = 3336.3658060900434
Iteration 3313/10000: Training Loss = 3336.214639971238
Validation Loss = 3336.214639971238
Iteration 3314/10000: Training Loss = 3336.0699379203315
Validation Loss = 3336.0699379203315
Iteration 3315/10000: Training Loss = 3335.9106269483505
Validation Loss = 3335.9106269483505
Iteration 3316/10000: Training Loss = 3335.7580580933645
Validation Loss = 3335.7580580933645
Iteration 3317/10000: Training Loss = 3335.600707456967
Validation Loss = 3335.600707456967
Iteration 3318/10000: Training Loss = 3335.4493413101636
Validation Loss = 3335.4493413101636
Iteration 3319/10000: Training Loss = 3335.297447767957
Validation Loss = 3335.297447767957
Iteration 3320/10000: Training Loss = 3335.151177625623
Validation Loss = 3335.151177625623
Iteration 3321/10000: Training Loss = 3334.9965913291676
Validation Loss = 3334.9965913291676
Iteration 3322/10000: Training Loss = 3334.8498059724016
Validation Loss = 3334.8498059724016
Iteration 3323/10000: Training Loss = 3334.6955490652313
Validation Loss = 3334.6955490652313
Iteration 3324/10000: Training Loss = 3334.544597307492
Validation Loss = 3334.544597307492
Iteration 3325/10000: Training Loss = 3334.397852205284
Validation Loss = 3334.397852205284
Iteration 3326/10000: Training Loss = 3334.251361025339
Validation Loss = 3334.251361025339
Iteration 3327/10000: Training Loss = 3334.0977229989185
Validation Loss = 3334.0977229989185
Iteration 3328/10000: Training Loss = 3333.943258738321
Validation Loss = 3333.943258738321
Iteration 3329/10000: Training Loss = 3333.792992860603
Validation Loss = 3333.792992860603
Iteration 3330/10000: Training Loss = 3333.64180204273
Validation Loss = 3333.64180204273
Iteration 3331/10000: Training Loss = 3333.4876837168367
Validation Loss = 3333.4876837168367
Iteration 3332/10000: Training Loss = 3333.3371279517264
Validation Loss = 3333.3371279517264
Iteration 3333/10000: Training Loss = 3333.186220945783
Validation Loss = 3333.186220945783
Iteration 3334/10000: Training Loss = 3333.040163000972
Validation Loss = 3333.040163000972
Iteration 3335/10000: Training Loss = 3332.886946432767
Validation Loss = 3332.886946432767
Iteration 3336/10000: Training Loss = 3332.7373694561875
Validation Loss = 3332.7373694561875
Iteration 3337/10000: Training Loss = 3332.5911876656946
Validation Loss = 3332.5911876656946
Iteration 3338/10000: Training Loss = 3332.439905752347
Validation Loss = 3332.439905752347
Iteration 3339/10000: Training Loss = 3332.289702370331
Validation Loss = 3332.289702370331
Iteration 3340/10000: Training Loss = 3332.151379369795
Validation Loss = 3332.151379369795
Iteration 3341/10000: Training Loss = 3331.9997777633293
Validation Loss = 3331.9997777633293
Iteration 3342/10000: Training Loss = 3331.850770150565
Validation Loss = 3331.850770150565
Iteration 3343/10000: Training Loss = 3331.6938656983566
Validation Loss = 3331.6938656983566
Iteration 3344/10000: Training Loss = 3331.544349607638
Validation Loss = 3331.544349607638
Iteration 3345/10000: Training Loss = 3331.395812566135
Validation Loss = 3331.395812566135
Iteration 3346/10000: Training Loss = 3331.247500914583
Validation Loss = 3331.247500914583
Iteration 3347/10000: Training Loss = 3331.0999744893566
Validation Loss = 3331.0999744893566
Iteration 3348/10000: Training Loss = 3330.9458078534954
Validation Loss = 3330.9458078534954
Iteration 3349/10000: Training Loss = 3330.7973884760304
Validation Loss = 3330.7973884760304
Iteration 3350/10000: Training Loss = 3330.6470301506683
Validation Loss = 3330.6470301506683
Iteration 3351/10000: Training Loss = 3330.4946399834093
Validation Loss = 3330.4946399834093
Iteration 3352/10000: Training Loss = 3330.3443629559606
Validation Loss = 3330.3443629559606
Iteration 3353/10000: Training Loss = 3330.1972165591783
Validation Loss = 3330.1972165591783
Iteration 3354/10000: Training Loss = 3330.049372569632
Validation Loss = 3330.049372569632
Iteration 3355/10000: Training Loss = 3329.9014861606292
Validation Loss = 3329.9014861606292
Iteration 3356/10000: Training Loss = 3329.753449977917
Validation Loss = 3329.753449977917
Iteration 3357/10000: Training Loss = 3329.6045121709167
Validation Loss = 3329.6045121709167
Iteration 3358/10000: Training Loss = 3329.457369275044
Validation Loss = 3329.457369275044
Iteration 3359/10000: Training Loss = 3329.3093234937546
Validation Loss = 3329.3093234937546
Iteration 3360/10000: Training Loss = 3329.1643820665154
Validation Loss = 3329.1643820665154
Iteration 3361/10000: Training Loss = 3329.0172197679003
Validation Loss = 3329.0172197679003
Iteration 3362/10000: Training Loss = 3328.867593213985
Validation Loss = 3328.867593213985
Iteration 3363/10000: Training Loss = 3328.7134652706854
Validation Loss = 3328.7134652706854
Iteration 3364/10000: Training Loss = 3328.5654921170017
Validation Loss = 3328.5654921170017
Iteration 3365/10000: Training Loss = 3328.4094823702826
Validation Loss = 3328.4094823702826
Iteration 3366/10000: Training Loss = 3328.2520442894665
Validation Loss = 3328.2520442894665
Iteration 3367/10000: Training Loss = 3328.103167434309
Validation Loss = 3328.103167434309
Iteration 3368/10000: Training Loss = 3327.952573693372
Validation Loss = 3327.952573693372
Iteration 3369/10000: Training Loss = 3327.8046630733716
Validation Loss = 3327.8046630733716
Iteration 3370/10000: Training Loss = 3327.65579517457
Validation Loss = 3327.65579517457
Iteration 3371/10000: Training Loss = 3327.509798727323
Validation Loss = 3327.509798727323
Iteration 3372/10000: Training Loss = 3327.3627989562788
Validation Loss = 3327.3627989562788
Iteration 3373/10000: Training Loss = 3327.217035764001
Validation Loss = 3327.217035764001
Iteration 3374/10000: Training Loss = 3327.0714420615395
Validation Loss = 3327.0714420615395
Iteration 3375/10000: Training Loss = 3326.926894265786
Validation Loss = 3326.926894265786
Iteration 3376/10000: Training Loss = 3326.7858717824283
Validation Loss = 3326.7858717824283
Iteration 3377/10000: Training Loss = 3326.658964339188
Validation Loss = 3326.658964339188
Iteration 3378/10000: Training Loss = 3326.5119145874082
Validation Loss = 3326.5119145874082
Iteration 3379/10000: Training Loss = 3326.3576401524538
Validation Loss = 3326.3576401524538
Iteration 3380/10000: Training Loss = 3326.217705406864
Validation Loss = 3326.217705406864
Iteration 3381/10000: Training Loss = 3326.08870983794
Validation Loss = 3326.08870983794
Early stopping at epoch 3381
Training finished after 3381 iterations
Mean squared error: 2705.02
Coefficient of determination: 0.44
