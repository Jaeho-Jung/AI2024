{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/100: Training Loss = 2.1899701191931253, Training Accuracy = 0.1771626984126984\n",
      "Validation Loss = 2.1887353657944395, Validation Accuracy = 0.17875\n",
      "Iteration 2/100: Training Loss = 2.094145572782628, Training Accuracy = 0.2745039682539683\n",
      "Validation Loss = 2.0947109730185494, Validation Accuracy = 0.27375\n",
      "Iteration 3/100: Training Loss = 2.0092337050446507, Training Accuracy = 0.3632142857142857\n",
      "Validation Loss = 2.011143401728373, Validation Accuracy = 0.35910714285714285\n",
      "Iteration 4/100: Training Loss = 1.9303000814880191, Training Accuracy = 0.42678571428571427\n",
      "Validation Loss = 1.9334300223245338, Validation Accuracy = 0.4251785714285714\n",
      "Iteration 5/100: Training Loss = 1.855321830802882, Training Accuracy = 0.47180555555555553\n",
      "Validation Loss = 1.8594352869962485, Validation Accuracy = 0.46482142857142855\n",
      "Iteration 6/100: Training Loss = 1.7834703565696275, Training Accuracy = 0.503531746031746\n",
      "Validation Loss = 1.7887172068136032, Validation Accuracy = 0.49625\n",
      "Iteration 7/100: Training Loss = 1.7152078747116968, Training Accuracy = 0.5220634920634921\n",
      "Validation Loss = 1.721720581681265, Validation Accuracy = 0.5110714285714286\n",
      "Iteration 8/100: Training Loss = 1.6509038241998855, Training Accuracy = 0.5370039682539682\n",
      "Validation Loss = 1.6587905596587806, Validation Accuracy = 0.5257142857142857\n",
      "Iteration 9/100: Training Loss = 1.590513847880994, Training Accuracy = 0.5491865079365079\n",
      "Validation Loss = 1.599671119852838, Validation Accuracy = 0.5394642857142857\n",
      "Iteration 10/100: Training Loss = 1.5339090447294221, Training Accuracy = 0.560952380952381\n",
      "Validation Loss = 1.5442815089435444, Validation Accuracy = 0.5521428571428572\n",
      "Iteration 11/100: Training Loss = 1.4808377808497675, Training Accuracy = 0.5725595238095238\n",
      "Validation Loss = 1.4923389533667377, Validation Accuracy = 0.5639285714285714\n",
      "Iteration 12/100: Training Loss = 1.4311679077321828, Training Accuracy = 0.5840277777777778\n",
      "Validation Loss = 1.4436605641015852, Validation Accuracy = 0.5739285714285715\n",
      "Iteration 13/100: Training Loss = 1.384689831142683, Training Accuracy = 0.5962301587301587\n",
      "Validation Loss = 1.3981027012090408, Validation Accuracy = 0.5833928571428572\n",
      "Iteration 14/100: Training Loss = 1.3411741915486386, Training Accuracy = 0.6088690476190476\n",
      "Validation Loss = 1.3553997534243798, Validation Accuracy = 0.5964285714285714\n",
      "Iteration 15/100: Training Loss = 1.3003566737633272, Training Accuracy = 0.6224007936507937\n",
      "Validation Loss = 1.3152903571131482, Validation Accuracy = 0.6130357142857142\n",
      "Iteration 16/100: Training Loss = 1.2619379668445772, Training Accuracy = 0.6370039682539682\n",
      "Validation Loss = 1.2775000220267583, Validation Accuracy = 0.6285714285714286\n",
      "Iteration 17/100: Training Loss = 1.2256900384631588, Training Accuracy = 0.6501785714285714\n",
      "Validation Loss = 1.2418080359211257, Validation Accuracy = 0.6416071428571428\n",
      "Iteration 18/100: Training Loss = 1.1913492420616798, Training Accuracy = 0.6628571428571428\n",
      "Validation Loss = 1.2079758900462774, Validation Accuracy = 0.6544642857142857\n",
      "Iteration 19/100: Training Loss = 1.1587426353608428, Training Accuracy = 0.6741468253968254\n",
      "Validation Loss = 1.1758234624213928, Validation Accuracy = 0.6655357142857142\n",
      "Iteration 20/100: Training Loss = 1.1277058231312356, Training Accuracy = 0.6861706349206349\n",
      "Validation Loss = 1.1451831223202342, Validation Accuracy = 0.6733928571428571\n",
      "Iteration 21/100: Training Loss = 1.0981158908941369, Training Accuracy = 0.6972619047619047\n",
      "Validation Loss = 1.1159567120670262, Validation Accuracy = 0.6869642857142857\n",
      "Iteration 22/100: Training Loss = 1.0699298240459554, Training Accuracy = 0.7088492063492063\n",
      "Validation Loss = 1.0880589917944343, Validation Accuracy = 0.6998214285714286\n",
      "Iteration 23/100: Training Loss = 1.0430876664598945, Training Accuracy = 0.7186706349206349\n",
      "Validation Loss = 1.0614552961433792, Validation Accuracy = 0.7085714285714285\n",
      "Iteration 24/100: Training Loss = 1.0175069133277148, Training Accuracy = 0.7282539682539683\n",
      "Validation Loss = 1.0361254406634752, Validation Accuracy = 0.72\n",
      "Iteration 25/100: Training Loss = 0.9931693580305547, Training Accuracy = 0.73625\n",
      "Validation Loss = 1.0119981054505889, Validation Accuracy = 0.73\n",
      "Iteration 26/100: Training Loss = 0.970041284717051, Training Accuracy = 0.744186507936508\n",
      "Validation Loss = 0.9890337197743084, Validation Accuracy = 0.7376785714285714\n",
      "Iteration 27/100: Training Loss = 0.9480637828062668, Training Accuracy = 0.7509722222222223\n",
      "Validation Loss = 0.9672196471290527, Validation Accuracy = 0.7426785714285714\n",
      "Iteration 28/100: Training Loss = 0.9272080742082142, Training Accuracy = 0.7559325396825397\n",
      "Validation Loss = 0.9465170100215965, Validation Accuracy = 0.7494642857142857\n",
      "Iteration 29/100: Training Loss = 0.907424278923511, Training Accuracy = 0.7613690476190477\n",
      "Validation Loss = 0.9268729260190441, Validation Accuracy = 0.7555357142857143\n",
      "Iteration 30/100: Training Loss = 0.8886740903713555, Training Accuracy = 0.766468253968254\n",
      "Validation Loss = 0.9082378404919893, Validation Accuracy = 0.7610714285714286\n",
      "Iteration 31/100: Training Loss = 0.8709154135952368, Training Accuracy = 0.7705952380952381\n",
      "Validation Loss = 0.8905819902660195, Validation Accuracy = 0.7653571428571428\n",
      "Iteration 32/100: Training Loss = 0.8540850436278071, Training Accuracy = 0.7743849206349206\n",
      "Validation Loss = 0.873854104482, Validation Accuracy = 0.7683928571428571\n",
      "Iteration 33/100: Training Loss = 0.8381181881902544, Training Accuracy = 0.7785714285714286\n",
      "Validation Loss = 0.8579780290368745, Validation Accuracy = 0.7717857142857143\n",
      "Iteration 34/100: Training Loss = 0.8229416737529598, Training Accuracy = 0.7822420634920635\n",
      "Validation Loss = 0.8429024809404255, Validation Accuracy = 0.7757142857142857\n",
      "Iteration 35/100: Training Loss = 0.8085039295463313, Training Accuracy = 0.7858333333333334\n",
      "Validation Loss = 0.8285767067451809, Validation Accuracy = 0.77875\n",
      "Iteration 36/100: Training Loss = 0.7947471626943475, Training Accuracy = 0.789702380952381\n",
      "Validation Loss = 0.8149429420184247, Validation Accuracy = 0.7832142857142858\n",
      "Iteration 37/100: Training Loss = 0.7816222250304687, Training Accuracy = 0.7933531746031746\n",
      "Validation Loss = 0.8019362346159356, Validation Accuracy = 0.78625\n",
      "Iteration 38/100: Training Loss = 0.7690782473006912, Training Accuracy = 0.7970238095238096\n",
      "Validation Loss = 0.7894980460487706, Validation Accuracy = 0.7885714285714286\n",
      "Iteration 39/100: Training Loss = 0.7570665809990654, Training Accuracy = 0.8005555555555556\n",
      "Validation Loss = 0.777592802502525, Validation Accuracy = 0.7905357142857142\n",
      "Iteration 40/100: Training Loss = 0.7455363673493691, Training Accuracy = 0.80375\n",
      "Validation Loss = 0.7661669265515599, Validation Accuracy = 0.7930357142857143\n",
      "Iteration 41/100: Training Loss = 0.7344476136561185, Training Accuracy = 0.8064484126984127\n",
      "Validation Loss = 0.755181939086263, Validation Accuracy = 0.7955357142857142\n",
      "Iteration 42/100: Training Loss = 0.7237699365046516, Training Accuracy = 0.8096031746031747\n",
      "Validation Loss = 0.7446112839300782, Validation Accuracy = 0.79875\n",
      "Iteration 43/100: Training Loss = 0.7134726932586359, Training Accuracy = 0.8128968253968254\n",
      "Validation Loss = 0.7344187142629454, Validation Accuracy = 0.8021428571428572\n",
      "Iteration 44/100: Training Loss = 0.7035229054397265, Training Accuracy = 0.8154960317460317\n",
      "Validation Loss = 0.7245637905424378, Validation Accuracy = 0.8053571428571429\n",
      "Iteration 45/100: Training Loss = 0.6938889596383329, Training Accuracy = 0.8179166666666666\n",
      "Validation Loss = 0.7150412797116583, Validation Accuracy = 0.8073214285714285\n",
      "Iteration 46/100: Training Loss = 0.6845641246461539, Training Accuracy = 0.8200198412698413\n",
      "Validation Loss = 0.7058314657054552, Validation Accuracy = 0.8101785714285714\n",
      "Iteration 47/100: Training Loss = 0.6755321467616173, Training Accuracy = 0.8224007936507937\n",
      "Validation Loss = 0.6969143594996634, Validation Accuracy = 0.8123214285714285\n",
      "Iteration 48/100: Training Loss = 0.6667724113809549, Training Accuracy = 0.8249007936507936\n",
      "Validation Loss = 0.6882697693000619, Validation Accuracy = 0.81375\n",
      "Iteration 49/100: Training Loss = 0.6582737201047614, Training Accuracy = 0.8270833333333333\n",
      "Validation Loss = 0.6798683837777428, Validation Accuracy = 0.8148214285714286\n",
      "Iteration 50/100: Training Loss = 0.6500202559076118, Training Accuracy = 0.8292857142857143\n",
      "Validation Loss = 0.6717064755537097, Validation Accuracy = 0.8176785714285715\n",
      "Iteration 51/100: Training Loss = 0.642007099911896, Training Accuracy = 0.8319642857142857\n",
      "Validation Loss = 0.6637792140890482, Validation Accuracy = 0.8217857142857142\n",
      "Iteration 52/100: Training Loss = 0.6342204849067532, Training Accuracy = 0.834186507936508\n",
      "Validation Loss = 0.6560528799816003, Validation Accuracy = 0.8244642857142858\n",
      "Iteration 53/100: Training Loss = 0.626650982527635, Training Accuracy = 0.8365674603174603\n",
      "Validation Loss = 0.6485502876075567, Validation Accuracy = 0.8260714285714286\n",
      "Iteration 54/100: Training Loss = 0.6193034895305803, Training Accuracy = 0.838313492063492\n",
      "Validation Loss = 0.6412640503859856, Validation Accuracy = 0.8294642857142858\n",
      "Iteration 55/100: Training Loss = 0.6121777394426587, Training Accuracy = 0.8402777777777778\n",
      "Validation Loss = 0.6341862595652414, Validation Accuracy = 0.8314285714285714\n",
      "Iteration 56/100: Training Loss = 0.6052630910021491, Training Accuracy = 0.8418055555555556\n",
      "Validation Loss = 0.6273102288972785, Validation Accuracy = 0.83375\n",
      "Iteration 57/100: Training Loss = 0.5985434388589093, Training Accuracy = 0.8431547619047619\n",
      "Validation Loss = 0.6206254540539802, Validation Accuracy = 0.8348214285714286\n",
      "Iteration 58/100: Training Loss = 0.5920199687440018, Training Accuracy = 0.8446825396825397\n",
      "Validation Loss = 0.6141294811022754, Validation Accuracy = 0.8366071428571429\n",
      "Iteration 59/100: Training Loss = 0.5856984132536053, Training Accuracy = 0.8458730158730159\n",
      "Validation Loss = 0.6078374124824216, Validation Accuracy = 0.8385714285714285\n",
      "Iteration 60/100: Training Loss = 0.5795638330723386, Training Accuracy = 0.8474007936507937\n",
      "Validation Loss = 0.6017362032519888, Validation Accuracy = 0.8403571428571428\n",
      "Iteration 61/100: Training Loss = 0.5736165657143026, Training Accuracy = 0.8492261904761905\n",
      "Validation Loss = 0.5958258045843597, Validation Accuracy = 0.8425\n",
      "Iteration 62/100: Training Loss = 0.5678632482075082, Training Accuracy = 0.8502777777777778\n",
      "Validation Loss = 0.5901134927405494, Validation Accuracy = 0.8448214285714286\n",
      "Iteration 63/100: Training Loss = 0.5622944575737882, Training Accuracy = 0.8518650793650794\n",
      "Validation Loss = 0.5845905050977495, Validation Accuracy = 0.8458928571428571\n",
      "Iteration 64/100: Training Loss = 0.5569091840093012, Training Accuracy = 0.8532539682539683\n",
      "Validation Loss = 0.5792546755765158, Validation Accuracy = 0.8466071428571429\n",
      "Iteration 65/100: Training Loss = 0.5516936081072928, Training Accuracy = 0.8541071428571428\n",
      "Validation Loss = 0.5740965748670133, Validation Accuracy = 0.8480357142857143\n",
      "Iteration 66/100: Training Loss = 0.5466485200194716, Training Accuracy = 0.8551984126984127\n",
      "Validation Loss = 0.5691071272390209, Validation Accuracy = 0.8485714285714285\n",
      "Iteration 67/100: Training Loss = 0.5417661178920735, Training Accuracy = 0.8561507936507936\n",
      "Validation Loss = 0.5642831795903589, Validation Accuracy = 0.8503571428571428\n",
      "Iteration 68/100: Training Loss = 0.5370377075510303, Training Accuracy = 0.8568253968253968\n",
      "Validation Loss = 0.5596199304068882, Validation Accuracy = 0.8510714285714286\n",
      "Iteration 69/100: Training Loss = 0.5324567165055595, Training Accuracy = 0.8579563492063492\n",
      "Validation Loss = 0.5550997532903449, Validation Accuracy = 0.8525\n",
      "Iteration 70/100: Training Loss = 0.5280200229572808, Training Accuracy = 0.8588690476190476\n",
      "Validation Loss = 0.5507204896170833, Validation Accuracy = 0.85375\n",
      "Iteration 71/100: Training Loss = 0.5237228524127722, Training Accuracy = 0.8599404761904762\n",
      "Validation Loss = 0.5464708903642046, Validation Accuracy = 0.8548214285714286\n",
      "Iteration 72/100: Training Loss = 0.5195620531465509, Training Accuracy = 0.8605753968253969\n",
      "Validation Loss = 0.5423597162051998, Validation Accuracy = 0.855\n",
      "Iteration 73/100: Training Loss = 0.515537738977274, Training Accuracy = 0.8615277777777778\n",
      "Validation Loss = 0.5383814390930873, Validation Accuracy = 0.8557142857142858\n",
      "Iteration 74/100: Training Loss = 0.5116391703950294, Training Accuracy = 0.8623412698412698\n",
      "Validation Loss = 0.5345278827190774, Validation Accuracy = 0.8564285714285714\n",
      "Iteration 75/100: Training Loss = 0.507855586460998, Training Accuracy = 0.8632936507936508\n",
      "Validation Loss = 0.5307914904321083, Validation Accuracy = 0.8569642857142857\n",
      "Iteration 76/100: Training Loss = 0.5041891761583159, Training Accuracy = 0.8640277777777777\n",
      "Validation Loss = 0.5271730970424113, Validation Accuracy = 0.8576785714285714\n",
      "Iteration 77/100: Training Loss = 0.5006346772959697, Training Accuracy = 0.8646031746031746\n",
      "Validation Loss = 0.5236717237493149, Validation Accuracy = 0.8580357142857142\n",
      "Iteration 78/100: Training Loss = 0.49719182310572224, Training Accuracy = 0.8654563492063492\n",
      "Validation Loss = 0.5202872226846329, Validation Accuracy = 0.8580357142857142\n",
      "Iteration 79/100: Training Loss = 0.49384822633267744, Training Accuracy = 0.8659920634920635\n",
      "Validation Loss = 0.5170037578245114, Validation Accuracy = 0.8583928571428572\n",
      "Iteration 80/100: Training Loss = 0.49060738791528574, Training Accuracy = 0.8666468253968254\n",
      "Validation Loss = 0.5138202161087586, Validation Accuracy = 0.8594642857142857\n",
      "Iteration 81/100: Training Loss = 0.4874572195245709, Training Accuracy = 0.8670436507936508\n",
      "Validation Loss = 0.5107155402594167, Validation Accuracy = 0.8608928571428571\n",
      "Iteration 82/100: Training Loss = 0.48439441239041703, Training Accuracy = 0.8678373015873015\n",
      "Validation Loss = 0.5077021358304922, Validation Accuracy = 0.86125\n",
      "Iteration 83/100: Training Loss = 0.48141843600435646, Training Accuracy = 0.868452380952381\n",
      "Validation Loss = 0.5047792750926017, Validation Accuracy = 0.8614285714285714\n",
      "Iteration 84/100: Training Loss = 0.4785242022979137, Training Accuracy = 0.8691468253968254\n",
      "Validation Loss = 0.5019406017571787, Validation Accuracy = 0.8619642857142857\n",
      "Iteration 85/100: Training Loss = 0.47570879285784495, Training Accuracy = 0.869702380952381\n",
      "Validation Loss = 0.49917946841923466, Validation Accuracy = 0.8625\n",
      "Iteration 86/100: Training Loss = 0.4729726109061065, Training Accuracy = 0.8701984126984127\n",
      "Validation Loss = 0.4965006714801271, Validation Accuracy = 0.8633928571428572\n",
      "Iteration 87/100: Training Loss = 0.4703084465590656, Training Accuracy = 0.8708333333333333\n",
      "Validation Loss = 0.4938939758306725, Validation Accuracy = 0.86375\n",
      "Iteration 88/100: Training Loss = 0.467718331572456, Training Accuracy = 0.8712896825396825\n",
      "Validation Loss = 0.49135873485279524, Validation Accuracy = 0.8641071428571429\n",
      "Iteration 89/100: Training Loss = 0.46519251293553016, Training Accuracy = 0.8718650793650794\n",
      "Validation Loss = 0.4888908902779776, Validation Accuracy = 0.8644642857142857\n",
      "Iteration 90/100: Training Loss = 0.4627298318951738, Training Accuracy = 0.8726388888888889\n",
      "Validation Loss = 0.48648850825913303, Validation Accuracy = 0.8642857142857143\n",
      "Iteration 91/100: Training Loss = 0.4603275602756084, Training Accuracy = 0.8732738095238095\n",
      "Validation Loss = 0.4841491429957174, Validation Accuracy = 0.865\n",
      "Iteration 92/100: Training Loss = 0.45798537812904055, Training Accuracy = 0.8739087301587302\n",
      "Validation Loss = 0.48186476524570104, Validation Accuracy = 0.8657142857142858\n",
      "Iteration 93/100: Training Loss = 0.45569986344844454, Training Accuracy = 0.8743253968253968\n",
      "Validation Loss = 0.47964540570743824, Validation Accuracy = 0.86625\n",
      "Iteration 94/100: Training Loss = 0.45347180807865717, Training Accuracy = 0.8748412698412699\n",
      "Validation Loss = 0.47748365780646007, Validation Accuracy = 0.8664285714285714\n",
      "Iteration 95/100: Training Loss = 0.4512959575880067, Training Accuracy = 0.8753769841269842\n",
      "Validation Loss = 0.4753801372434471, Validation Accuracy = 0.86625\n",
      "Iteration 96/100: Training Loss = 0.44917588977966805, Training Accuracy = 0.8759920634920635\n",
      "Validation Loss = 0.4733301735206717, Validation Accuracy = 0.8667857142857143\n",
      "Iteration 97/100: Training Loss = 0.4471056863136331, Training Accuracy = 0.8764880952380952\n",
      "Validation Loss = 0.47133002653243905, Validation Accuracy = 0.8675\n",
      "Iteration 98/100: Training Loss = 0.44507907076964964, Training Accuracy = 0.8769246031746032\n",
      "Validation Loss = 0.4693755196096954, Validation Accuracy = 0.8678571428571429\n",
      "Iteration 99/100: Training Loss = 0.44309619869578715, Training Accuracy = 0.8774404761904762\n",
      "Validation Loss = 0.46746791708303687, Validation Accuracy = 0.8685714285714285\n",
      "Iteration 100/100: Training Loss = 0.4411559054432107, Training Accuracy = 0.8779365079365079\n",
      "Validation Loss = 0.46560437938306337, Validation Accuracy = 0.8691071428571429\n",
      "Training finished after 100 iterations\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MLP' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 60\u001b[0m\n\u001b[1;32m     57\u001b[0m             f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00ml\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m layers accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 60\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 52\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m     50\u001b[0m model\u001b[38;5;241m.\u001b[39msave_model(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMLP_mnist_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00ml\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 52\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m(X_test)\n\u001b[1;32m     54\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(y_test, y_pred)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy_report.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MLP' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from MLP import MLP\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def split_dataset(X, y, test_ratio=0.2):\n",
    "    N = X.shape[1]\n",
    "    indices = np.random.permutation(N)\n",
    "    \n",
    "    train_ratio = 1 - test_ratio\n",
    "\n",
    "    train_end = int(train_ratio * N)\n",
    "    \n",
    "    train_indices = indices[:train_end]\n",
    "    test_indices = indices[train_end:]\n",
    "    \n",
    "    X_train, y_train = X[:, train_indices], y[train_indices]\n",
    "    X_test, y_test = X[:, test_indices], y[test_indices]\n",
    "    \n",
    "    return (X_train, y_train), (X_test, y_test)\n",
    "\n",
    "def main():\n",
    "    # load datasets\n",
    "    X, y = datasets.fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "    X = X.to_numpy()\n",
    "    y = y.to_numpy()\n",
    "\n",
    "    # print(X.shape)\n",
    "    # print(y.shape)\n",
    "\n",
    "    # preprocess\n",
    "    X = X.astype(np.float32)\n",
    "    X /= 255.\n",
    "    X -= X.mean(axis=0)\n",
    "    X = X.T\n",
    "\n",
    "    y = y.astype(np.int16)\n",
    "\n",
    "    train_set, test_set = split_dataset(X, y)\n",
    "    X_train, y_train = train_set\n",
    "    X_test, y_test = test_set\n",
    "\n",
    "    # train and save models\n",
    "    L = [1, 2, 3, 4]\n",
    "    for l in L:\n",
    "        hidden_sizes = [32] * l\n",
    "        model = MLP(hidden_sizes=hidden_sizes, alpha=0.001, max_iter=100, early_stopping=True)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        model.save_model(f'MLP_mnist_{l}.pkl')\n",
    "\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        \n",
    "        with open('accuracy_report.txt', 'a') as f:\n",
    "            f.write(f'Model with {l} layers accuracy: {accuracy}\\n')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
